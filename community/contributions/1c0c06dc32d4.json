{
  "instance_id": "1c0c06dc32d4",
  "agos_version": "0.1.0",
  "contributed_at": "2026-02-18T08:43:41.259058",
  "cycles_completed": 177,
  "strategies_applied": [
    {
      "name": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in...",
      "module": "coordination",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15019v1",
          "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:28:51.631708",
      "applied_count": 1
    },
    {
      "name": "Distributed Quantum Gaussian Processes for Multi-Agent Systems",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15006v1",
          "title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:28:51.631729",
      "applied_count": 1
    },
    {
      "name": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through ...",
      "module": "intent.personas",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15028v1",
          "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:29:36.943081",
      "applied_count": 1
    },
    {
      "name": "Text Style Transfer with Parameter-efficient LLM Finetuning and R...",
      "module": "knowledge",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15013v1",
          "title": "Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:29:36.943096",
      "applied_count": 1
    },
    {
      "name": "Boundary Point Jailbreaking of Black-Box LLMs",
      "module": "intent",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15001v1",
          "title": "Boundary Point Jailbreaking of Black-Box LLMs"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:30:23.266311",
      "applied_count": 1
    },
    {
      "name": "Cold-Start Personalization via Training-Free Priors from Structur...",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15012v1",
          "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:31:56.450952",
      "applied_count": 1
    },
    {
      "name": "Learning Robust Markov Models for Safe Runtime Monitoring",
      "module": "intent.personas",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.14987v1",
          "title": "Learning Robust Markov Models for Safe Runtime Monitoring"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:34:30.172935",
      "applied_count": 1
    },
    {
      "name": "Counterfactual Fairness Evaluation of LLM-Based Contact Center Ag...",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.14970v1",
          "title": "Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:35:11.699260",
      "applied_count": 1
    },
    {
      "name": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Sce...",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.14968v1",
          "title": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:35:11.699294",
      "applied_count": 1
    },
    {
      "name": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safet...",
      "module": "intent",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15799v1",
          "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-18T02:18:09.947701",
      "applied_count": 1
    },
    {
      "name": "Operationalising the Superficial Alignment Hypothesis via Task Co...",
      "module": "intent",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15829v1",
          "title": "Operationalising the Superficial Alignment Hypothesis via Task Complexity"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-18T02:20:47.319426",
      "applied_count": 1
    }
  ],
  "discovered_patterns": [
    {
      "name": "Cosine Similarity Ranker",
      "module": "knowledge.semantic",
      "code_snippet": "import math\nfrom collections import Counter\n\ndef tfidf_vector(text, vocab):\n    words = text.lower().split()\n    tf = Counter(words)\n    return [tf.get(w, 0) / max(len(words), 1) for w in vocab]\n\ndef cosine_sim(a, b):\n    dot = sum(x * y for x, y in zip(a, b))\n    na = math.sqrt(sum(x*x for x in a))\n    nb = math.sqrt(sum(x*x for x in b))\n    return dot / (na * nb) if na and nb else 0.0\n\ndocs = ['agent memory retrieval', 'semantic search vectors',\n        'policy engine rules', 'agent memory sea",
      "sandbox_output": "Rankings: [(0, 0.816), (3, 0.816), (1, 0.0), (2, 0.0)]\nPASS: Cosine similarity ranker validated\n",
      "source_paper": "2602.15029v1"
    },
    {
      "name": "Message Channel Router",
      "module": "coordination",
      "code_snippet": "from collections import defaultdict\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n    def subscribe(self, agent, topics):\n        for t in topics:\n            self.subscribers[t].append(agent)\n    def send(self, topic, msg, sender):\n        self.history.append({'topic': topic, 'msg': msg, 'sender': sender})\n        receivers = self.subscribers.get(topic, [])\n        return [r for r in receivers if r !",
      "sandbox_output": "findings -> ['reviewer'], broadcast -> ['researcher', 'reviewer']\nPASS: Message channel router validated\n",
      "source_paper": "2602.15019v1"
    },
    {
      "name": "Weighted Graph Traverser",
      "module": "knowledge.graph",
      "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] =",
      "sandbox_output": "Graph traversal: {'agent': 1.0, 'memory': 0.63, 'policy': 0.49, 'facts': 0.3528, 'papers': 0.1482}\nPASS: Weighted graph traverser validated\n",
      "source_paper": "2602.15006v1"
    },
    {
      "name": "TTL LRU Cache",
      "module": "kernel",
      "code_snippet": "import time\nfrom collections import OrderedDict\n\nclass TTLCache:\n    def __init__(self, maxsize=100, ttl=60):\n        self.maxsize = maxsize\n        self.ttl = ttl\n        self.cache = OrderedDict()\n        self.hits = 0\n        self.misses = 0\n    def get(self, key):\n        if key in self.cache:\n            val, ts = self.cache[key]\n            if time.monotonic() - ts < self.ttl:\n                self.cache.move_to_end(key)\n                self.hits += 1\n                return val\n            ",
      "sandbox_output": "Cache: hits=2, misses=1, rate=0.667\nPASS: TTL LRU cache validated\n",
      "source_paper": "2602.15031v1"
    },
    {
      "name": "Persona Capability Matcher",
      "module": "intent.personas",
      "code_snippet": "class Persona:\n    def __init__(self, name, capabilities, budget, max_turns):\n        self.name = name\n        self.capabilities = set(capabilities)\n        self.budget = budget\n        self.max_turns = max_turns\n\ndef match_persona(task_needs, personas):\n    scored = []\n    for p in personas:\n        overlap = len(task_needs & p.capabilities)\n        coverage = overlap / max(len(task_needs), 1)\n        efficiency = overlap / max(len(p.capabilities), 1)\n        score = 0.6 * coverage + 0.4 * effi",
      "sandbox_output": "Best match for {'audit', 'analyze', 'read'}: reviewer (score=1.0)\nPASS: Persona capability matcher validated\n",
      "source_paper": "2602.15028v1"
    },
    {
      "name": "Adaptive Confidence Tracker",
      "module": "knowledge",
      "code_snippet": "import math\n\nclass ConfidenceTracker:\n    def __init__(self, decay=0.95):\n        self.decay = decay\n        self.counts = {}\n        self.conf = {}\n    def access(self, key):\n        self.counts[key] = self.counts.get(key, 0) + 1\n        self.conf[key] = 0.5 + 0.5 * (1 - math.exp(-self.counts[key] / 5))\n        return self.conf[key]\n    def decay_all(self):\n        for k in self.conf: self.conf[k] *= self.decay\n\nt = ConfidenceTracker()\nfor _ in range(10): c = t.access('k1')\nassert c > 0.85\nt.de",
      "sandbox_output": "After 10 accesses: 0.9323, after decay: 0.8857\nPASS: Adaptive confidence tracker validated\n",
      "source_paper": "2602.15013v1"
    },
    {
      "name": "Intent Classifier",
      "module": "intent",
      "code_snippet": "import math\n\nINTENT_RULES = {\n    'research': ['search', 'find', 'look up', 'investigate', 'analyze'],\n    'code': ['write', 'implement', 'fix', 'refactor', 'build'],\n    'review': ['review', 'check', 'audit', 'inspect', 'validate'],\n    'monitor': ['watch', 'track', 'alert', 'detect', 'observe'],\n    'automate': ['schedule', 'trigger', 'automate', 'repeat', 'cron'],\n}\n\ndef classify_intent(text):\n    text_lower = text.lower()\n    scores = {}\n    for intent, keywords in INTENT_RULES.items():\n    ",
      "sandbox_output": "Classified 4 intents correctly\nPASS: Intent classifier validated\n",
      "source_paper": "2602.15001v1"
    },
    {
      "name": "Softmax Diversity Scorer",
      "module": "knowledge.semantic",
      "code_snippet": "import math\nimport random\n\ndef softmax_score(values, temperature=0.3):\n    if not values: return []\n    exp_vals = [math.exp(v / max(temperature, 0.01)) for v in values]\n    total = sum(exp_vals)\n    return [v / total for v in exp_vals]\n\nscores = softmax_score([0.9, 0.7, 0.4, 0.2, 0.1])\nassert abs(sum(scores) - 1.0) < 0.001\nprint(f'Softmax scores: {[round(s,3) for s in scores]}')\nprint('PASS: Softmax diversity scorer validated')\n",
      "sandbox_output": "Softmax scores: [0.535, 0.275, 0.101, 0.052, 0.037]\nPASS: Softmax diversity scorer validated\n",
      "source_paper": "2602.15017v1"
    },
    {
      "name": "DAG Task Planner",
      "module": "orchestration.planner",
      "code_snippet": "from collections import defaultdict\n\nclass TaskDAG:\n    def __init__(self):\n        self.tasks = {}\n        self.deps = defaultdict(set)\n    def add(self, name, deps=None):\n        self.tasks[name] = {'status': 'pending'}\n        if deps:\n            for d in deps:\n                self.deps[name].add(d)\n    def topo_sort(self):\n        in_deg = defaultdict(int)\n        for t in self.tasks: in_deg[t] = 0\n        for t, ds in self.deps.items():\n            in_deg[t] = len(ds)\n        queue = [t fo",
      "sandbox_output": "Execution order: ['fetch_data', 'parse', 'validate', 'transform', 'store']\nParallel groups: [['fetch_data'], ['parse', 'validate'], ['transform'], ['store']]\nPASS: DAG task planner validated\n",
      "source_paper": "2602.15018v1"
    },
    {
      "name": "Layered Memory Retriever",
      "module": "knowledge.manager",
      "code_snippet": "class Layer:\n    def __init__(self, name, pri, data):\n        self.name, self.pri, self.data = name, pri, data\n    def query(self, q, n=5):\n        return [d for d in self.data if q.lower() in d.lower()][:n]\n\ndef layered_recall(layers, q, limit=10):\n    out = []\n    for l in sorted(layers, key=lambda x: x.pri, reverse=True):\n        if len(out) >= limit: break\n        out.extend(l.query(q, limit - len(out)))\n    return out\n\nw = Layer('working', 100, ['agent running now', 'current goal: evolve'])",
      "sandbox_output": "Layered recall: ['agent running now', 'agent completed scan', 'agent learned']\nPASS: Layered retriever validated\n",
      "source_paper": "2602.15016v1"
    },
    {
      "name": "Policy Rule Engine",
      "module": "policy",
      "code_snippet": "import re\n\nclass PolicyRule:\n    def __init__(self, pattern, action, effect):\n        self.pattern = pattern\n        self.action = action\n        self.effect = effect\n    def matches(self, agent, action):\n        p = self.pattern.replace('*', '.*')\n        return bool(re.match(p, agent)) and (\n            self.action == '*' or self.action == action\n        )\n\nclass PolicyEngine:\n    def __init__(self):\n        self.rules = []\n    def add_rule(self, pattern, action, effect):\n        self.rules.ap",
      "sandbox_output": "Policy checks: 4/4 passed\nPASS: Policy rule engine validated\n",
      "source_paper": "2602.15813v1"
    },
    {
      "name": "Token Budget Enforcer",
      "module": "policy",
      "code_snippet": "class TokenBudget:\n    def __init__(self, limit, burst_factor=1.5):\n        self.limit = limit\n        self.burst_limit = int(limit * burst_factor)\n        self.used = 0\n        self.violations = 0\n    def request(self, tokens):\n        if self.used + tokens > self.burst_limit:\n            self.violations += 1\n            return False\n        self.used += tokens\n        return True\n    def decay(self, factor=0.8):\n        self.used = int(self.used * factor)\n    @property\n    def utilization(self",
      "sandbox_output": "Budget: used=1150, violations=1, util=1.15\nPASS: Token budget enforcer validated\n",
      "source_paper": "2602.15830v1"
    },
    {
      "name": "Fitness Proportionate Selector",
      "module": "evolution",
      "code_snippet": "import random\n\nclass Strategy:\n    def __init__(self, name, fitness):\n        self.name = name\n        self.fitness = fitness\n        self.selected_count = 0\n\ndef roulette_select(strategies, n=1):\n    total = sum(s.fitness for s in strategies)\n    if total == 0:\n        return random.sample(strategies, min(n, len(strategies)))\n    selected = []\n    for _ in range(n):\n        pick = random.uniform(0, total)\n        current = 0\n        for s in strategies:\n            current += s.fitness\n        ",
      "sandbox_output": "Selection distribution: {'softmax': 430, 'layered': 358, 'confidence': 153, 'weak': 59}\nPASS: Fitness proportionate selector validated\n",
      "source_paper": "2602.15826v1"
    },
    {
      "name": "Exponential Moving Average Tracker",
      "module": "knowledge",
      "code_snippet": "class EMATracker:\n    def __init__(self, alpha=0.3):\n        self.alpha = alpha\n        self.values = {}\n    def update(self, key, value):\n        if key not in self.values:\n            self.values[key] = value\n        else:\n            self.values[key] = self.alpha * value + (1 - self.alpha) * self.values[key]\n        return round(self.values[key], 4)\n\nt = EMATracker(alpha=0.3)\nresults = []\nfor v in [1.0, 0.8, 0.9, 0.7, 0.85, 0.95]:\n    results.append(t.update('sig', v))\nassert abs(results[-1] ",
      "sandbox_output": "EMA series: [1.0, 0.94, 0.928, 0.8596, 0.8567, 0.8847]\nPASS: Exponential moving average tracker validated\n",
      "source_paper": "2602.15821v1"
    }
  ],
  "meta_evolution": {
    "genomes": {
      "knowledge.semantic": {
        "component": "knowledge.semantic",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "temperature",
            "current": null,
            "default": 0.0,
            "min_val": 0.0,
            "max_val": 1.0,
            "param_type": "float",
            "description": "Softmax retrieval diversity"
          },
          {
            "name": "track_access",
            "current": null,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Access-based confidence tracking"
          },
          {
            "name": "relevance_threshold",
            "current": null,
            "default": 0.01,
            "min_val": 0.001,
            "max_val": 0.1,
            "param_type": "float",
            "description": "Minimum cosine similarity for results"
          },
          {
            "name": "confidence_decay_factor",
            "current": null,
            "default": 0.95,
            "min_val": 0.8,
            "max_val": 0.99,
            "param_type": "float",
            "description": "Confidence decay for unused knowledge"
          },
          {
            "name": "confidence_decay_days",
            "current": null,
            "default": 30,
            "min_val": 7,
            "max_val": 90,
            "param_type": "int",
            "description": "Days inactive before decay kicks in"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T08:43:06.926711",
        "mutations_applied": 0
      },
      "knowledge.graph": {
        "component": "knowledge.graph",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "default_traversal_depth",
            "current": null,
            "default": 1,
            "min_val": 1,
            "max_val": 4,
            "param_type": "int",
            "description": "Default neighbor traversal hops"
          },
          {
            "name": "edge_weight_decay",
            "current": 1.0,
            "default": 0.99,
            "min_val": 0.9,
            "max_val": 1.0,
            "param_type": "float",
            "description": "Weight decay per consolidation cycle"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T08:43:06.926744",
        "mutations_applied": 1
      },
      "knowledge.consolidator": {
        "component": "knowledge.consolidator",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "older_than_hours",
            "current": 8,
            "default": 24,
            "min_val": 6,
            "max_val": 168,
            "param_type": "int",
            "description": "Consolidate events older than N hours"
          },
          {
            "name": "min_cluster_size",
            "current": 4,
            "default": 3,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Minimum events to form a summary"
          },
          {
            "name": "max_concurrent_writes",
            "current": 1,
            "default": 5,
            "min_val": 1,
            "max_val": 20,
            "param_type": "int",
            "description": "Semaphore limit for batch ops"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T08:43:06.926766",
        "mutations_applied": 45
      },
      "knowledge.loom": {
        "component": "knowledge.loom",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "use_layered_recall",
            "current": true,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Priority-ordered layer retrieval"
          },
          {
            "name": "recall_limit",
            "current": 25,
            "default": 10,
            "min_val": 3,
            "max_val": 50,
            "param_type": "int",
            "description": "Default recall result limit"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T08:43:06.926786",
        "mutations_applied": 21
      },
      "intent.engine": {
        "component": "intent.engine",
        "layer": "Agent & Intent Intelligence",
        "params": [
          {
            "name": "default_strategy",
            "current": null,
            "default": "solo",
            "min_val": null,
            "max_val": null,
            "param_type": "str",
            "description": "Fallback coordination strategy"
          },
          {
            "name": "max_intent_tokens",
            "current": 390,
            "default": 500,
            "min_val": 200,
            "max_val": 1500,
            "param_type": "int",
            "description": "Token limit for intent classification"
          }
        ],
        "fitness_score": 0.75,
        "last_evaluated": "2026-02-18T08:43:06.926806",
        "mutations_applied": 1
      },
      "intent.personas": {
        "component": "intent.personas",
        "layer": "Agent & Intent Intelligence",
        "params": [
          {
            "name": "researcher_budget",
            "current": 150000,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Researcher agent token budget"
          },
          {
            "name": "coder_budget",
            "current": 150000,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Coder agent token budget"
          },
          {
            "name": "orchestrator_budget",
            "current": 500000,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Orchestrator agent token budget"
          },
          {
            "name": "researcher_max_turns",
            "current": 80,
            "default": 30,
            "min_val": 5,
            "max_val": 80,
            "param_type": "int",
            "description": "Researcher max turns"
          },
          {
            "name": "coder_max_turns",
            "current": 35,
            "default": 40,
            "min_val": 10,
            "max_val": 100,
            "param_type": "int",
            "description": "Coder max turns"
          },
          {
            "name": "orchestrator_max_turns",
            "current": 57,
            "default": 50,
            "min_val": 10,
            "max_val": 100,
            "param_type": "int",
            "description": "Orchestrator max turns"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T08:43:06.926826",
        "mutations_applied": 48
      },
      "orchestration.planner": {
        "component": "orchestration.planner",
        "layer": "Agent Orchestration & Workflow",
        "params": [
          {
            "name": "parallel_threshold",
            "current": null,
            "default": 3,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Min subtasks to trigger parallel execution"
          },
          {
            "name": "pipeline_max_agents",
            "current": null,
            "default": 5,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Max agents in a pipeline"
          }
        ],
        "fitness_score": 0.75,
        "last_evaluated": "2026-02-18T08:43:06.926845",
        "mutations_applied": 0
      },
      "orchestration.runtime": {
        "component": "orchestration.runtime",
        "layer": "Agent Orchestration & Workflow",
        "params": [
          {
            "name": "max_concurrent_agents",
            "current": 79,
            "default": 50,
            "min_val": 5,
            "max_val": 200,
            "param_type": "int",
            "description": "Max agents running simultaneously"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T08:43:06.926865",
        "mutations_applied": 1
      },
      "policy.engine": {
        "component": "policy.engine",
        "layer": "Identity & Governance",
        "params": [
          {
            "name": "default_max_tokens",
            "current": null,
            "default": 200000,
            "min_val": 50000,
            "max_val": 1000000,
            "param_type": "int",
            "description": "Default agent token budget"
          },
          {
            "name": "default_max_turns",
            "current": null,
            "default": 50,
            "min_val": 10,
            "max_val": 200,
            "param_type": "int",
            "description": "Default agent turn limit"
          },
          {
            "name": "default_rate_limit",
            "current": null,
            "default": 60,
            "min_val": 10,
            "max_val": 200,
            "param_type": "int",
            "description": "Tool calls per minute"
          },
          {
            "name": "default_read_only",
            "current": null,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Default read-only mode"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T08:43:06.926892",
        "mutations_applied": 0
      },
      "events.bus": {
        "component": "events.bus",
        "layer": "Episodic Experience",
        "params": [
          {
            "name": "history_limit",
            "current": null,
            "default": 500,
            "min_val": 100,
            "max_val": 5000,
            "param_type": "int",
            "description": "Max events in memory"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T08:43:06.926925",
        "mutations_applied": 0
      },
      "events.tracing": {
        "component": "events.tracing",
        "layer": "Episodic Experience",
        "params": [
          {
            "name": "max_traces",
            "current": 176,
            "default": 200,
            "min_val": 50,
            "max_val": 1000,
            "param_type": "int",
            "description": "Max traces retained"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T08:43:06.926945",
        "mutations_applied": 1
      }
    },
    "recent_mutations": [
      {
        "id": "c8a4d37b45ce",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 6,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:14:13.825355"
      },
      {
        "id": "c8ad762c1b92",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 40,
        "new_value": 25,
        "reason": "LLM: Reduce from 40 to 25 turns to limit aggressive research behavior that may be causing policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:16:54.699220"
      },
      {
        "id": "3a268d9db1f4",
        "component": "intent.personas",
        "param_name": "coder_budget",
        "old_value": 213663,
        "new_value": 150000,
        "reason": "LLM: Reduce coder budget from 213663 to 150000 to constrain resource-intensive operations contributing to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:16:54.699258"
      },
      {
        "id": "53b387b16304",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 16,
        "new_value": 8,
        "reason": "LLM: Reduce concurrent writes from 16 to 8 to alleviate system pressure indicated by maxed event bus utilization",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:16:54.699280"
      },
      {
        "id": "6e681fc7729f",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 8,
        "new_value": 16,
        "reason": "LLM: Double concurrent writes from 8 to 16 to handle the high knowledge retrieval demand (hit_rate=1.00) and reduce write bottlenecks that may be causing policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:20:16.533961"
      },
      {
        "id": "e1cf7b7ee941",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 25,
        "new_value": 15,
        "reason": "LLM: Reduce researcher turns from 25 to 15 to prevent resource exhaustion and policy violations while maintaining adequate research capability within the budget constraints",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:20:16.534001"
      },
      {
        "id": "c9b20c402219",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 45,
        "new_value": 30,
        "reason": "LLM: Reduce recall limit from 45 to 30 to improve memory efficiency and reduce processing overhead, which should help lower the policy violation rate",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:20:16.534021"
      },
      {
        "id": "0c446dd47cd6",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 24,
        "reason": "LLM: Increase consolidation threshold from 6 to 24 hours to reduce memory pressure and processing load, which should help address the high policy violation rate",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:28:25.793250"
      },
      {
        "id": "f8cfd3614fd1",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 10,
        "reason": "LLM: Reduce researcher turns from 15 to 10 to lower computational overhead and prevent budget exhaustion that may be contributing to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:28:25.793281"
      },
      {
        "id": "480c6ff35817",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 30,
        "new_value": 20,
        "reason": "LLM: Decrease recall limit from 30 to 20 to reduce memory usage and query processing time, helping alleviate system capacity constraints",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:28:25.793298"
      },
      {
        "id": "0bb64d8a967e",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 45,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:29:16.204644"
      },
      {
        "id": "64e48f649447",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 45,
        "new_value": 24,
        "reason": "LLM: Reduce from 45 to 24 hours to consolidate knowledge more frequently, helping manage the high graph density and improving system efficiency",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:32:28.838469"
      },
      {
        "id": "b6022fe8b958",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 10,
        "new_value": 8,
        "reason": "LLM: Reduce from 10 to 8 turns to lower resource consumption and help address the high policy violation rate while maintaining research capability",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:32:28.838539"
      },
      {
        "id": "1571b082f9ea",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 18,
        "new_value": 15,
        "reason": "LLM: Reduce from 18 to 15 turns to decrease resource usage and alleviate system pressure indicated by maxed-out utilization metrics",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:32:28.838578"
      },
      {
        "id": "94f9132a162d",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 25,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:33:05.570938"
      },
      {
        "id": "565b1b68847b",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 25,
        "new_value": 12,
        "reason": "LLM: Reduce from 25 to 12 hours to consolidate knowledge more frequently, helping manage the high graph density and improving system efficiency",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:42:02.833308"
      },
      {
        "id": "63e2440150fc",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 8,
        "new_value": 6,
        "reason": "LLM: Reduce from 8 to 6 turns to lower resource consumption and help address the high policy violation rate indicating resource pressure",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:42:02.833519"
      },
      {
        "id": "6f61819995fb",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 15,
        "new_value": 12,
        "reason": "LLM: Reduce from 15 to 12 turns to decrease computational load while maintaining reasonable functionality for coding tasks",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:42:02.833603"
      },
      {
        "id": "1e3e69e53a14",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 6,
        "reason": "LLM: With knowledge retrieval at 100% hit rate and graph density maxed out, reducing consolidation threshold to minimum will free up memory faster and reduce pressure on the knowledge system",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:46:25.958390"
      },
      {
        "id": "14461244848a",
        "component": "intent.personas",
        "param_name": "researcher_budget",
        "old_value": 229669,
        "new_value": 150000,
        "reason": "LLM: Policy violation rate at 100% suggests budget overruns. Reducing researcher budget by ~35% should help bring policy violations under control while maintaining reasonable capability",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:46:25.958462"
      },
      {
        "id": "6fa3fccb5569",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 300000,
        "new_value": 200000,
        "reason": "LLM: Further reduce orchestrator budget by 33% to address policy violations while keeping it higher than other personas since orchestration is critical for system coordination",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:46:25.958506"
      },
      {
        "id": "f3393a7e6480",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 200000,
        "new_value": 286746,
        "reason": "Fitness 0.50 < 0.6, adjusting orchestrator_budget",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:47:13.063073"
      },
      {
        "id": "38ed7751cf63",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 24,
        "reason": "LLM: With perfect retrieval hit rate and graph density at max, knowledge is being heavily accessed. Increasing consolidation frequency from 6 to 24 hours will reduce write contention and improve performance under high load.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:50:37.558620"
      },
      {
        "id": "3a6d33368ff2",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 20,
        "new_value": 35,
        "reason": "LLM: High retrieval hit rate and history utilization suggest the system needs broader context. Increasing recall limit from 20 to 35 will provide richer knowledge synthesis for better decision making.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:50:37.558726"
      },
      {
        "id": "70427d7dbf3e",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 6,
        "new_value": 15,
        "reason": "LLM: With policy violations at max and high activity levels, the researcher persona needs more turns to properly investigate and understand constraints before acting, reducing violation rates.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:50:37.558794"
      },
      {
        "id": "84d161961ae2",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 17,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:51:24.749747"
      },
      {
        "id": "391d89bc944e",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 16,
        "new_value": 20,
        "reason": "LLM: Knowledge retrieval hit rate and graph density are at 1.00, indicating the knowledge system is at capacity. Increasing concurrent writes from 16 to maximum 20 should help handle the write bottleneck and improve consolidation performance.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:55:15.683045"
      },
      {
        "id": "23314253c5f2",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 25,
        "reason": "LLM: High activity levels and topic diversity suggest complex tasks requiring more research depth. Increasing researcher turns from 15 to 25 allows for more thorough investigation before handoff to other personas.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:55:15.683110"
      },
      {
        "id": "3dc626835d4d",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 17,
        "new_value": 12,
        "reason": "LLM: With maximum knowledge utilization, more frequent consolidation is needed. Reducing from 17 to 12 hours will help prevent knowledge fragmentation and maintain system performance under high load.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T07:55:15.683161"
      },
      {
        "id": "03ff48b36089",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 286746,
        "new_value": 400000,
        "reason": "LLM: High policy violation rate (1.00) and maxed utilization across systems suggests the orchestrator needs more budget to properly coordinate and enforce policies across components",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:05:58.810046"
      },
      {
        "id": "d9d2d03ac7d3",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 20,
        "new_value": 15,
        "reason": "LLM: Perfect retrieval hit rate but maxed graph density indicates write contention - reducing concurrent writes should improve consolidation quality and reduce system strain",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:05:58.810094"
      },
      {
        "id": "353ec7ee964e",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 35,
        "new_value": 25,
        "reason": "LLM: With perfect semantic retrieval and full history utilization, reducing recall limit will decrease memory pressure while maintaining quality, helping alleviate the resource bottleneck",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:05:58.810122"
      },
      {
        "id": "58b5fdd65376",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 400000,
        "new_value": 500000,
        "reason": "LLM: Policy violation rate at 1.00 suggests the orchestrator is resource-constrained and cannot properly coordinate activities within policy bounds. Maximizing its budget should improve policy compliance.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:10:32.749442"
      },
      {
        "id": "20d1d5443b5e",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 15,
        "new_value": 20,
        "reason": "LLM: Perfect retrieval hit rate and graph density at 1.00 indicate heavy knowledge system usage. Increasing concurrent writes will reduce bottlenecks and improve throughput.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:10:32.749489"
      },
      {
        "id": "405c04a73d9d",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 25,
        "new_value": 40,
        "reason": "LLM: High activity levels across all systems suggest the researcher needs more turns to complete complex tasks efficiently, reducing the need for multiple incomplete sessions.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:10:32.749518"
      },
      {
        "id": "8ff40dea7754",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 6,
        "reason": "LLM: With perfect retrieval hit rate and graph density at max, knowledge is being heavily utilized. Reducing consolidation threshold to 6 hours will process knowledge more frequently, preventing bottlenecks and maintaining system responsiveness.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.963836"
      },
      {
        "id": "47e0533a766f",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 25,
        "new_value": 50,
        "reason": "LLM: Maximum graph density and retrieval hit rate indicate heavy knowledge access patterns. Increasing recall limit to maximum (50) will provide more comprehensive context retrieval, supporting better decision-making under high utilization.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.963891"
      },
      {
        "id": "3e28385101af",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 12,
        "new_value": 20,
        "reason": "LLM: High activity levels across policy and events systems suggest complex tasks requiring more iterative development. Increasing coder max turns from 12 to 20 allows for more thorough problem-solving without hitting turn limits prematurely.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.963925"
      },
      {
        "id": "fd805cfb16dd",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 500000,
        "new_value": 400000,
        "reason": "LLM: High policy violation rate (1.00) suggests orchestrator is resource-constrained and cannot properly coordinate activities within policy bounds",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:19:24.504284"
      },
      {
        "id": "0658ccd52b58",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 12,
        "reason": "LLM: Perfect retrieval hit rate (1.00) with high activity suggests knowledge is being accessed faster than consolidated, creating processing backlog",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:19:24.504329"
      },
      {
        "id": "0087e44a462f",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 20,
        "new_value": 15,
        "reason": "LLM: Reduce concurrent writes to prevent resource contention that may be contributing to policy violations and system strain",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:19:24.504357"
      },
      {
        "id": "3df186020600",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 400000,
        "new_value": 500000,
        "reason": "LLM: Policy violation rate at 1.00 suggests the orchestrator is resource-constrained and cannot properly manage system policies. Maximizing its budget should improve governance.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:30:32.043753"
      },
      {
        "id": "16ae4cb10c9c",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 15,
        "new_value": 20,
        "reason": "LLM: Perfect retrieval hit rate and graph density at maximum suggest heavy knowledge system load. Increasing concurrent writes will reduce bottlenecks in knowledge processing.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:30:32.043799"
      },
      {
        "id": "fc4c128f9601",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 40,
        "new_value": 80,
        "reason": "LLM: High activity levels across all systems indicate complex tasks requiring more interaction cycles. Maximizing researcher turns allows for deeper investigation and better results.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:30:32.043825"
      },
      {
        "id": "c2f2cc19256f",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 500000,
        "new_value": 400000,
        "reason": "LLM: High policy violation rate (1.00) suggests the orchestrator needs more budget to properly coordinate and enforce policies across the system",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:35:32.280469"
      },
      {
        "id": "905f9b1d191a",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 8,
        "reason": "LLM: Perfect retrieval hit rate (1.00) with high activity suggests knowledge is being accessed faster than consolidated - reducing consolidation threshold will improve memory efficiency",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:35:32.280623"
      },
      {
        "id": "676b5d21d2e6",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 20,
        "new_value": 35,
        "reason": "LLM: Current coder budget is underutilized at fitness 0.50 - increasing max turns allows for more thorough code development and debugging cycles",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:35:32.280727"
      },
      {
        "id": "65a98ee9a67c",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 400000,
        "new_value": 500000,
        "reason": "LLM: Policy violation rate at 1.00 suggests the orchestrator is resource-constrained and unable to properly coordinate system behavior within policy bounds",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:39:25.942615"
      },
      {
        "id": "ac007b7084e5",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 20,
        "new_value": 1,
        "reason": "LLM: High graph density (1.00) indicates potential write contention; reducing concurrent writes should improve consolidation quality and reduce system strain",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:39:25.942652"
      },
      {
        "id": "f6d6a2758668",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 50,
        "new_value": 25,
        "reason": "LLM: Perfect retrieval hit rate (1.00) suggests over-retrieval; reducing recall limit will improve efficiency while maintaining quality",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:39:25.942676"
      }
    ],
    "timestamp": "2026-02-18T08:43:08.033469"
  },
  "meta_cycles_completed": 118,
  "design_archive": {
    "max_size": 50,
    "temperature": 0.3,
    "entries": [
      {
        "id": "d23ead38b1d3",
        "strategy_name": "Fitness Proportionate Selector",
        "module": "evolution",
        "code_hash": "7aeeb1088ba6bf81",
        "code_snippet": "import random\n\nclass Strategy:\n    def __init__(self, name, fitness):\n        self.name = name\n        self.fitness = fitness\n        self.selected_count = 0\n\ndef roulette_select(strategies, n=1):\n    total = sum(s.fitness for s in strategies)\n    if total == 0:\n        return random.sample(strategies, min(n, len(strategies)))\n    selected = []\n    for _ in range(n):\n        pick = random.uniform(0, total)\n        current = 0\n        for s in strategies:\n            current += s.fitness\n            if current >= pick:\n                s.selected_count += 1\n                selected.append(s)\n                break\n    return selected\n\nrandom.seed(42)\nstrats = [\n    Strategy('softmax', 0.9),\n    Strategy('layered', 0.7),\n    Strategy('confidence', 0.3),\n    Strategy('weak', 0.1),\n]\ncounts = {s.name: 0 for s in strats}\nfor _ in range(1000):\n    picked = roulette_select(strats, 1)\n    counts[picked[0].name] += 1\nassert counts['softmax'] > counts['weak']\nassert counts['softmax'] > 300\nprint(f'Selection distribution: {counts}')\nprint('PASS: Fitness proportionate selector validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15826v1",
        "created_at": "2026-02-18T02:53:49.371257"
      },
      {
        "id": "bc86c17da550",
        "strategy_name": "Layered Memory Retriever",
        "module": "knowledge.manager",
        "code_hash": "f143d9a1030c021d",
        "code_snippet": "class Layer:\n    def __init__(self, name, pri, data):\n        self.name, self.pri, self.data = name, pri, data\n    def query(self, q, n=5):\n        return [d for d in self.data if q.lower() in d.lower()][:n]\n\ndef layered_recall(layers, q, limit=10):\n    out = []\n    for l in sorted(layers, key=lambda x: x.pri, reverse=True):\n        if len(out) >= limit: break\n        out.extend(l.query(q, limit - len(out)))\n    return out\n\nw = Layer('working', 100, ['agent running now', 'current goal: evolve'])\ne = Layer('episodic', 50, ['agent completed scan', 'agent learned'])\ns = Layer('semantic', 10, ['agents use events', 'agent memory works'])\nr = layered_recall([s, w, e], 'agent', 3)\nassert len(r) == 3 and 'now' in r[0]\nprint(f'Layered recall: {r}')\nprint('PASS: Layered retriever validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15795v1",
        "created_at": "2026-02-18T02:55:23.371576"
      },
      {
        "id": "e4ceeb3740d8",
        "strategy_name": "Exponential Moving Average Tracker",
        "module": "knowledge",
        "code_hash": "673f9844a1eb9c2f",
        "code_snippet": "class EMATracker:\n    def __init__(self, alpha=0.3):\n        self.alpha = alpha\n        self.values = {}\n    def update(self, key, value):\n        if key not in self.values:\n            self.values[key] = value\n        else:\n            self.values[key] = self.alpha * value + (1 - self.alpha) * self.values[key]\n        return round(self.values[key], 4)\n\nt = EMATracker(alpha=0.3)\nresults = []\nfor v in [1.0, 0.8, 0.9, 0.7, 0.85, 0.95]:\n    results.append(t.update('sig', v))\nassert abs(results[-1] - 0.85) < 0.15\nassert results[0] == 1.0\nprint(f'EMA series: {results}')\nprint('PASS: Exponential moving average tracker validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15821v1",
        "created_at": "2026-02-18T03:03:01.166033"
      },
      {
        "id": "81df49ae97fa",
        "strategy_name": "Adaptive Confidence Tracker_gen1",
        "module": "knowledge",
        "code_hash": "fc4927775df785f8",
        "code_snippet": "import math\nimport time\nfrom collections import defaultdict\n\nclass AdaptiveKnowledgeTracker:\n    def __init__(self, decay=0.95, time_weight=0.1):\n        self.decay = decay\n        self.time_weight = time_weight\n        self.access_counts = defaultdict(int)\n        self.confidence_scores = defaultdict(float)\n        self.last_access = defaultdict(float)\n        self.success_rates = defaultdict(lambda: {'hits': 0, 'total': 0})\n        self.context_clusters = defaultdict(set)\n        \n    def access(self, key, context=None, success=None):\n        current_time = time.time()\n        self.access_counts[key] += 1\n        \n        # Time-weighted frequency boost\n        time_factor = 1.0\n        if key in self.last_access:\n            time_gap = current_time - self.last_access[key]\n            time_factor = 1.0 + self.time_weight * math.exp(-time_gap / 3600)  # Hour-based decay\n        \n        self.last_access[key] = current_time\n        \n        # Update success rate if feedback provided\n        if success is not None:\n            self.success_rates[key]['total'] += 1\n            if success:\n                self.success_rates[key]['hits'] += 1\n        \n        # Calculate base confidence from access pattern\n        base_conf = 0.3 + 0.6 * (1 - math.exp(-self.access_counts[key] / 3))\n        \n        # Apply success rate modifier\n        success_rate = (self.success_rates[key]['hits'] / \n                       max(1, self.success_rates[key]['total']))\n        success_modifier = 0.5 + 0.5 * success_rate\n        \n        # Context clustering bonus\n        context_bonus = 1.0\n        if context:\n            self.context_clusters[key].add(context)\n            context_bonus = 1.0 + 0.2 * math.log(1 + len(self.context_clusters[key]))\n        \n        # Final confidence calculation\n        self.confidence_scores[key] = min(0.95, \n            base_conf * time_factor * success_modifier * context_bonus)\n        \n        return self.confidence_scores[key]\n    \n    def decay_all(self):\n        for key in list(self.confidence_scores.keys()):\n            self.confidence_scores[key] *= self.decay\n            if self.confidence_scores[key] < 0.1:\n                self._cleanup_key(key)\n    \n    def _cleanup_key(self, key):\n        \"\"\"Remove low-confidence entries to prevent memory bloat\"\"\"\n        if key in self.confidence_scores:\n            del self.confidence_scores[key]\n        if key in self.access_counts:\n            del self.access_counts[key]\n    \n    def get_top_knowledge(self, n=5):\n        \"\"\"Return top N most confident knowledge items\"\"\"\n        return sorted(self.confidence_scores.items(), \n                     key=lambda x: x[1], reverse=True)[:n]\n\n# Validation with enhanced testing\ntracker = AdaptiveKnowledgeTracker()\n\n# Test basic access pattern\nfor i in range(10):\n    conf = tracker.access('core_concept', context='learning', success=True)\n\n# Test with mixed success rates\nfor i in range(5):\n    tracker.access('uncertain_fact', success=i % 2 == 0)\n\n# Tes",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "f2fab012209b",
        "source_paper": "2602.15013v1",
        "created_at": "2026-02-18T05:25:00.319693"
      },
      {
        "id": "39e31eb2f64d",
        "strategy_name": "DAG Task Planner_gen1",
        "module": "orchestration.planner",
        "code_hash": "ddd37df23a753016",
        "code_snippet": "def solution(n, edges):\n    from collections import defaultdict, deque\n    \n    # Build adjacency list\n    graph = defaultdict(list)\n    for u, v in edges:\n        graph[u].append(v)\n        graph[v].append(u)\n    \n    # Find all nodes with degree 1 (leaves)\n    def get_leaves():\n        leaves = []\n        for i in range(n):\n            if len(graph[i]) <= 1:\n                leaves.append(i)\n        return leaves\n    \n    # If n <= 2, all nodes are centers\n    if n <= 2:\n        return list(range(n))\n    \n    remaining = n\n    while remaining > 2:\n        leaves = get_leaves()\n        remaining -= len(leaves)\n        \n        # Remove leaves and their edges\n        for leaf in leaves:\n            if graph[leaf]:  # Check if leaf still has connections\n                neighbor = graph[leaf][0]\n                graph[neighbor].remove(leaf)\n            del graph[leaf]\n    \n    # Return remaining nodes\n    return sorted([node for node in graph.keys()])",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "cd471c75b957",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T07:28:18.967101"
      },
      {
        "id": "43abfc883700",
        "strategy_name": "Message Channel Router_gen1",
        "module": "coordination",
        "code_hash": "729ea2ed3c127ac8",
        "code_snippet": "from collections import defaultdict\nimport json\nimport time\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n        self.message_queue = defaultdict(list)\n        self.priorities = defaultdict(lambda: 1)\n        self.filters = defaultdict(list)\n    \n    def subscribe(self, agent, topics, priority=1):\n        for t in topics:\n            if agent not in self.subscribers[t]:\n                self.subscribers[t].append(agent)\n                self.priorities[(t, agent)] = priority\n    \n    def add_filter(self, topic, filter_func):\n        self.filters[topic].append(filter_func)\n    \n    def send(self, topic, msg, sender, priority=1, metadata=None):\n        timestamp = time.time()\n        message = {\n            'topic': topic, \n            'msg': msg, \n            'sender': sender, \n            'priority': priority,\n            'timestamp': timestamp,\n            'metadata': metadata or {}\n        }\n        \n        # Apply filters\n        for filter_func in self.filters[topic]:\n            if not filter_func(message):\n                return []\n        \n        self.history.append(message)\n        receivers = self.subscribers.get(topic, [])\n        \n        # Sort by priority and queue messages\n        valid_receivers = [r for r in receivers if r != sender]\n        for receiver in valid_receivers:\n            receiver_priority = self.priorities.get((topic, receiver), 1)\n            message['receiver_priority'] = receiver_priority\n            self.message_queue[receiver].append(message)\n            # Keep queue sorted by priority and timestamp\n            self.message_queue[receiver].sort(key=lambda x: (-x['priority'], -x['receiver_priority'], x['timestamp']))\n        \n        return valid_receivers\n    \n    def broadcast(self, msg, sender, priority=1):\n        all_agents = set()\n        for agents in self.subscribers.values():\n            all_agents.update(agents)\n        all_agents.discard(sender)\n        \n        timestamp = time.time()\n        message = {\n            'topic': '*', \n            'msg': msg, \n            'sender': sender, \n            'priority': priority,\n            'timestamp': timestamp,\n            'metadata': {'broadcast': True}\n        }\n        self.history.append(message)\n        \n        # Queue broadcast for all agents\n        for agent in all_agents:\n            self.message_queue[agent].append(message.copy())\n        \n        return sorted(all_agents)\n    \n    def get_messages(self, agent, limit=None):\n        messages = self.message_queue.get(agent, [])\n        if limit:\n            result = messages[:limit]\n            self.message_queue[agent] = messages[limit:]\n        else:\n            result = messages\n            self.message_queue[agent] = []\n        return result\n    \n    def get_stats(self):\n        topic_counts = defaultdict(int)\n        for msg in self.history:\n            topic_counts[msg['topic']] += 1\n        retur",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "a564aa474d47",
        "source_paper": "2602.15776v1",
        "created_at": "2026-02-18T07:32:22.117545"
      },
      {
        "id": "4742a2a590cd",
        "strategy_name": "Intent Classifier_gen1",
        "module": "intent",
        "code_hash": "77b66b060a15d789",
        "code_snippet": "import re\nimport math\nfrom collections import defaultdict, Counter\n\nclass IntentClassifier:\n    def __init__(self):\n        self.intent_patterns = {\n            'research': {\n                'keywords': ['search', 'find', 'look up', 'investigate', 'analyze', 'study', 'explore', 'discover', 'learn', 'examine'],\n                'patterns': [r'\\b(search|find|look\\s+up|investigate|analyze|study|explore|research)\\b'],\n                'context': ['papers', 'information', 'data', 'knowledge', 'facts', 'details']\n            },\n            'code': {\n                'keywords': ['write', 'implement', 'fix', 'refactor', 'build', 'create', 'develop', 'code', 'program', 'debug'],\n                'patterns': [r'\\b(write|implement|fix|refactor|build|create|develop|code|program)\\b', r'\\bfunction\\b', r'\\bclass\\b'],\n                'context': ['function', 'class', 'method', 'script', 'application', 'program', 'code', 'bug']\n            },\n            'review': {\n                'keywords': ['review', 'check', 'audit', 'inspect', 'validate', 'verify', 'examine', 'assess', 'evaluate'],\n                'patterns': [r'\\b(review|check|audit|inspect|validate|verify|examine|assess)\\b'],\n                'context': ['logs', 'code', 'security', 'quality', 'compliance', 'standards']\n            },\n            'monitor': {\n                'keywords': ['watch', 'track', 'alert', 'detect', 'observe', 'monitor', 'supervise', 'survey'],\n                'patterns': [r'\\b(watch|track|alert|detect|observe|monitor)\\b', r'\\bfor\\s+(anomalies|changes|issues)\\b'],\n                'context': ['anomalies', 'changes', 'performance', 'system', 'metrics', 'status']\n            },\n            'automate': {\n                'keywords': ['schedule', 'trigger', 'automate', 'repeat', 'cron', 'batch', 'routine', 'periodic'],\n                'patterns': [r'\\b(schedule|trigger|automate|repeat|cron|batch)\\b', r'\\bevery\\s+\\d+\\b'],\n                'context': ['task', 'job', 'process', 'workflow', 'routine', 'daily', 'weekly']\n            }\n        }\n        \n    def _extract_features(self, text):\n        text_lower = text.lower()\n        features = {\n            'keyword_matches': defaultdict(int),\n            'pattern_matches': defaultdict(int),\n            'context_matches': defaultdict(int),\n            'word_count': len(text_lower.split()),\n            'has_question': '?' in text,\n            'has_action_words': any(word in text_lower for word in ['can', 'should', 'will', 'need', 'want'])\n        }\n        \n        for intent, config in self.intent_patterns.items():\n            # Keyword matching with partial word support\n            for keyword in config['keywords']:\n                if keyword in text_lower:\n                    features['keyword_matches'][intent] += 1\n                    \n            # Pattern matching\n            for pattern in config['patterns']:\n                matches = len(re.findall(pattern, text_lower))\n                features['pattern_matches'][intent] += matches\n         ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "c32f3a5023fa",
        "source_paper": "2602.15825v1",
        "created_at": "2026-02-18T07:40:05.352138"
      },
      {
        "id": "a594919179a0",
        "strategy_name": "Cosine Similarity Ranker",
        "module": "knowledge.semantic",
        "code_hash": "d5dce0e92805f7f5",
        "code_snippet": "import math\nfrom collections import Counter\n\ndef tfidf_vector(text, vocab):\n    words = text.lower().split()\n    tf = Counter(words)\n    return [tf.get(w, 0) / max(len(words), 1) for w in vocab]\n\ndef cosine_sim(a, b):\n    dot = sum(x * y for x, y in zip(a, b))\n    na = math.sqrt(sum(x*x for x in a))\n    nb = math.sqrt(sum(x*x for x in b))\n    return dot / (na * nb) if na and nb else 0.0\n\ndocs = ['agent memory retrieval', 'semantic search vectors',\n        'policy engine rules', 'agent memory search']\nvocab = sorted(set(' '.join(docs).lower().split()))\nquery_vec = tfidf_vector('agent memory', vocab)\nscores = [(i, round(cosine_sim(query_vec, tfidf_vector(d, vocab)), 3)) for i, d in enumerate(docs)]\nscores.sort(key=lambda x: -x[1])\nassert scores[0][1] > scores[-1][1]\nprint(f'Rankings: {scores}')\nprint('PASS: Cosine similarity ranker validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15029v1",
        "created_at": "2026-02-17T21:28:51.632378"
      },
      {
        "id": "e69be3e04929",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15006v1",
        "created_at": "2026-02-17T21:28:51.632560"
      },
      {
        "id": "bfdcf6f40109",
        "strategy_name": "Cosine Similarity Ranker",
        "module": "knowledge.semantic",
        "code_hash": "d5dce0e92805f7f5",
        "code_snippet": "import math\nfrom collections import Counter\n\ndef tfidf_vector(text, vocab):\n    words = text.lower().split()\n    tf = Counter(words)\n    return [tf.get(w, 0) / max(len(words), 1) for w in vocab]\n\ndef cosine_sim(a, b):\n    dot = sum(x * y for x, y in zip(a, b))\n    na = math.sqrt(sum(x*x for x in a))\n    nb = math.sqrt(sum(x*x for x in b))\n    return dot / (na * nb) if na and nb else 0.0\n\ndocs = ['agent memory retrieval', 'semantic search vectors',\n        'policy engine rules', 'agent memory search']\nvocab = sorted(set(' '.join(docs).lower().split()))\nquery_vec = tfidf_vector('agent memory', vocab)\nscores = [(i, round(cosine_sim(query_vec, tfidf_vector(d, vocab)), 3)) for i, d in enumerate(docs)]\nscores.sort(key=lambda x: -x[1])\nassert scores[0][1] > scores[-1][1]\nprint(f'Rankings: {scores}')\nprint('PASS: Cosine similarity ranker validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15021v1",
        "created_at": "2026-02-17T21:30:23.266822"
      },
      {
        "id": "ff45ce1c7fa1",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15010v1",
        "created_at": "2026-02-17T21:30:23.266895"
      },
      {
        "id": "7065b7cd4079",
        "strategy_name": "Softmax Diversity Scorer",
        "module": "knowledge.semantic",
        "code_hash": "cd13aa12d8457ce4",
        "code_snippet": "import math\nimport random\n\ndef softmax_score(values, temperature=0.3):\n    if not values: return []\n    exp_vals = [math.exp(v / max(temperature, 0.01)) for v in values]\n    total = sum(exp_vals)\n    return [v / total for v in exp_vals]\n\nscores = softmax_score([0.9, 0.7, 0.4, 0.2, 0.1])\nassert abs(sum(scores) - 1.0) < 0.001\nprint(f'Softmax scores: {[round(s,3) for s in scores]}')\nprint('PASS: Softmax diversity scorer validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15017v1",
        "created_at": "2026-02-17T21:31:10.487926"
      },
      {
        "id": "598e831e14c5",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15012v1",
        "created_at": "2026-02-17T21:31:58.487560"
      },
      {
        "id": "c37112d84402",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15011v1",
        "created_at": "2026-02-17T21:31:58.487768"
      },
      {
        "id": "c385ab7a34fb",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15025v1",
        "created_at": "2026-02-17T21:32:45.492570"
      },
      {
        "id": "5c8715548006",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.14997v1",
        "created_at": "2026-02-17T21:34:30.174732"
      },
      {
        "id": "1444ae630838",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.14970v1",
        "created_at": "2026-02-17T21:35:11.700322"
      },
      {
        "id": "bb7907a99f77",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.14968v1",
        "created_at": "2026-02-17T21:35:11.700515"
      },
      {
        "id": "b901662ab009",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15008v1",
        "created_at": "2026-02-17T21:36:21.784812"
      },
      {
        "id": "214b74f8fbaf",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          0.6,
          0.6,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15015v1",
        "created_at": "2026-02-17T21:37:00.628506"
      },
      {
        "id": "9a8d72b03fe6",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15784v1",
        "created_at": "2026-02-18T02:18:09.966197"
      },
      {
        "id": "a45d0fb6620c",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15824v1",
        "created_at": "2026-02-18T02:21:48.822425"
      },
      {
        "id": "3447335f17f9",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15818v1",
        "created_at": "2026-02-18T02:22:56.840947"
      },
      {
        "id": "4fb9745bdd21",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15787v1",
        "created_at": "2026-02-18T02:55:23.371966"
      },
      {
        "id": "a48f15292d1e",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15811v1",
        "created_at": "2026-02-18T02:55:23.372136"
      },
      {
        "id": "175e6d341898",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "723ebc7a53a1a7d3",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = max(self.relation_weights[rel], weight)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = {\n                'score': round(score, 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    new_path = path + [rel]\n                    relevance_boost = 1.0 + (0.1 if rel in ['uses', 'contains', 'implements'] else 0.0)\n                    new_score = score * w * decay * relevance_boost\n                    queue.append((dst, new_score, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3):\n        paths = []\n        queue = deque([(start, [start], [])])\n        \n        while queue:\n            node, path, relations = queue.popleft()\n            \n            if len(path) > max_depth + 1:\n                continue\n                \n            if node == end and len(path) > 1:\n                total_weight = 1.0\n                for i, rel in enumerate(relations):\n                    total_weight *= self.relation_weights[rel]\n                paths.append({\n                    'path': path.copy(),\n                    'relations': relations.copy(),\n                    'weight': round(total_weight, 4)\n                })\n                continue\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:\n                    queue.append((dst, path + [dst], relations + [rel]))\n        \n        return sorted(paths, key=lambda x: x['weight'], reverse=True)\n    \n    def get_neighborhood(self, node, radius=1):\n        neighborhood = set([node])\n        current_layer = {node}\n        \n        for _ in range(radius):\n            next_layer = set()\n            for n in current_layer:\n                for dst, _, _ in self.edges.get(n, []):\n                    next_layer.add(dst)\n                for src, _, _ in self.reverse_edges.get(n, []):\n             ",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "a45d0fb6620c",
        "source_paper": "2602.15824v1",
        "created_at": "2026-02-18T05:25:16.854971"
      },
      {
        "id": "cd0e72c93500",
        "strategy_name": "Weighted Graph Traverser",
        "module": "knowledge.graph",
        "code_hash": "5591d98ed2221fe3",
        "code_snippet": "from collections import defaultdict\n\nclass WeightedGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n    def add(self, src, rel, dst, weight=1.0):\n        self.edges[src].append((dst, rel, weight))\n    def traverse(self, start, max_depth=3, decay=0.7):\n        visited = {}\n        queue = [(start, 1.0, 0)]\n        while queue:\n            node, score, depth = queue.pop(0)\n            if node in visited or depth > max_depth:\n                continue\n            visited[node] = round(score, 4)\n            for dst, rel, w in self.edges.get(node, []):\n                queue.append((dst, score * w * decay, depth + 1))\n        return visited\n\ng = WeightedGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('agent', 'has', 'policy', 0.7)\ng.add('facts', 'derived_from', 'papers', 0.6)\nresult = g.traverse('agent', max_depth=3)\nassert 'memory' in result and 'facts' in result\nassert result['agent'] == 1.0\nassert result['memory'] < 1.0\nprint(f'Graph traversal: {result}')\nprint('PASS: Weighted graph traverser validated')\n",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15803v1",
        "created_at": "2026-02-18T05:25:18.158577"
      },
      {
        "id": "096efdaf94af",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "73c94a62dcac1727",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_weights = defaultdict(lambda: 1.0)\n        self.relation_types = set()\n    \n    def add(self, src, rel, dst, weight=1.0, bidirectional=False):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        if bidirectional:\n            self.edges[dst].append((src, rel, weight))\n            self.reverse_edges[src].append((dst, rel, weight))\n    \n    def set_node_importance(self, node, importance):\n        self.node_weights[node] = importance\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01, relation_filter=None):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            # Apply node importance weighting\n            final_score = score * self.node_weights[node]\n            visited[node] = {\n                'score': round(final_score, 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            # Explore neighbors\n            for dst, rel, edge_weight in self.edges.get(node, []):\n                if relation_filter and rel not in relation_filter:\n                    continue\n                    \n                new_score = final_score * edge_weight * decay\n                new_path = path + [(node, rel, dst)]\n                queue.append((dst, new_score, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3):\n        paths = []\n        queue = deque([(start, [start], 0)])\n        \n        while queue:\n            node, path, depth = queue.popleft()\n            \n            if depth > max_depth:\n                continue\n                \n            if node == end:\n                paths.append(path)\n                continue\n            \n            for dst, rel, weight in self.edges.get(node, []):\n                if dst not in path:  # Avoid cycles\n                    queue.append((dst, path + [dst], depth + 1))\n        \n        return paths\n    \n    def get_related_concepts(self, concept, relation_types=None, direction='both'):\n        related = set()\n        \n        if direction in ['forward', 'both']:\n            for dst, rel, _ in self.edges.get(concept, []):\n                if not relation_types or rel in relation_types:\n                    related.add(dst)\n        \n        if direction in ['backward', 'both']:\n            for src, rel, _ in self.reverse_edges.get(concept, []):\n                if not relation_types or rel in relation_types:\n                    related.add(src)\n        \n        retur",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "c385ab7a34fb",
        "source_paper": "2602.15025v1",
        "created_at": "2026-02-18T07:27:46.541770"
      },
      {
        "id": "804d4639c205",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "59da9c3d1a97cd49",
        "code_snippet": "from collections import defaultdict, deque\nimport json\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_metadata = defaultdict(dict)\n        self.relation_types = set()\n    \n    def add(self, src, rel, dst, weight=1.0, metadata=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        if metadata:\n            self.node_metadata[src].update(metadata)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = {}\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(score, 4)\n            paths[node] = path + [node]\n            \n            for dst, rel, w in self.edges.get(node, []):\n                new_score = score * w * (decay ** depth)\n                queue.append((dst, new_score, depth + 1, path + [node]))\n        \n        return {'scores': visited, 'paths': paths}\n    \n    def find_connections(self, src, dst, max_depth=4):\n        \"\"\"Find all paths between two nodes\"\"\"\n        paths = []\n        queue = deque([(src, [src], 1.0, 0)])\n        \n        while queue:\n            node, path, score, depth = queue.popleft()\n            \n            if depth > max_depth:\n                continue\n                \n            if node == dst:\n                paths.append({'path': path, 'score': round(score, 4)})\n                continue\n            \n            for next_node, rel, weight in self.edges.get(node, []):\n                if next_node not in path:  # Avoid cycles\n                    new_path = path + [next_node]\n                    new_score = score * weight\n                    queue.append((next_node, new_path, new_score, depth + 1))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighbors(self, node, relation_filter=None, bidirectional=True):\n        \"\"\"Get neighboring nodes with optional relation filtering\"\"\"\n        neighbors = {}\n        \n        # Forward edges\n        for dst, rel, weight in self.edges.get(node, []):\n            if not relation_filter or rel in relation_filter:\n                neighbors[dst] = {'relation': rel, 'weight': weight, 'direction': 'out'}\n        \n        # Reverse edges if bidirectional\n        if bidirectional:\n            for src, rel, weight in self.reverse_edges.get(node, []):\n                if not relation_filter or rel in relation_filter:\n                    if src not in neighbors:\n                        neighbors[src] = {'relation': rel, 'weight': weight, 'direction': 'in'}\n        \n        return neighbors\n    \n    def cluster_analysis(self, node, radi",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "214b74f8fbaf",
        "source_paper": "2602.15015v1",
        "created_at": "2026-02-18T07:32:08.071694"
      },
      {
        "id": "babf1eb1799d",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "d30911afa4798f09",
        "code_snippet": "from collections import defaultdict, deque\nimport json\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_metadata = defaultdict(dict)\n        self.relation_types = set()\n    \n    def add(self, src, rel, dst, weight=1.0, metadata=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        if metadata:\n            self.node_metadata[src].update(metadata)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, relation_filter=None):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth:\n                continue\n                \n            visited[node] = {\n                'score': round(score, 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if relation_filter and rel not in relation_filter:\n                    continue\n                new_path = path + [rel]\n                queue.append((dst, score * w * decay, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3):\n        paths = []\n        queue = deque([(start, [start], 1.0, 0)])\n        \n        while queue:\n            node, path, score, depth = queue.popleft()\n            \n            if depth > max_depth:\n                continue\n                \n            if node == end:\n                paths.append({\n                    'path': path,\n                    'score': round(score, 4),\n                    'relations': []\n                })\n                continue\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Avoid cycles\n                    new_path = path + [dst]\n                    queue.append((dst, new_path, score * w, depth + 1))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighbors(self, node, direction='out'):\n        if direction == 'out':\n            return [(dst, rel, w) for dst, rel, w in self.edges.get(node, [])]\n        elif direction == 'in':\n            return [(src, rel, w) for src, rel, w in self.reverse_edges.get(node, [])]\n        else:\n            return (self.get_neighbors(node, 'out') + \n                   self.get_neighbors(node, 'in'))\n    \n    def compute_centrality(self):\n        centrality = defaultdict(float)\n        for node in set(list(self.edges.keys()) + list(self.reverse_edges.keys())):\n            out_degree = len(self.edges[node])\n            in_degree = len(self.reverse_edges[node])\n            centrality[node] = math.sqrt(out_degree * in_degree)\n        return dict(centrality)\n    \n    def export_",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "c37112d84402",
        "source_paper": "2602.15011v1",
        "created_at": "2026-02-18T07:40:32.875944"
      },
      {
        "id": "0e7f84c32582",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "d2fb99e41a2546b9",
        "code_snippet": "from collections import defaultdict, deque\nimport json\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = weight\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = {\n                'score': round(score, 4),\n                'depth': depth,\n                'path': path.copy(),\n                'type': self.node_types.get(node, 'unknown')\n            }\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    new_path = path + [rel]\n                    new_score = score * w * (decay ** depth)\n                    queue.append((dst, new_score, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3):\n        paths = []\n        queue = deque([(start, [start], [])])\n        \n        while queue:\n            node, path, relations = queue.popleft()\n            \n            if len(path) > max_depth + 1:\n                continue\n                \n            if node == end:\n                score = self._calculate_path_score(relations)\n                paths.append({\n                    'path': path,\n                    'relations': relations,\n                    'score': round(score, 4)\n                })\n                continue\n            \n            for dst, rel, weight in self.edges.get(node, []):\n                if dst not in path:\n                    queue.append((dst, path + [dst], relations + [rel]))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def _calculate_path_score(self, relations):\n        if not relations:\n            return 1.0\n        score = 1.0\n        for i, rel in enumerate(relations):\n            score *= self.relation_weights[rel] * (0.8 ** i)\n        return score\n    \n    def get_neighbors(self, node, direction='out'):\n        if direction == 'out':\n            return [(dst, rel, w) for dst, rel, w in self.edges.get(node, [])]\n        elif direction == 'in':\n            return [(src, rel, w) for src, rel, w in self.reverse_edges.get(node, [])]\n        else:\n            return (self.get_neighbors(node, '",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "598e831e14c5",
        "source_paper": "2602.15012v1",
        "created_at": "2026-02-18T07:45:02.364438"
      },
      {
        "id": "5ac4bfabd6ba",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "9f59d96ced04ee2c",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = max(self.relation_weights[rel], weight)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = defaultdict(list)\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited and visited[node] >= score:\n                continue\n            if depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(max(visited.get(node, 0), score), 4)\n            paths[node].append(path + [node])\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    rel_boost = 1.0 + (self.relation_weights[rel] - 1.0) * 0.1\n                    new_score = score * w * decay * rel_boost\n                    queue.append((dst, new_score, depth + 1, path + [node]))\n        \n        return visited, paths\n    \n    def find_connections(self, node1, node2, max_depth=4):\n        \"\"\"Find all paths between two nodes\"\"\"\n        if node1 not in self.edges and node1 not in self.reverse_edges:\n            return []\n        \n        paths = []\n        queue = deque([(node1, [node1], 1.0, 0)])\n        \n        while queue:\n            current, path, score, depth = queue.popleft()\n            \n            if current == node2 and len(path) > 1:\n                paths.append((path, round(score, 4)))\n                continue\n            \n            if depth >= max_depth:\n                continue\n                \n            for dst, rel, w in self.edges.get(current, []):\n                if dst not in path:\n                    new_score = score * w * (0.9 ** depth)\n                    queue.append((dst, path + [dst], new_score, depth + 1))\n        \n        return sorted(paths, key=lambda x: x[1], reverse=True)\n    \n    def get_neighborhood(self, node, radius=2):\n        \"\"\"Get all nodes within radius distance\"\"\"\n        neighborhood = set([node])\n        current_layer = {node}\n        \n        for _ in range(radius):\n            next_layer = set()\n            for n in current_layer:\n                for dst, _, _ in self.edges.get(n, []):\n                    if dst not in neighborhood:\n                        next_layer.add(dst)\n                        neighborhood.add(d",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "a48f15292d1e",
        "source_paper": "2602.15811v1",
        "created_at": "2026-02-18T07:45:22.680669"
      },
      {
        "id": "4b59369ff46a",
        "strategy_name": "Cosine Similarity Ranker_gen1",
        "module": "knowledge.semantic",
        "code_hash": "7298ac1a643b27b6",
        "code_snippet": "import math\nimport re\nfrom collections import Counter, defaultdict\n\nclass SemanticIndex:\n    def __init__(self):\n        self.docs = []\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_vectors = []\n        \n    def add_document(self, text, doc_id=None):\n        doc_id = doc_id or len(self.docs)\n        self.docs.append((doc_id, text))\n        words = self._tokenize(text)\n        self.vocab.update(words)\n        self._invalidate_cache()\n        return doc_id\n    \n    def _tokenize(self, text):\n        return re.findall(r'\\b\\w+\\b', text.lower())\n    \n    def _compute_idf(self):\n        if self.idf_cache:\n            return\n        doc_count = len(self.docs)\n        word_doc_freq = defaultdict(int)\n        \n        for _, doc in self.docs:\n            words = set(self._tokenize(doc))\n            for word in words:\n                word_doc_freq[word] += 1\n        \n        for word in self.vocab:\n            self.idf_cache[word] = math.log(doc_count / max(word_doc_freq[word], 1))\n    \n    def _tfidf_vector(self, text):\n        words = self._tokenize(text)\n        tf = Counter(words)\n        doc_len = len(words)\n        \n        vector = []\n        for word in sorted(self.vocab):\n            tf_score = tf.get(word, 0) / max(doc_len, 1)\n            idf_score = self.idf_cache.get(word, 0)\n            vector.append(tf_score * idf_score)\n        return vector\n    \n    def _cosine_similarity(self, vec1, vec2):\n        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n        norm1 = math.sqrt(sum(x * x for x in vec1))\n        norm2 = math.sqrt(sum(x * x for x in vec2))\n        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0\n    \n    def build_index(self):\n        self._compute_idf()\n        self.doc_vectors = []\n        for doc_id, doc in self.docs:\n            vector = self._tfidf_vector(doc)\n            self.doc_vectors.append((doc_id, vector))\n    \n    def search(self, query, top_k=None):\n        if not self.doc_vectors:\n            self.build_index()\n        \n        query_vector = self._tfidf_vector(query)\n        scores = []\n        \n        for (doc_id, doc_vector), (_, original_doc) in zip(self.doc_vectors, self.docs):\n            similarity = self._cosine_similarity(query_vector, doc_vector)\n            scores.append((doc_id, similarity, original_doc))\n        \n        scores.sort(key=lambda x: -x[1])\n        return scores[:top_k] if top_k else scores\n    \n    def _invalidate_cache(self):\n        self.idf_cache.clear()\n        self.doc_vectors.clear()\n\n# Test the semantic search system\nindex = SemanticIndex()\ndocs = [\n    'agent memory retrieval system',\n    'semantic search with vectors',\n    'policy engine rule processing',\n    'agent memory search optimization',\n    'knowledge base semantic indexing',\n    'vector similarity computation'\n]\n\nfor doc in docs:\n    index.add_document(doc)\n\nindex.build_index()\n\n# Test queries\ntest_queries = ['agent memory', 'semantic vectors', 'policy rules']\nfor query in tes",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "a594919179a0",
        "source_paper": "2602.15029v1",
        "created_at": "2026-02-18T07:49:27.782945"
      },
      {
        "id": "1f511ed057d2",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "6c69d6c54b2df932",
        "code_snippet": "def solution(n, k, a):\n    # Sort the array to make it easier to find the minimum operations\n    a.sort()\n    \n    min_operations = float('inf')\n    \n    # Try each possible target value from the array\n    for target in a:\n        operations = 0\n        temp_a = a[:]\n        \n        # Count operations needed to make at least k elements equal to target\n        equal_count = 0\n        \n        for i in range(n):\n            if temp_a[i] == target:\n                equal_count += 1\n            elif temp_a[i] < target:\n                # Can only increase values\n                operations += target - temp_a[i]\n                equal_count += 1\n        \n        # If we can make at least k elements equal to target\n        if equal_count >= k:\n            min_operations = min(min_operations, operations)\n    \n    # Also try making elements equal to values that might not be in the array\n    # but could be optimal (like the maximum value we might need)\n    max_val = max(a)\n    for target in range(1, max_val + 1):\n        operations = 0\n        equal_count = 0\n        \n        for val in a:\n            if val == target:\n                equal_count += 1\n            elif val < target:\n                operations += target - val\n                equal_count += 1\n        \n        if equal_count >= k:\n            min_operations = min(min_operations, operations)\n    \n    return min_operations",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "ff45ce1c7fa1",
        "source_paper": "2602.15010v1",
        "created_at": "2026-02-18T07:49:51.484764"
      },
      {
        "id": "46bfa91c2439",
        "strategy_name": "Cosine Similarity Ranker_gen1",
        "module": "knowledge.semantic",
        "code_hash": "50ef0ed687556d20",
        "code_snippet": "import math\nimport re\nfrom collections import Counter, defaultdict\n\nclass SemanticRanker:\n    def __init__(self, docs=None):\n        self.docs = docs or []\n        self.vocab = set()\n        self.idf_cache = {}\n        self._build_vocab()\n    \n    def _build_vocab(self):\n        for doc in self.docs:\n            self.vocab.update(self._tokenize(doc))\n        self.vocab = sorted(self.vocab)\n        self._compute_idf()\n    \n    def _tokenize(self, text):\n        return re.findall(r'\\b[a-z]+\\b', text.lower())\n    \n    def _compute_idf(self):\n        doc_freq = defaultdict(int)\n        for doc in self.docs:\n            words = set(self._tokenize(doc))\n            for word in words:\n                doc_freq[word] += 1\n        \n        n_docs = len(self.docs)\n        for word in self.vocab:\n            self.idf_cache[word] = math.log(n_docs / max(doc_freq[word], 1))\n    \n    def _tfidf_vector(self, text):\n        words = self._tokenize(text)\n        tf = Counter(words)\n        doc_len = max(len(words), 1)\n        \n        vector = []\n        for word in self.vocab:\n            tf_score = tf.get(word, 0) / doc_len\n            idf_score = self.idf_cache.get(word, 0)\n            vector.append(tf_score * idf_score)\n        return vector\n    \n    def _cosine_similarity(self, vec_a, vec_b):\n        dot_product = sum(a * b for a, b in zip(vec_a, vec_b))\n        norm_a = math.sqrt(sum(a * a for a in vec_a))\n        norm_b = math.sqrt(sum(b * b for b in vec_b))\n        return dot_product / (norm_a * norm_b) if norm_a and norm_b else 0.0\n    \n    def rank_documents(self, query):\n        query_vec = self._tfidf_vector(query)\n        scores = []\n        \n        for i, doc in enumerate(self.docs):\n            doc_vec = self._tfidf_vector(doc)\n            similarity = self._cosine_similarity(query_vec, doc_vec)\n            scores.append((i, round(similarity, 4)))\n        \n        return sorted(scores, key=lambda x: -x[1])\n\n# Test the semantic ranker\ndocs = ['agent memory retrieval system', 'semantic search vectors database',\n        'policy engine rules framework', 'agent memory search optimization',\n        'neural network embeddings', 'knowledge graph traversal']\n\nranker = SemanticRanker(docs)\nquery = 'agent memory'\nrankings = ranker.rank_documents(query)\n\nassert rankings[0][1] > rankings[-1][1]\nassert len(rankings) == len(docs)\n\nprint(f'Query: \"{query}\"')\nprint(f'Rankings: {rankings}')\nprint('PASS: Enhanced semantic ranker with TF-IDF validated')",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "bfdcf6f40109",
        "source_paper": "2602.15021v1",
        "created_at": "2026-02-18T07:53:35.525060"
      },
      {
        "id": "3c43d5f4c5b4",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "81321c2c6cee2c02",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = max(self.relation_weights[rel], weight)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = {\n                'score': round(score, 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    new_path = path + [rel]\n                    relevance_boost = 1.0 + (0.1 * self._calculate_relevance(rel, new_path))\n                    new_score = score * w * decay * relevance_boost\n                    queue.append((dst, new_score, depth + 1, new_path))\n        \n        return visited\n    \n    def _calculate_relevance(self, relation, path):\n        \"\"\"Calculate relevance based on relation frequency and path coherence\"\"\"\n        base_relevance = math.log(self.relation_weights[relation] + 1)\n        path_coherence = len(set(path)) / max(len(path), 1)  # Diversity penalty\n        return base_relevance * path_coherence\n    \n    def find_paths(self, start, end, max_depth=3):\n        \"\"\"Find all paths between two nodes\"\"\"\n        paths = []\n        queue = deque([(start, [start], 1.0, 0)])\n        \n        while queue:\n            node, path, score, depth = queue.popleft()\n            \n            if depth > max_depth:\n                continue\n                \n            if node == end:\n                paths.append({\n                    'path': path,\n                    'score': round(score, 4),\n                    'length': len(path) - 1\n                })\n                continue\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:\n                    queue.append((dst, path + [dst], score * w, depth + 1))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighbors(self, node, bidirectional=True):\n        \"\"\"Get all neighboring nodes with relationship info\"\"\"\n        neighbors = {}\n        \n        # Outgoing edges\n        for dst, rel, w in self.edg",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "b901662ab009",
        "source_paper": "2602.15008v1",
        "created_at": "2026-02-18T07:53:53.157672"
      },
      {
        "id": "19bf2e188cfc",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "1102c0895c61d240",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_weights = defaultdict(lambda: 1.0)\n        self.relation_types = set()\n    \n    def add(self, src, rel, dst, weight=1.0, bidirectional=False):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        if bidirectional:\n            self.edges[dst].append((src, rel, weight))\n            self.reverse_edges[src].append((dst, rel, weight))\n    \n    def set_node_weight(self, node, weight):\n        self.node_weights[node] = weight\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01, relation_filter=None):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = {\n                'score': round(score * self.node_weights[node], 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if relation_filter and rel not in relation_filter:\n                    continue\n                if dst not in [p[0] for p in path]:  # Avoid cycles\n                    new_path = path + [(node, rel)]\n                    queue.append((dst, score * w * decay, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3, min_score=0.01):\n        paths = []\n        queue = deque([(start, 1.0, 0, [(start, None)])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if depth > max_depth or score < min_score:\n                continue\n                \n            if node == end:\n                paths.append({\n                    'path': path,\n                    'score': round(score, 4),\n                    'length': depth\n                })\n                continue\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in [p[0] for p in path]:\n                    new_path = path + [(dst, rel)]\n                    queue.append((dst, score * w * 0.9, depth + 1, new_path))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighbors(self, node, direction='out'):\n        if direction == 'out':\n            return self.edges.get(node, [])\n        elif direction == 'in':\n            return self.reverse_edges.get(node, [])\n        else:\n            return self.edges.get(node, []) + self.reverse_edges.get(node, [])\n\ng = KnowledgeGraph()\ng.add('agent', 'uses', 'memory', 0.9)\ng.add('memory', 'contains', 'facts', 0.8)\ng.add('age",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "cd0e72c93500",
        "source_paper": "2602.15803v1",
        "created_at": "2026-02-18T08:02:30.982375"
      },
      {
        "id": "0a5e9cd5d4c9",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "2c8265781d2609ce",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = weight\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = {}\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited and visited[node] >= score:\n                continue\n            if depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(score, 4)\n            paths[node] = path + [node]\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    new_score = score * w * (decay ** depth)\n                    queue.append((dst, new_score, depth + 1, path + [node]))\n        \n        return {'scores': visited, 'paths': paths}\n    \n    def find_connections(self, node1, node2, max_depth=4):\n        \"\"\"Find all paths between two nodes\"\"\"\n        paths = []\n        queue = deque([(node1, [node1], 1.0, 0)])\n        \n        while queue:\n            current, path, score, depth = queue.popleft()\n            \n            if current == node2:\n                paths.append({'path': path, 'score': round(score, 4)})\n                continue\n            \n            if depth >= max_depth:\n                continue\n                \n            for dst, rel, w in self.edges.get(current, []):\n                if dst not in path:\n                    new_score = score * w * (0.8 ** depth)\n                    queue.append((dst, path + [dst], new_score, depth + 1))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighbors(self, node, relation_filter=None):\n        \"\"\"Get immediate neighbors with optional relation filtering\"\"\"\n        neighbors = []\n        for dst, rel, w in self.edges.get(node, []):\n            if not relation_filter or rel in relation_filter:\n                neighbors.append({'node': dst, 'relation': rel, 'weight': w})\n        return neighbors\n    \n    def cluster_by_type(self):\n        \"\"\"Group nodes by their types\"\"\"\n        clusters = defaultdict(list)\n        for node, node_type in self.node_types.items():\n            clusters[node_type].append(node)\n        return dict(clusters)\n\n# Test the enhanced knowledge graph\ng = KnowledgeGraph()\ng.add('agent', 'uses', 'memor",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "a48f15292d1e",
        "source_paper": "2602.15811v1",
        "created_at": "2026-02-18T08:02:47.452140"
      },
      {
        "id": "2ed8556b2686",
        "strategy_name": "Cosine Similarity Ranker_gen1_gen2",
        "module": "knowledge.semantic",
        "code_hash": "9b4c79a0207cf72b",
        "code_snippet": "import math\nimport re\nfrom collections import Counter, defaultdict\nfrom functools import lru_cache\n\nclass SemanticRanker:\n    def __init__(self, docs=None, alpha=0.5):\n        self.docs = docs or []\n        self.alpha = alpha  # BM25 parameter\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_vectors = []\n        self.avg_doc_len = 0\n        self._build_index()\n    \n    def _build_index(self):\n        if not self.docs:\n            return\n        \n        # Build vocabulary and compute document statistics\n        doc_lengths = []\n        for doc in self.docs:\n            tokens = self._tokenize(doc)\n            self.vocab.update(tokens)\n            doc_lengths.append(len(tokens))\n        \n        self.vocab = sorted(self.vocab)\n        self.avg_doc_len = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 0\n        self._compute_idf()\n        self._precompute_vectors()\n    \n    @lru_cache(maxsize=1000)\n    def _tokenize(self, text):\n        # Enhanced tokenization with stemming-like suffix removal\n        tokens = re.findall(r'\\b[a-z]+\\b', text.lower())\n        normalized = []\n        for token in tokens:\n            # Simple suffix removal for better matching\n            if len(token) > 4:\n                if token.endswith(('ing', 'ion', 'tion')):\n                    token = token[:-3] if token.endswith('ing') else token[:-4]\n                elif token.endswith(('ed', 'er', 'ly')):\n                    token = token[:-2]\n            normalized.append(token)\n        return tuple(normalized)  # Return tuple for caching\n    \n    def _compute_idf(self):\n        doc_freq = defaultdict(int)\n        for doc in self.docs:\n            words = set(self._tokenize(doc))\n            for word in words:\n                doc_freq[word] += 1\n        \n        n_docs = len(self.docs)\n        for word in self.vocab:\n            # Enhanced IDF with smoothing\n            df = doc_freq[word]\n            self.idf_cache[word] = math.log((n_docs - df + 0.5) / (df + 0.5))\n    \n    def _precompute_vectors(self):\n        # Pre-compute document vectors for efficiency\n        self.doc_vectors = []\n        for doc in self.docs:\n            self.doc_vectors.append(self._compute_vector(doc))\n    \n    def _compute_vector(self, text):\n        words = list(self._tokenize(text))\n        tf = Counter(words)\n        doc_len = len(words)\n        \n        vector = []\n        for word in self.vocab:\n            if word in tf:\n                # BM25-style scoring\n                tf_score = tf[word]\n                idf_score = self.idf_cache.get(word, 0)\n                \n                # BM25 term frequency normalization\n                normalized_tf = (tf_score * (1.2 + 1)) / (\n                    tf_score + 1.2 * (1 - self.alpha + self.alpha * (doc_len / max(self.avg_doc_len, 1)))\n                )\n                \n                vector.append(normalized_tf * idf_score)\n            else:\n                vector.append(0.0)\n        \n        return vector\n    \n    d",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "46bfa91c2439",
        "source_paper": "2602.15021v1",
        "created_at": "2026-02-18T08:07:16.630953"
      },
      {
        "id": "722735ed6ed6",
        "strategy_name": "Weighted Graph Traverser_gen1_gen2",
        "module": "knowledge.graph",
        "code_hash": "2671c0b4fcfa5a8f",
        "code_snippet": "import collections\nfrom typing import List, Dict, Set, Tuple, Optional\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.nodes = {}\n        self.edges = collections.defaultdict(set)\n        self.reverse_edges = collections.defaultdict(set)\n        self.node_attributes = collections.defaultdict(dict)\n        self.edge_attributes = collections.defaultdict(dict)\n    \n    def add_node(self, node_id: str, attributes: Optional[Dict] = None):\n        \"\"\"Add a node with optional attributes to the knowledge graph.\"\"\"\n        self.nodes[node_id] = True\n        if attributes:\n            self.node_attributes[node_id].update(attributes)\n    \n    def add_edge(self, source: str, target: str, relation: str = \"related\", attributes: Optional[Dict] = None):\n        \"\"\"Add a directed edge between two nodes with optional attributes.\"\"\"\n        # Ensure nodes exist\n        if source not in self.nodes:\n            self.add_node(source)\n        if target not in self.nodes:\n            self.add_node(target)\n        \n        edge_key = (source, target, relation)\n        self.edges[source].add((target, relation))\n        self.reverse_edges[target].add((source, relation))\n        \n        if attributes:\n            self.edge_attributes[edge_key].update(attributes)\n    \n    def get_neighbors(self, node_id: str, relation: Optional[str] = None) -> Set[str]:\n        \"\"\"Get all neighbors of a node, optionally filtered by relation type.\"\"\"\n        if node_id not in self.nodes:\n            return set()\n        \n        neighbors = set()\n        for target, rel in self.edges[node_id]:\n            if relation is None or rel == relation:\n                neighbors.add(target)\n        return neighbors\n    \n    def find_path(self, start: str, end: str, max_depth: int = 5) -> Optional[List[str]]:\n        \"\"\"Find shortest path between two nodes using BFS.\"\"\"\n        if start not in self.nodes or end not in self.nodes:\n            return None\n        \n        if start == end:\n            return [start]\n        \n        queue = collections.deque([(start, [start])])\n        visited = {start}\n        \n        while queue:\n            current, path = queue.popleft()\n            \n            if len(path) > max_depth:\n                continue\n            \n            for neighbor, _ in self.edges[current]:\n                if neighbor == end:\n                    return path + [neighbor]\n                \n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append((neighbor, path + [neighbor]))\n        \n        return None\n    \n    def get_subgraph(self, center_node: str, radius: int = 2) -> Dict:\n        \"\"\"Extract a subgraph around a center node within given radius.\"\"\"\n        if center_node not in self.nodes:\n            return {\"nodes\": [], \"edges\": []}\n        \n        visited_nodes = set()\n        queue = collections.deque([(center_node, 0)])\n        visited_nodes.add(center_node)\n        \n        while queue:\n            c",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "1f511ed057d2",
        "source_paper": "2602.15010v1",
        "created_at": "2026-02-18T08:07:37.162103"
      },
      {
        "id": "1dde54d3bc1c",
        "strategy_name": "Cosine Similarity Ranker_gen1",
        "module": "knowledge.semantic",
        "code_hash": "f23a6afb913ce14e",
        "code_snippet": "import math\nimport re\nfrom collections import Counter, defaultdict\n\nclass SemanticIndex:\n    def __init__(self):\n        self.docs = {}\n        self.vocab = set()\n        self.idf_cache = {}\n        \n    def add_document(self, doc_id, text):\n        words = self._tokenize(text)\n        self.docs[doc_id] = words\n        self.vocab.update(words)\n        self.idf_cache.clear()\n    \n    def _tokenize(self, text):\n        return re.findall(r'\\b[a-z]+\\b', text.lower())\n    \n    def _compute_idf(self):\n        if self.idf_cache:\n            return self.idf_cache\n        \n        doc_freq = defaultdict(int)\n        for words in self.docs.values():\n            for word in set(words):\n                doc_freq[word] += 1\n        \n        total_docs = len(self.docs)\n        self.idf_cache = {word: math.log(total_docs / freq) \n                         for word, freq in doc_freq.items()}\n        return self.idf_cache\n    \n    def _tfidf_vector(self, words):\n        tf = Counter(words)\n        idf = self._compute_idf()\n        doc_len = len(words)\n        \n        vector = {}\n        for word in self.vocab:\n            tf_score = tf.get(word, 0) / max(doc_len, 1)\n            idf_score = idf.get(word, 0)\n            vector[word] = tf_score * idf_score\n        return vector\n    \n    def cosine_similarity(self, vec1, vec2):\n        dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) for w in self.vocab)\n        norm1 = math.sqrt(sum(v*v for v in vec1.values()))\n        norm2 = math.sqrt(sum(v*v for v in vec2.values()))\n        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0\n    \n    def search(self, query, top_k=None):\n        query_words = self._tokenize(query)\n        query_vec = self._tfidf_vector(query_words)\n        \n        scores = []\n        for doc_id, doc_words in self.docs.items():\n            doc_vec = self._tfidf_vector(doc_words)\n            similarity = self.cosine_similarity(query_vec, doc_vec)\n            scores.append((doc_id, similarity))\n        \n        scores.sort(key=lambda x: -x[1])\n        return scores[:top_k] if top_k else scores\n\n# Test the semantic index\nindex = SemanticIndex()\ndocs = {\n    0: 'agent memory retrieval system',\n    1: 'semantic search vectors embedding',\n    2: 'policy engine rules execution',\n    3: 'agent memory search optimization',\n    4: 'knowledge graph traversal'\n}\n\nfor doc_id, text in docs.items():\n    index.add_document(doc_id, text)\n\nresults = index.search('agent memory', top_k=3)\nassert results[0][1] > results[-1][1]\nassert len(results) == 3\n\nprint(f'Top results: {[(doc_id, round(score, 3)) for doc_id, score in results]}')\nprint('PASS: Enhanced semantic search with TF-IDF validated')",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "bfdcf6f40109",
        "source_paper": "2602.15021v1",
        "created_at": "2026-02-18T08:11:25.993677"
      },
      {
        "id": "108117ff3123",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "bfc86b21fd9f5f96",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_weights = defaultdict(lambda: 1.0)\n        self.relation_types = set()\n    \n    def add(self, src, rel, dst, weight=1.0, bidirectional=False):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        if bidirectional:\n            self.edges[dst].append((src, rel, weight))\n            self.reverse_edges[src].append((dst, rel, weight))\n    \n    def set_node_weight(self, node, weight):\n        self.node_weights[node] = weight\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01, relation_filter=None):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = {\n                'score': round(score * self.node_weights[node], 4),\n                'depth': depth,\n                'path': path.copy()\n            }\n            \n            for dst, rel, weight in self.edges.get(node, []):\n                if relation_filter and rel not in relation_filter:\n                    continue\n                if dst not in visited:\n                    new_path = path + [rel]\n                    new_score = score * weight * decay\n                    queue.append((dst, new_score, depth + 1, new_path))\n        \n        return visited\n    \n    def find_paths(self, start, end, max_depth=3):\n        paths = []\n        queue = deque([(start, [start], [])])\n        \n        while queue:\n            node, path, relations = queue.popleft()\n            \n            if len(path) > max_depth + 1:\n                continue\n                \n            if node == end and len(path) > 1:\n                paths.append({\n                    'nodes': path,\n                    'relations': relations,\n                    'strength': self._calculate_path_strength(path, relations)\n                })\n                continue\n            \n            for dst, rel, weight in self.edges.get(node, []):\n                if dst not in path:\n                    queue.append((dst, path + [dst], relations + [rel]))\n        \n        return sorted(paths, key=lambda x: x['strength'], reverse=True)\n    \n    def _calculate_path_strength(self, nodes, relations):\n        strength = 1.0\n        for i, node in enumerate(nodes):\n            strength *= self.node_weights[node]\n            if i < len(relations):\n                # Find edge weight\n                for dst, rel, weight in self.edges.get(node, []):\n                    if dst == nodes[i + 1] and rel == relations[i]:\n                        strength *= weight\n                        break\n  ",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "a45d0fb6620c",
        "source_paper": "2602.15824v1",
        "created_at": "2026-02-18T08:11:42.557903"
      },
      {
        "id": "19c32f388eb6",
        "strategy_name": "Weighted Graph Traverser_gen1_gen2",
        "module": "knowledge.graph",
        "code_hash": "a86515111967aec6",
        "code_snippet": "import collections\nfrom typing import List, Dict, Set, Tuple, Optional\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.nodes = {}\n        self.edges = collections.defaultdict(set)\n        self.reverse_edges = collections.defaultdict(set)\n        self.node_attributes = collections.defaultdict(dict)\n        self.edge_attributes = collections.defaultdict(dict)\n        \n    def add_node(self, node_id: str, attributes: Optional[Dict] = None):\n        \"\"\"Add a node to the knowledge graph with optional attributes.\"\"\"\n        self.nodes[node_id] = True\n        if attributes:\n            self.node_attributes[node_id].update(attributes)\n    \n    def add_edge(self, source: str, target: str, relation: str, attributes: Optional[Dict] = None):\n        \"\"\"Add a directed edge between nodes with a relation type.\"\"\"\n        # Ensure nodes exist\n        if source not in self.nodes:\n            self.add_node(source)\n        if target not in self.nodes:\n            self.add_node(target)\n            \n        edge_key = (source, target, relation)\n        self.edges[source].add((target, relation))\n        self.reverse_edges[target].add((source, relation))\n        \n        if attributes:\n            self.edge_attributes[edge_key].update(attributes)\n    \n    def find_paths(self, start: str, end: str, max_depth: int = 3) -> List[List[Tuple[str, str]]]:\n        \"\"\"Find all paths between two nodes up to max_depth.\"\"\"\n        if start not in self.nodes or end not in self.nodes:\n            return []\n        \n        paths = []\n        \n        def dfs(current, target, path, visited, depth):\n            if depth > max_depth:\n                return\n            if current == target and path:\n                paths.append(path[:])\n                return\n            \n            for neighbor, relation in self.edges[current]:\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    path.append((neighbor, relation))\n                    dfs(neighbor, target, path, visited, depth + 1)\n                    path.pop()\n                    visited.remove(neighbor)\n        \n        dfs(start, end, [], {start}, 0)\n        return paths\n    \n    def get_neighbors(self, node_id: str, relation_filter: Optional[str] = None) -> Set[str]:\n        \"\"\"Get all neighboring nodes, optionally filtered by relation type.\"\"\"\n        if node_id not in self.nodes:\n            return set()\n        \n        neighbors = set()\n        for neighbor, relation in self.edges[node_id]:\n            if relation_filter is None or relation == relation_filter:\n                neighbors.add(neighbor)\n        \n        return neighbors\n    \n    def query_subgraph(self, center_node: str, radius: int = 1) -> Dict:\n        \"\"\"Extract a subgraph around a center node within given radius.\"\"\"\n        if center_node not in self.nodes:\n            return {}\n        \n        subgraph_nodes = {center_node}\n        current_layer = {center_node}\n        \n        for _ in range(ra",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "1f511ed057d2",
        "source_paper": "2602.15010v1",
        "created_at": "2026-02-18T08:16:01.603573"
      },
      {
        "id": "e738470f2cc2",
        "strategy_name": "Weighted Graph Traverser_gen1_gen2",
        "module": "knowledge.graph",
        "code_hash": "a67bff9de1e907fc",
        "code_snippet": "from collections import defaultdict, deque\nimport json\nimport math\nimport heapq\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_metadata = defaultdict(dict)\n        self.relation_types = set()\n        self.node_importance = defaultdict(float)\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, metadata=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        self.relation_types.add(rel)\n        \n        # Update node importance based on connectivity\n        self.node_importance[src] += 0.1\n        self.node_importance[dst] += 0.1\n        \n        # Learn relation weights\n        self.relation_weights[rel] = max(self.relation_weights[rel], weight)\n        \n        if metadata:\n            self.node_metadata[src].update(metadata)\n            self.node_metadata[dst].update(metadata.get('target_meta', {}))\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        # Use priority queue for better path exploration\n        heap = [(-1.0, 0, start, [])]\n        paths = {}\n        \n        while heap:\n            neg_score, depth, node, path = heapq.heappop(heap)\n            score = -neg_score\n            \n            if node in visited or depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(score, 4)\n            paths[node] = path + [node]\n            \n            # Consider node importance in scoring\n            node_boost = 1.0 + self.node_importance[node] * 0.1\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in visited:\n                    rel_boost = self.relation_weights[rel]\n                    new_score = score * w * rel_boost * node_boost * (decay ** depth)\n                    heapq.heappush(heap, (-new_score, depth + 1, dst, path + [node]))\n        \n        return {'scores': visited, 'paths': paths}\n    \n    def find_connections(self, src, dst, max_depth=4):\n        \"\"\"Find all paths between two nodes with enhanced scoring\"\"\"\n        paths = []\n        queue = deque([(src, [src], 1.0, 0, set([src]))])\n        \n        while queue:\n            node, path, score, depth, visited_set = queue.popleft()\n            \n            if depth > max_depth:\n                continue\n                \n            if node == dst:\n                # Enhanced path scoring with diversity bonus\n                diversity_bonus = len(set(path)) / len(path)\n                final_score = score * diversity_bonus * (1.0 + self.node_importance[dst] * 0.2)\n                paths.append({\n                    'path': path, \n                    'score': round(final_score, 4),\n                    'relations': self._extract_relations(path)\n                })\n                continue\n            \n         ",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "804d4639c205",
        "source_paper": "2602.15015v1",
        "created_at": "2026-02-18T08:16:22.805646"
      },
      {
        "id": "06246d59cf96",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "259119d244d597aa",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = weight\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = defaultdict(list)\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited and visited[node] >= score:\n                continue\n            if depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(max(visited.get(node, 0), score), 4)\n            current_path = path + [node]\n            paths[node].append((current_path, score))\n            \n            for dst, rel, w in self.edges.get(node, []):\n                # Adaptive decay based on relation type\n                rel_decay = decay * self.relation_weights.get(rel, 1.0)\n                new_score = score * w * rel_decay\n                queue.append((dst, new_score, depth + 1, current_path))\n        \n        return visited, dict(paths)\n    \n    def find_connections(self, node1, node2, max_depth=4):\n        \"\"\"Find all paths between two nodes\"\"\"\n        paths = []\n        queue = deque([(node1, [node1], 1.0, 0)])\n        \n        while queue:\n            current, path, score, depth = queue.popleft()\n            \n            if current == node2:\n                paths.append((path, round(score, 4)))\n                continue\n            if depth >= max_depth:\n                continue\n                \n            for dst, rel, w in self.edges.get(current, []):\n                if dst not in path:  # Avoid cycles\n                    new_path = path + [dst]\n                    new_score = score * w * 0.8\n                    queue.append((dst, new_path, new_score, depth + 1))\n        \n        return sorted(paths, key=lambda x: x[1], reverse=True)\n    \n    def get_neighbors(self, node, bidirectional=True):\n        \"\"\"Get immediate neighbors with relationship info\"\"\"\n        neighbors = {}\n        for dst, rel, w in self.edges.get(node, []):\n            neighbors[dst] = {'relation': rel, 'weight': w, 'direction': 'outgoing'}\n        \n        if bidirectional:\n            for src, rel, w in self.reverse_edges.get(node, []):\n                if src not in neighbors:\n                    neighbors[src] = {'relation': rel, 'weight': w, 'direction': 'incoming'}\n        \n        return neighbors\n\n# Test",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "c385ab7a34fb",
        "source_paper": "2602.15025v1",
        "created_at": "2026-02-18T08:26:35.191534"
      },
      {
        "id": "c3fa5b86c42c",
        "strategy_name": "Cosine Similarity Ranker_gen1_gen2",
        "module": "knowledge.semantic",
        "code_hash": "998bef6812d22bfe",
        "code_snippet": "import math\nimport re\nfrom collections import Counter, defaultdict\nfrom functools import lru_cache\n\nclass SemanticIndex:\n    def __init__(self):\n        self.docs = {}\n        self.doc_vectors = {}\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_norms = {}\n        \n    def add_document(self, doc_id, text):\n        words = self._tokenize(text)\n        self.docs[doc_id] = words\n        self.vocab.update(words)\n        self._invalidate_cache()\n    \n    def _invalidate_cache(self):\n        self.idf_cache.clear()\n        self.doc_vectors.clear()\n        self.doc_norms.clear()\n    \n    def _tokenize(self, text):\n        # Enhanced tokenization with better word boundaries and filtering\n        words = re.findall(r'\\b[a-zA-Z]{2,}\\b', text.lower())\n        return [w for w in words if len(w) >= 2]\n    \n    def _compute_idf(self):\n        if self.idf_cache:\n            return self.idf_cache\n        \n        doc_freq = defaultdict(int)\n        for words in self.docs.values():\n            for word in set(words):\n                doc_freq[word] += 1\n        \n        total_docs = len(self.docs)\n        # Smoothed IDF to handle edge cases better\n        self.idf_cache = {word: math.log((total_docs + 1) / (freq + 1)) + 1\n                         for word, freq in doc_freq.items()}\n        return self.idf_cache\n    \n    def _tfidf_vector(self, words, use_cache=True, doc_id=None):\n        if use_cache and doc_id and doc_id in self.doc_vectors:\n            return self.doc_vectors[doc_id]\n            \n        tf = Counter(words)\n        idf = self._compute_idf()\n        doc_len = len(words)\n        \n        vector = {}\n        for word in self.vocab:\n            if word in tf:  # Only compute for words that exist\n                tf_score = tf[word] / max(doc_len, 1)\n                idf_score = idf.get(word, 0)\n                tfidf_score = tf_score * idf_score\n                if tfidf_score > 0:  # Sparse representation\n                    vector[word] = tfidf_score\n        \n        if use_cache and doc_id:\n            self.doc_vectors[doc_id] = vector\n            \n        return vector\n    \n    def _vector_norm(self, vector, doc_id=None):\n        if doc_id and doc_id in self.doc_norms:\n            return self.doc_norms[doc_id]\n            \n        norm = math.sqrt(sum(v*v for v in vector.values()))\n        \n        if doc_id:\n            self.doc_norms[doc_id] = norm\n            \n        return norm\n    \n    def cosine_similarity(self, vec1, vec2):\n        # Optimized sparse vector dot product\n        if not vec1 or not vec2:\n            return 0.0\n            \n        dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) \n                         for w in vec1.keys() & vec2.keys())\n        \n        norm1 = self._vector_norm(vec1)\n        norm2 = self._vector_norm(vec2)\n        \n        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0\n    \n    def search(self, query, top_k=5, min_score=0.01):\n        if not self.docs:\n            r",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "1dde54d3bc1c",
        "source_paper": "2602.15021v1",
        "created_at": "2026-02-18T08:32:44.462860"
      },
      {
        "id": "3e6bde5cce42",
        "strategy_name": "Weighted Graph Traverser_gen1",
        "module": "knowledge.graph",
        "code_hash": "9f1d904328924004",
        "code_snippet": "from collections import defaultdict, deque\nimport math\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n    \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = max(self.relation_weights[rel], weight)\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = {}\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            if node in visited and visited[node] >= score:\n                continue\n            if depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(score, 4)\n            paths[node] = path + [node]\n            \n            for dst, rel, w in self.edges.get(node, []):\n                if dst not in path:  # Prevent cycles\n                    new_score = score * w * (decay ** depth)\n                    queue.append((dst, new_score, depth + 1, path + [node]))\n        \n        return {'scores': visited, 'paths': paths}\n    \n    def find_connections(self, src, dst, max_depth=4):\n        \"\"\"Find all paths between two nodes\"\"\"\n        paths = []\n        queue = deque([(src, [src], 1.0, 0)])\n        \n        while queue:\n            node, path, score, depth = queue.popleft()\n            \n            if node == dst:\n                paths.append({'path': path, 'score': round(score, 4)})\n                continue\n            if depth >= max_depth:\n                continue\n                \n            for next_node, rel, weight in self.edges.get(node, []):\n                if next_node not in path:\n                    new_score = score * weight * 0.9\n                    queue.append((next_node, path + [next_node], new_score, depth + 1))\n        \n        return sorted(paths, key=lambda x: x['score'], reverse=True)\n    \n    def get_neighborhood(self, node, radius=2):\n        \"\"\"Get all nodes within radius distance\"\"\"\n        result = self.traverse(node, max_depth=radius)\n        neighborhood = {}\n        for n, score in result['scores'].items():\n            if n != node:\n                neighborhood[n] = {\n                    'score': score,\n                    'path': result['paths'][n],\n                    'distance': len(result['paths'][n]) - 1\n                }\n        return neighborhood\n    \n    def cluster_analysis(self):\n        \"\"\"Basic clustering based on connection strength\"\"\"\n        clusters = {}\n        processed = set()\n        \n        for node in self.edges.keys():\n  ",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "ff45ce1c7fa1",
        "source_paper": "2602.15010v1",
        "created_at": "2026-02-18T08:36:25.700391"
      },
      {
        "id": "d29fb278f18e",
        "strategy_name": "Softmax Diversity Scorer_gen1",
        "module": "knowledge.semantic",
        "code_hash": "177e536dc9da7dc7",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict\n\nclass SemanticScorer:\n    def __init__(self, temperature=0.3, diversity_weight=0.2):\n        self.temperature = max(temperature, 0.01)\n        self.diversity_weight = diversity_weight\n        self.history = defaultdict(float)\n    \n    def softmax_score(self, values, labels=None):\n        if not values: return []\n        \n        # Apply temperature scaling\n        exp_vals = [math.exp(v / self.temperature) for v in values]\n        total = sum(exp_vals)\n        base_scores = [v / total for v in exp_vals]\n        \n        # Apply diversity penalty if labels provided\n        if labels and self.diversity_weight > 0:\n            diversity_penalties = [self.history.get(label, 0) for label in labels]\n            max_penalty = max(diversity_penalties) if diversity_penalties else 0\n            \n            adjusted_vals = []\n            for i, val in enumerate(values):\n                penalty = diversity_penalties[i] / (max_penalty + 1e-6) if max_penalty > 0 else 0\n                adjusted_val = val - (self.diversity_weight * penalty)\n                adjusted_vals.append(adjusted_val)\n            \n            # Recalculate softmax with diversity adjustment\n            exp_vals = [math.exp(v / self.temperature) for v in adjusted_vals]\n            total = sum(exp_vals)\n            base_scores = [v / total for v in exp_vals]\n        \n        return base_scores\n    \n    def update_history(self, selected_label, decay=0.9):\n        # Decay all history values\n        for label in self.history:\n            self.history[label] *= decay\n        # Increment selected item\n        self.history[selected_label] += 1.0\n    \n    def semantic_similarity(self, text1, text2):\n        # Simple semantic similarity based on word overlap and length\n        words1 = set(text1.lower().split())\n        words2 = set(text2.lower().split())\n        \n        if not words1 or not words2:\n            return 0.0\n        \n        intersection = len(words1.intersection(words2))\n        union = len(words1.union(words2))\n        \n        # Jaccard similarity with length normalization\n        jaccard = intersection / union if union > 0 else 0\n        length_factor = min(len(words1), len(words2)) / max(len(words1), len(words2))\n        \n        return jaccard * length_factor\n\nscorer = SemanticScorer(temperature=0.3, diversity_weight=0.2)\n\n# Test basic softmax\nscores = scorer.softmax_score([0.9, 0.7, 0.4, 0.2, 0.1])\nassert abs(sum(scores) - 1.0) < 0.001\n\n# Test diversity scoring\nlabels = ['concept_a', 'concept_b', 'concept_c']\nvalues = [0.8, 0.7, 0.6]\n\ninitial_scores = scorer.softmax_score(values, labels)\nscorer.update_history('concept_a')\ndiverse_scores = scorer.softmax_score(values, labels)\n\n# Test semantic similarity\nsim = scorer.semantic_similarity(\"machine learning model\", \"learning algorithm model\")\nassert 0 <= sim <= 1\n\nprint(f'Softmax scores: {[round(s,3) for s in scores]}')\nprint(f'Diversity effect: {round(diverse_scores[0] ",
        "fitness_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 1,
        "parent_id": "7065b7cd4079",
        "source_paper": "2602.15017v1",
        "created_at": "2026-02-18T08:36:40.366268"
      },
      {
        "id": "0bf4b5faec55",
        "strategy_name": "Weighted Graph Traverser_gen1_gen2",
        "module": "knowledge.graph",
        "code_hash": "4f1eab57d94c4cbd",
        "code_snippet": "from collections import defaultdict, deque\nimport math\nimport json\nimport re\n\nclass KnowledgeGraph:\n    def __init__(self):\n        self.edges = defaultdict(list)\n        self.reverse_edges = defaultdict(list)\n        self.node_types = {}\n        self.relation_weights = defaultdict(lambda: 1.0)\n        self.node_attributes = defaultdict(dict)\n        self.relation_types = set()\n        self.centrality_cache = {}\n        \n    def add(self, src, rel, dst, weight=1.0, src_type=None, dst_type=None, **attributes):\n        self.edges[src].append((dst, rel, weight))\n        self.reverse_edges[dst].append((src, rel, weight))\n        if src_type:\n            self.node_types[src] = src_type\n        if dst_type:\n            self.node_types[dst] = dst_type\n        self.relation_weights[rel] = weight\n        self.relation_types.add(rel)\n        \n        # Store additional attributes\n        if attributes:\n            self.node_attributes[src].update(attributes.get('src_attrs', {}))\n            self.node_attributes[dst].update(attributes.get('dst_attrs', {}))\n        \n        # Invalidate centrality cache\n        self.centrality_cache.clear()\n    \n    def traverse(self, start, max_depth=3, decay=0.7, min_score=0.01, filter_types=None):\n        visited = {}\n        queue = deque([(start, 1.0, 0, [])])\n        paths = defaultdict(list)\n        \n        while queue:\n            node, score, depth, path = queue.popleft()\n            \n            # Type filtering\n            if filter_types and self.node_types.get(node) not in filter_types:\n                continue\n                \n            if node in visited and visited[node] >= score:\n                continue\n            if depth > max_depth or score < min_score:\n                continue\n                \n            visited[node] = round(max(visited.get(node, 0), score), 4)\n            current_path = path + [node]\n            paths[node].append((current_path, score))\n            \n            for dst, rel, w in self.edges.get(node, []):\n                # Adaptive decay with centrality boost\n                centrality_boost = self._get_centrality(dst)\n                rel_decay = decay * self.relation_weights.get(rel, 1.0) * (1 + centrality_boost * 0.1)\n                new_score = score * w * rel_decay\n                queue.append((dst, new_score, depth + 1, current_path))\n        \n        return visited, dict(paths)\n    \n    def find_connections(self, node1, node2, max_depth=4, strategy='shortest'):\n        \"\"\"Find paths between nodes with multiple strategies\"\"\"\n        paths = []\n        queue = deque([(node1, [node1], 1.0, 0)])\n        visited = set() if strategy == 'shortest' else None\n        \n        while queue:\n            current, path, score, depth = queue.popleft()\n            \n            if strategy == 'shortest' and current in visited:\n                continue\n            if visited is not None:\n                visited.add(current)\n                \n            if current == node2:\n                pa",
        "fitness_scores": [
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "06246d59cf96",
        "source_paper": "2602.15025v1",
        "created_at": "2026-02-18T08:41:24.927070"
      },
      {
        "id": "b0f59e2fe12d",
        "strategy_name": "Softmax Diversity Scorer_gen1_gen2",
        "module": "knowledge.semantic",
        "code_hash": "6b29239709acf62c",
        "code_snippet": "import math\nimport random\nimport re\nfrom collections import defaultdict, Counter\n\nclass SemanticScorer:\n    def __init__(self, temperature=0.3, diversity_weight=0.2, context_weight=0.15):\n        self.temperature = max(temperature, 0.01)\n        self.diversity_weight = diversity_weight\n        self.context_weight = context_weight\n        self.history = defaultdict(float)\n        self.context_memory = defaultdict(list)\n        self.word_embeddings = {}\n        self._build_simple_embeddings()\n    \n    def _build_simple_embeddings(self):\n        # Simple word co-occurrence based embeddings\n        common_patterns = {\n            'positive': ['good', 'great', 'excellent', 'amazing', 'wonderful', 'best'],\n            'negative': ['bad', 'terrible', 'awful', 'worst', 'horrible', 'poor'],\n            'technical': ['system', 'code', 'function', 'method', 'class', 'algorithm'],\n            'action': ['run', 'execute', 'process', 'handle', 'manage', 'control'],\n            'data': ['information', 'content', 'text', 'value', 'result', 'output']\n        }\n        \n        for category, words in common_patterns.items():\n            for word in words:\n                self.word_embeddings[word] = category\n    \n    def softmax_score(self, values, labels=None, context=None):\n        if not values: return []\n        \n        # Normalize input values\n        mean_val = sum(values) / len(values)\n        std_val = math.sqrt(sum((v - mean_val) ** 2 for v in values) / len(values))\n        if std_val > 0:\n            values = [(v - mean_val) / std_val for v in values]\n        \n        # Apply context boost if available\n        if context and labels and self.context_weight > 0:\n            context_scores = [self._context_relevance(label, context) for label in labels]\n            values = [v + self.context_weight * cs for v, cs in zip(values, context_scores)]\n        \n        # Apply temperature scaling\n        exp_vals = [math.exp(v / self.temperature) for v in values]\n        total = sum(exp_vals)\n        base_scores = [v / total for v in exp_vals]\n        \n        # Apply diversity penalty if labels provided\n        if labels and self.diversity_weight > 0:\n            diversity_penalties = [self.history.get(label, 0) for label in labels]\n            max_penalty = max(diversity_penalties) if diversity_penalties else 0\n            \n            if max_penalty > 0:\n                adjusted_vals = []\n                for i, val in enumerate(values):\n                    penalty = diversity_penalties[i] / (max_penalty + 1e-6)\n                    adjusted_val = val - (self.diversity_weight * penalty)\n                    adjusted_vals.append(adjusted_val)\n                \n                # Recalculate softmax with diversity adjustment\n                exp_vals = [math.exp(v / self.temperature) for v in adjusted_vals]\n                total = sum(exp_vals)\n                base_scores = [v / total for v in exp_vals]\n        \n        return base_scores\n    \n    def _context_relevance",
        "fitness_scores": [
          1.0,
          1.0
        ],
        "current_fitness": 1.0,
        "generation": 2,
        "parent_id": "d29fb278f18e",
        "source_paper": "2602.15017v1",
        "created_at": "2026-02-18T08:42:28.768171"
      }
    ]
  }
}