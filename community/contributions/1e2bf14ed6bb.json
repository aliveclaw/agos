{
  "instance_id": "1e2bf14ed6bb",
  "agos_version": "0.1.0",
  "contributed_at": "2026-02-18T09:19:09.114821",
  "cycles_completed": 222,
  "strategies_applied": [
    {
      "name": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in...",
      "module": "coordination",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15019v1",
          "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation"
        }
      ],
      "sandbox_passed": true,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:26:58.851386",
      "applied_count": 1
    },
    {
      "name": "Distributed Quantum Gaussian Processes for Multi-Agent Systems",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15006v1",
          "title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:26:58.851595",
      "applied_count": 1
    },
    {
      "name": "Cold-Start Personalization via Training-Free Priors from Structur...",
      "module": "knowledge.graph",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15012v1",
          "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:27:49.902012",
      "applied_count": 1
    },
    {
      "name": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through ...",
      "module": "intent.personas",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15028v1",
          "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:29:33.377896",
      "applied_count": 1
    },
    {
      "name": "Boundary Point Jailbreaking of Black-Box LLMs",
      "module": "intent",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15001v1",
          "title": "Boundary Point Jailbreaking of Black-Box LLMs"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-17T21:29:33.377943",
      "applied_count": 1
    },
    {
      "name": "Service Orchestration in the Computing Continuum: Structural Chal...",
      "module": "knowledge.manager",
      "parameters": {},
      "source_papers": [
        {
          "arxiv_id": "2602.15794v1",
          "title": "Service Orchestration in the Computing Continuum: Structural Challenges and Vision"
        }
      ],
      "sandbox_passed": false,
      "health_check_passed": true,
      "applied_at": "2026-02-18T02:17:35.741448",
      "applied_count": 1
    }
  ],
  "discovered_patterns": [
    {
      "name": "Message Channel Router",
      "module": "coordination",
      "code_snippet": "from collections import defaultdict\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n    def subscribe(self, agent, topics):\n        for t in topics:\n            self.subscribers[t].append(agent)\n    def send(self, topic, msg, sender):\n        self.history.append({'topic': topic, 'msg': msg, 'sender': sender})\n        receivers = self.subscribers.get(topic, [])\n        return [r for r in receivers if r !",
      "sandbox_output": "findings -> ['reviewer'], broadcast -> ['researcher', 'reviewer']\nPASS: Message channel router validated\n",
      "source_paper": "2602.15019v1"
    },
    {
      "name": "Strategy Selector",
      "module": "orchestration.planner",
      "code_snippet": "class StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        success_rate = self.successes / total\n        efficiency = 1.0 / (1 + self.total_tokens / max(total, 1) / 10000)\n        return round(0.7 * success_rate + 0.3 * efficiency, 3)\n\ndef select_strategy(records, task_size)",
      "sandbox_output": "Small task: solo (score=0.76)\nLarge task: parallel (score=0.675)\nPASS: Strategy selector validated\n",
      "source_paper": "2602.14993v1"
    }
  ],
  "meta_evolution": {
    "genomes": {
      "knowledge.semantic": {
        "component": "knowledge.semantic",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "temperature",
            "current": null,
            "default": 0.0,
            "min_val": 0.0,
            "max_val": 1.0,
            "param_type": "float",
            "description": "Softmax retrieval diversity"
          },
          {
            "name": "track_access",
            "current": null,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Access-based confidence tracking"
          },
          {
            "name": "relevance_threshold",
            "current": null,
            "default": 0.01,
            "min_val": 0.001,
            "max_val": 0.1,
            "param_type": "float",
            "description": "Minimum cosine similarity for results"
          },
          {
            "name": "confidence_decay_factor",
            "current": null,
            "default": 0.95,
            "min_val": 0.8,
            "max_val": 0.99,
            "param_type": "float",
            "description": "Confidence decay for unused knowledge"
          },
          {
            "name": "confidence_decay_days",
            "current": null,
            "default": 30,
            "min_val": 7,
            "max_val": 90,
            "param_type": "int",
            "description": "Days inactive before decay kicks in"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T09:18:16.906587",
        "mutations_applied": 0
      },
      "knowledge.graph": {
        "component": "knowledge.graph",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "default_traversal_depth",
            "current": 2,
            "default": 1,
            "min_val": 1,
            "max_val": 4,
            "param_type": "int",
            "description": "Default neighbor traversal hops"
          },
          {
            "name": "edge_weight_decay",
            "current": 1.0,
            "default": 0.99,
            "min_val": 0.9,
            "max_val": 1.0,
            "param_type": "float",
            "description": "Weight decay per consolidation cycle"
          }
        ],
        "fitness_score": 0.9219999999999999,
        "last_evaluated": "2026-02-18T09:18:16.906805",
        "mutations_applied": 2
      },
      "knowledge.consolidator": {
        "component": "knowledge.consolidator",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "older_than_hours",
            "current": 24,
            "default": 24,
            "min_val": 6,
            "max_val": 168,
            "param_type": "int",
            "description": "Consolidate events older than N hours"
          },
          {
            "name": "min_cluster_size",
            "current": 4,
            "default": 3,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Minimum events to form a summary"
          },
          {
            "name": "max_concurrent_writes",
            "current": 4,
            "default": 5,
            "min_val": 1,
            "max_val": 20,
            "param_type": "int",
            "description": "Semaphore limit for batch ops"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T09:18:16.906929",
        "mutations_applied": 65
      },
      "knowledge.loom": {
        "component": "knowledge.loom",
        "layer": "Semantic Work Substrate",
        "params": [
          {
            "name": "use_layered_recall",
            "current": true,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Priority-ordered layer retrieval"
          },
          {
            "name": "recall_limit",
            "current": 40,
            "default": 10,
            "min_val": 3,
            "max_val": 50,
            "param_type": "int",
            "description": "Default recall result limit"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T09:18:16.907038",
        "mutations_applied": 19
      },
      "intent.engine": {
        "component": "intent.engine",
        "layer": "Agent & Intent Intelligence",
        "params": [
          {
            "name": "default_strategy",
            "current": null,
            "default": "solo",
            "min_val": null,
            "max_val": null,
            "param_type": "str",
            "description": "Fallback coordination strategy"
          },
          {
            "name": "max_intent_tokens",
            "current": 370,
            "default": 500,
            "min_val": 200,
            "max_val": 1500,
            "param_type": "int",
            "description": "Token limit for intent classification"
          }
        ],
        "fitness_score": 0.75,
        "last_evaluated": "2026-02-18T09:18:16.907253",
        "mutations_applied": 1
      },
      "intent.personas": {
        "component": "intent.personas",
        "layer": "Agent & Intent Intelligence",
        "params": [
          {
            "name": "researcher_budget",
            "current": 150000,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Researcher agent token budget"
          },
          {
            "name": "coder_budget",
            "current": 238940,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Coder agent token budget"
          },
          {
            "name": "orchestrator_budget",
            "current": 256022,
            "default": 200000,
            "min_val": 50000,
            "max_val": 500000,
            "param_type": "int",
            "description": "Orchestrator agent token budget"
          },
          {
            "name": "researcher_max_turns",
            "current": 25,
            "default": 30,
            "min_val": 5,
            "max_val": 80,
            "param_type": "int",
            "description": "Researcher max turns"
          },
          {
            "name": "coder_max_turns",
            "current": 25,
            "default": 40,
            "min_val": 10,
            "max_val": 100,
            "param_type": "int",
            "description": "Coder max turns"
          },
          {
            "name": "orchestrator_max_turns",
            "current": 65,
            "default": 50,
            "min_val": 10,
            "max_val": 100,
            "param_type": "int",
            "description": "Orchestrator max turns"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T09:18:16.907368",
        "mutations_applied": 77
      },
      "orchestration.planner": {
        "component": "orchestration.planner",
        "layer": "Agent Orchestration & Workflow",
        "params": [
          {
            "name": "parallel_threshold",
            "current": 4,
            "default": 3,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Min subtasks to trigger parallel execution"
          },
          {
            "name": "pipeline_max_agents",
            "current": 6,
            "default": 5,
            "min_val": 2,
            "max_val": 10,
            "param_type": "int",
            "description": "Max agents in a pipeline"
          }
        ],
        "fitness_score": 0.75,
        "last_evaluated": "2026-02-18T09:18:16.907478",
        "mutations_applied": 2
      },
      "orchestration.runtime": {
        "component": "orchestration.runtime",
        "layer": "Agent Orchestration & Workflow",
        "params": [
          {
            "name": "max_concurrent_agents",
            "current": 49,
            "default": 50,
            "min_val": 5,
            "max_val": 200,
            "param_type": "int",
            "description": "Max agents running simultaneously"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T09:18:16.907584",
        "mutations_applied": 1
      },
      "policy.engine": {
        "component": "policy.engine",
        "layer": "Identity & Governance",
        "params": [
          {
            "name": "default_max_tokens",
            "current": null,
            "default": 200000,
            "min_val": 50000,
            "max_val": 1000000,
            "param_type": "int",
            "description": "Default agent token budget"
          },
          {
            "name": "default_max_turns",
            "current": null,
            "default": 50,
            "min_val": 10,
            "max_val": 200,
            "param_type": "int",
            "description": "Default agent turn limit"
          },
          {
            "name": "default_rate_limit",
            "current": null,
            "default": 60,
            "min_val": 10,
            "max_val": 200,
            "param_type": "int",
            "description": "Tool calls per minute"
          },
          {
            "name": "default_read_only",
            "current": null,
            "default": false,
            "min_val": null,
            "max_val": null,
            "param_type": "bool",
            "description": "Default read-only mode"
          }
        ],
        "fitness_score": 1.0,
        "last_evaluated": "2026-02-18T09:18:16.907741",
        "mutations_applied": 0
      },
      "events.bus": {
        "component": "events.bus",
        "layer": "Episodic Experience",
        "params": [
          {
            "name": "history_limit",
            "current": null,
            "default": 500,
            "min_val": 100,
            "max_val": 5000,
            "param_type": "int",
            "description": "Max events in memory"
          }
        ],
        "fitness_score": 0.999752,
        "last_evaluated": "2026-02-18T09:18:16.907931",
        "mutations_applied": 0
      },
      "events.tracing": {
        "component": "events.tracing",
        "layer": "Episodic Experience",
        "params": [
          {
            "name": "max_traces",
            "current": 248,
            "default": 200,
            "min_val": 50,
            "max_val": 1000,
            "param_type": "int",
            "description": "Max traces retained"
          }
        ],
        "fitness_score": 0.5,
        "last_evaluated": "2026-02-18T09:18:16.908041",
        "mutations_applied": 1
      }
    },
    "recent_mutations": [
      {
        "id": "0cbc60fd9d59",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 25,
        "reason": "LLM: 100% policy violation rate suggests personas may be hitting turn limits too quickly. Increasing researcher turns from 15 to 25 allows more thorough investigation before escalation",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:04:12.307595"
      },
      {
        "id": "93cffebae204",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 15,
        "new_value": 30,
        "reason": "LLM: Similarly increasing coder turns from 15 to 30 to prevent premature termination that could lead to incomplete solutions and policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:04:12.307787"
      },
      {
        "id": "ac8740b55940",
        "component": "orchestration.planner",
        "param_name": "pipeline_max_agents",
        "old_value": 5,
        "new_value": 6,
        "reason": "Fitness 0.25 < 0.6, adjusting pipeline_max_agents",
        "fitness_before": 0.25,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:05:07.399965"
      },
      {
        "id": "0e91724725d9",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 400000,
        "new_value": 450000,
        "reason": "LLM: Policy violation rate at 1.00 suggests orchestrator is hitting budget limits and making suboptimal decisions under resource constraints",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:09:37.024839"
      },
      {
        "id": "e91a5502792c",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 25,
        "new_value": 40,
        "reason": "LLM: High activity level (1.00) indicates researchers need more turns to complete thorough analysis before hitting limits",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:09:37.024965"
      },
      {
        "id": "e594f1296d0d",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 12,
        "reason": "LLM: Perfect retrieval hit rate (1.00) with high graph density (0.74) suggests we can afford slightly less aggressive consolidation to reduce system load",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:09:37.025032"
      },
      {
        "id": "4be355eb967c",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 24,
        "reason": "LLM: Double consolidation delay to reduce write frequency and policy violations while maintaining knowledge quality",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.146483"
      },
      {
        "id": "ce92968024a0",
        "component": "knowledge.consolidator",
        "param_name": "max_concurrent_writes",
        "old_value": 8,
        "new_value": 4,
        "reason": "LLM: Halve concurrent writes to reduce system load and policy engine conflicts",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.146603"
      },
      {
        "id": "25cc2413578f",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 40,
        "new_value": 25,
        "reason": "LLM: Reduce researcher turns to prevent excessive exploration that may trigger policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:14:40.146665"
      },
      {
        "id": "a538d3189481",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 39,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:15:29.244104"
      },
      {
        "id": "109317e68010",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 450000,
        "new_value": 300000,
        "reason": "LLM: High policy violation rate (1.00) suggests orchestrator needs more budget to properly coordinate and enforce policies across the system",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:21:07.939075"
      },
      {
        "id": "99d7e48e3cf5",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 39,
        "new_value": 18,
        "reason": "LLM: With perfect retrieval hit rates and high graph density, consolidating knowledge more frequently will improve system coherence and may help reduce policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:21:07.939270"
      },
      {
        "id": "74bfd0fbfde3",
        "component": "intent.personas",
        "param_name": "researcher_budget",
        "old_value": 100000,
        "new_value": 150000,
        "reason": "LLM: Increase researcher budget to enable more thorough analysis that could help identify root causes of the policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:21:07.939368"
      },
      {
        "id": "2b440297c3ab",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 18,
        "new_value": 48,
        "reason": "LLM: Increase consolidation threshold to reduce aggressive knowledge processing that may be contributing to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:29:41.859022"
      },
      {
        "id": "21058429f836",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 25,
        "new_value": 15,
        "reason": "LLM: Reduce recall limit to prevent information overload that could lead to policy-violating decisions",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:29:41.859064"
      },
      {
        "id": "09b9b1c9b961",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 300000,
        "new_value": 200000,
        "reason": "LLM: Reduce orchestrator budget to constrain overly aggressive coordination that may be causing policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:29:41.859090"
      },
      {
        "id": "1bf35c72572e",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 200000,
        "new_value": 115074,
        "reason": "Fitness 0.50 < 0.6, adjusting orchestrator_budget",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:30:23.266920"
      },
      {
        "id": "ee133937aca4",
        "component": "intent.personas",
        "param_name": "researcher_budget",
        "old_value": 150000,
        "new_value": 250000,
        "reason": "LLM: Increase researcher budget to reduce policy violations - current budget may be forcing rushed decisions that bypass policy checks",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:34:11.822503"
      },
      {
        "id": "aeb1b3abe11a",
        "component": "intent.personas",
        "param_name": "coder_budget",
        "old_value": 100000,
        "new_value": 200000,
        "reason": "LLM: Double coder budget to allow more thorough policy compliance during code generation and review phases",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:34:11.822588"
      },
      {
        "id": "d364879b246e",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 48,
        "new_value": 24,
        "reason": "LLM: Reduce consolidation threshold to process knowledge faster, supporting better-informed decision making that could reduce policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:34:11.822871"
      },
      {
        "id": "e8e17f462c68",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 6,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:34:57.420118"
      },
      {
        "id": "cf4e3b078d68",
        "component": "intent.personas",
        "param_name": "coder_budget",
        "old_value": 200000,
        "new_value": 238940,
        "reason": "Fitness 0.50 < 0.6, adjusting coder_budget",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:34:57.420170"
      },
      {
        "id": "cfff9571f348",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 115074,
        "new_value": 350000,
        "reason": "LLM: Increase orchestrator budget to improve coordination and reduce policy violations - current budget of 115074 is much lower than researcher/coder budgets",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:41:00.979143"
      },
      {
        "id": "d68a60d7347b",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 24,
        "reason": "LLM: Reduce consolidation threshold to process knowledge more frequently, helping maintain policy compliance with faster knowledge updates",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:41:00.979200"
      },
      {
        "id": "11b605fd1018",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 15,
        "new_value": 25,
        "reason": "LLM: Increase recall limit to provide agents with more context for better decision-making and policy adherence",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:41:00.979236"
      },
      {
        "id": "635f837b367c",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 13,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:41:47.350913"
      },
      {
        "id": "a34232fc4046",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 13,
        "new_value": 6,
        "reason": "LLM: With perfect retrieval hit rates and high graph density, the system is consolidating knowledge too slowly at 13 hours. Reducing to 6 hours will help manage the high activity levels and potentially reduce policy violations by keeping knowledge more current and organized.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:45:45.071409"
      },
      {
        "id": "ea341f94d051",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 25,
        "new_value": 15,
        "reason": "LLM: The 100% policy violation rate suggests personas are operating too aggressively. Reducing researcher max turns from 25 to 15 will constrain exploration behavior and help reduce policy violations while maintaining effectiveness.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:45:45.071605"
      },
      {
        "id": "16d6956f94c8",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 30,
        "new_value": 20,
        "reason": "LLM: Similarly constraining coder max turns from 30 to 20 to reduce aggressive behavior that may be contributing to the high policy violation rate, while still allowing sufficient iterations for code development.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:45:45.071696"
      },
      {
        "id": "36293696240e",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 25,
        "reason": "LLM: Increase from 15 to 25 to reduce policy violations by allowing researchers more turns to complete tasks properly without rushing",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:51:51.515414"
      },
      {
        "id": "5bb415957a2e",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 20,
        "new_value": 35,
        "reason": "LLM: Increase from 20 to 35 to give coders sufficient turns to implement solutions correctly and reduce policy violations from incomplete work",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:51:51.515454"
      },
      {
        "id": "e2c9670a68c2",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 12,
        "reason": "LLM: Increase consolidation threshold from 6 to 12 hours to reduce write contention that may be contributing to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:51:51.515480"
      },
      {
        "id": "2ba73d9eb97e",
        "component": "intent.personas",
        "param_name": "researcher_budget",
        "old_value": 250000,
        "new_value": 150000,
        "reason": "LLM: Reduce from 250000 to help lower the 100% policy violation rate by constraining resource usage",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:57:39.365324"
      },
      {
        "id": "788c822252c5",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 350000,
        "new_value": 200000,
        "reason": "LLM: Reduce from 350000 as the highest budget persona likely contributing most to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:57:39.365372"
      },
      {
        "id": "95e9ade6e521",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 24,
        "reason": "LLM: Increase consolidation frequency from 12 to 24 hours to reduce write pressure and potential policy conflicts",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:57:39.365404"
      },
      {
        "id": "d22b2fc3d78d",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 27,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:58:24.827200"
      },
      {
        "id": "2e29b9e4a4a1",
        "component": "intent.personas",
        "param_name": "orchestrator_budget",
        "old_value": 200000,
        "new_value": 256022,
        "reason": "Fitness 0.50 < 0.6, adjusting orchestrator_budget",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T08:58:24.827268"
      },
      {
        "id": "9ed1831e53fc",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 27,
        "new_value": 12,
        "reason": "LLM: With perfect retrieval hit rates and high graph density, reducing consolidation delay from 27 to 12 hours will enable faster knowledge integration and potentially reduce policy violations by keeping information more current",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:04:00.495541"
      },
      {
        "id": "f692fb09126a",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 25,
        "new_value": 15,
        "reason": "LLM: 100% policy violation rate suggests personas may be exceeding boundaries; reducing researcher turns from 25 to 15 will constrain exploration and improve compliance while maintaining effectiveness",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:04:00.495578"
      },
      {
        "id": "a5722785cfab",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 25,
        "new_value": 35,
        "reason": "LLM: Perfect retrieval performance indicates we can safely increase recall limit from 25 to 35, providing richer context for decision-making which may help reduce policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:04:00.495601"
      },
      {
        "id": "39fb080943da",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 12,
        "new_value": 6,
        "reason": "LLM: With perfect retrieval hit rates and high graph density, knowledge is being accessed frequently. Reducing consolidation delay from 12 to 6 hours will help organize knowledge faster and potentially reduce policy violations by improving information structure.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:08:13.215459"
      },
      {
        "id": "30ad1247394b",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 10,
        "reason": "LLM: 100% policy violation rate suggests personas are operating too aggressively. Reducing researcher turns from 15 to 10 will constrain exploration and reduce the likelihood of policy violations while maintaining effectiveness.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:08:13.215519"
      },
      {
        "id": "e0c60e82bb12",
        "component": "intent.personas",
        "param_name": "coder_max_turns",
        "old_value": 35,
        "new_value": 25,
        "reason": "LLM: Similarly constraining coder turns from 35 to 25 will reduce aggressive behavior that may be contributing to policy violations, while still allowing sufficient iterations for code development tasks.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:08:13.215553"
      },
      {
        "id": "85dd20ea3f4d",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 6,
        "new_value": 24,
        "reason": "LLM: Increase consolidation threshold from 6 to 24 hours to reduce frequent consolidation operations that may be causing policy violations while maintaining knowledge quality",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:13:09.051244"
      },
      {
        "id": "80e5c88ac10e",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 35,
        "new_value": 25,
        "reason": "LLM: Reduce recall limit from 35 to 25 to decrease memory pressure and processing overhead that could be contributing to policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:13:09.051368"
      },
      {
        "id": "180fe255bfdf",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 10,
        "new_value": 15,
        "reason": "LLM: Increase researcher max turns from 10 to 15 to allow more thorough research cycles, potentially reducing the need for repeated operations that trigger policy violations",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:13:09.051481"
      },
      {
        "id": "a21267fd772e",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 24,
        "new_value": 11,
        "reason": "Fitness 0.50 < 0.6, adjusting older_than_hours",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:13:45.172042"
      },
      {
        "id": "fa579d2dc2f4",
        "component": "knowledge.consolidator",
        "param_name": "older_than_hours",
        "old_value": 11,
        "new_value": 24,
        "reason": "LLM: With perfect retrieval hit rates and high graph density, knowledge is being accessed frequently. Increasing consolidation threshold from 11 to 24 hours will reduce premature consolidation of actively used knowledge, allowing more time for related information to accumulate before clustering.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:18:24.939677"
      },
      {
        "id": "38e010d64d81",
        "component": "knowledge.loom",
        "param_name": "recall_limit",
        "old_value": 25,
        "new_value": 40,
        "reason": "LLM: Given the 100% policy violation rate and high system activity, the current recall limit of 25 may be too restrictive. Increasing to 40 will provide more context for decision-making, potentially reducing policy violations by giving agents access to more relevant historical information.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:18:24.939714"
      },
      {
        "id": "be433681ccb5",
        "component": "intent.personas",
        "param_name": "researcher_max_turns",
        "old_value": 15,
        "new_value": 25,
        "reason": "LLM: The researcher has the lowest budget but only 15 max turns, creating an imbalance. Increasing to 25 turns (matching coder) will allow more thorough research phases, which could improve overall system decision-making and reduce the high policy violation rate.",
        "fitness_before": 0.5,
        "fitness_after": null,
        "applied": true,
        "timestamp": "2026-02-18T09:18:24.939737"
      }
    ],
    "timestamp": "2026-02-18T09:18:26.322472"
  },
  "meta_cycles_completed": 168,
  "design_archive": {
    "max_size": 50,
    "temperature": 0.3,
    "entries": [
      {
        "id": "8318140dffae",
        "strategy_name": "Message Channel Router",
        "module": "coordination",
        "code_hash": "c454a49fab15e10e",
        "code_snippet": "from collections import defaultdict\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n    def subscribe(self, agent, topics):\n        for t in topics:\n            self.subscribers[t].append(agent)\n    def send(self, topic, msg, sender):\n        self.history.append({'topic': topic, 'msg': msg, 'sender': sender})\n        receivers = self.subscribers.get(topic, [])\n        return [r for r in receivers if r != sender]\n    def broadcast(self, msg, sender):\n        all_agents = set()\n        for agents in self.subscribers.values():\n            all_agents.update(agents)\n        all_agents.discard(sender)\n        self.history.append({'topic': '*', 'msg': msg, 'sender': sender})\n        return sorted(all_agents)\n\nch = Channel('team-alpha')\nch.subscribe('researcher', ['findings', 'requests'])\nch.subscribe('coder', ['requests', 'reviews'])\nch.subscribe('reviewer', ['reviews', 'findings'])\nr1 = ch.send('findings', 'found paper', 'researcher')\nassert 'reviewer' in r1 and 'researcher' not in r1\nr2 = ch.broadcast('done', 'coder')\nassert 'researcher' in r2 and 'reviewer' in r2\nprint(f'findings -> {r1}, broadcast -> {r2}')\nprint('PASS: Message channel router validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15019v1",
        "created_at": "2026-02-17T21:26:58.853804"
      },
      {
        "id": "37958b65a336",
        "strategy_name": "Strategy Selector",
        "module": "orchestration.planner",
        "code_hash": "93717c7c13b6b679",
        "code_snippet": "class StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        success_rate = self.successes / total\n        efficiency = 1.0 / (1 + self.total_tokens / max(total, 1) / 10000)\n        return round(0.7 * success_rate + 0.3 * efficiency, 3)\n\ndef select_strategy(records, task_size):\n    if task_size == 'small':\n        candidates = [r for r in records if r.name in ('solo', 'pipeline')]\n    else:\n        candidates = [r for r in records if r.name in ('parallel', 'debate')]\n    if not candidates: candidates = records\n    return max(candidates, key=lambda r: r.score)\n\nsolo = StrategyRecord('solo')\nsolo.successes, solo.failures, solo.total_tokens = 8, 2, 50000\nparallel = StrategyRecord('parallel')\nparallel.successes, parallel.failures, parallel.total_tokens = 15, 5, 200000\ndebate = StrategyRecord('debate')\ndebate.successes, debate.failures, debate.total_tokens = 4, 1, 80000\nrecords = [solo, parallel, debate]\nsmall = select_strategy(records, 'small')\nlarge = select_strategy(records, 'large')\nassert small.name == 'solo'\nassert large.name in ('parallel', 'debate')\nprint(f'Small task: {small.name} (score={small.score})')\nprint(f'Large task: {large.name} (score={large.score})')\nprint('PASS: Strategy selector validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-17T21:30:22.159005"
      },
      {
        "id": "17d4ca2306e0",
        "strategy_name": "Strategy Selector",
        "module": "orchestration.planner",
        "code_hash": "93717c7c13b6b679",
        "code_snippet": "class StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        success_rate = self.successes / total\n        efficiency = 1.0 / (1 + self.total_tokens / max(total, 1) / 10000)\n        return round(0.7 * success_rate + 0.3 * efficiency, 3)\n\ndef select_strategy(records, task_size):\n    if task_size == 'small':\n        candidates = [r for r in records if r.name in ('solo', 'pipeline')]\n    else:\n        candidates = [r for r in records if r.name in ('parallel', 'debate')]\n    if not candidates: candidates = records\n    return max(candidates, key=lambda r: r.score)\n\nsolo = StrategyRecord('solo')\nsolo.successes, solo.failures, solo.total_tokens = 8, 2, 50000\nparallel = StrategyRecord('parallel')\nparallel.successes, parallel.failures, parallel.total_tokens = 15, 5, 200000\ndebate = StrategyRecord('debate')\ndebate.successes, debate.failures, debate.total_tokens = 4, 1, 80000\nrecords = [solo, parallel, debate]\nsmall = select_strategy(records, 'small')\nlarge = select_strategy(records, 'large')\nassert small.name == 'solo'\nassert large.name in ('parallel', 'debate')\nprint(f'Small task: {small.name} (score={small.score})')\nprint(f'Large task: {large.name} (score={large.score})')\nprint('PASS: Strategy selector validated')\n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 0,
        "parent_id": "",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T02:13:05.972032"
      },
      {
        "id": "7103ef2c101f",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "4eea7cef3a0e8442",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Efficiency with logarithmic scaling\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 1000))\n        \n        # Recent performance trend\n        recent_trend = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success_rate = sum(self.recent_outcomes[-5:]) / len(self.recent_outcomes[-5:])\n            trend_factor = recent_success_rate / max(success_rate, 0.1)\n            recent_trend = min(1.2, max(0.8, trend_factor))\n        \n        return round(0.6 * adjusted_success_rate + 0.25 * efficiency + 0.15 * recent_trend, 3)\n    \n    def update(self, success, tokens_used, context=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        self.total_tokens += tokens_used\n        self.recent_outcomes.append(1 if success else 0)\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes.pop(0)\n        \n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n\ndef select_strategy(records, task_size, context=None, complexity=1.0):\n    # Define strategy compatibility\n    strategy_rules = {\n        'small': {'preferred': ['solo', 'pipeline'], 'avoid': []},\n        'large': {'preferred': ['parallel', 'debate', 'hierarchical'], 'avoid': ['solo']},\n        'medium': {'preferred': ['pipeline', 'parallel'], 'avoid': []}\n    }\n    \n    rules = strategy_rules.get(task_size, {'preferred': [], 'avoid': []})\n    \n    # Score each strategy\n    scored_strategies = []\n    for record in records:\n        base_score = record.score\n        \n        # Task size compatibility bonus/penalty\n        if record.name in rules['preferred']:\n            base_score *= 1.2\n        elif record.name in rules['avoid']:\n            base_score *= 0.7\n        \n        # Context-specific performance\n        if context and context in record.context_performance:\n            ctx_perf = record.context_performance[context]\n            if ctx_perf['total'] > 0:\n                ctx_success_rate = ctx_perf['success'] / ctx_perf['total']\n                base_score = 0.7 * base_score + 0.3 * ctx_success_rate\n        \n",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "37958b65a336",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T07:23:50.524818"
      },
      {
        "id": "1371a65d937d",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "fd581b7f67a03f6c",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        \n        # Token efficiency (normalized)\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + avg_tokens / 5000)\n        \n        # Recent performance trend\n        recent_trend = self._calculate_trend()\n        \n        # Weighted score with trend consideration\n        base_score = 0.6 * success_rate + 0.3 * efficiency + 0.1 * recent_trend\n        return round(confidence * base_score + (1 - confidence) * 0.5, 3)\n    \n    def _calculate_trend(self):\n        if len(self.recent_outcomes) < 3:\n            return 0.5\n        recent = self.recent_outcomes[-5:]  # Last 5 outcomes\n        return sum(recent) / len(recent)\n    \n    def update_performance(self, success, tokens, context=None):\n        self.total_tokens += tokens\n        if success:\n            self.successes += 1\n            self.recent_outcomes.append(1.0)\n        else:\n            self.failures += 1\n            self.recent_outcomes.append(0.0)\n        \n        # Keep only recent outcomes\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes = self.recent_outcomes[-10:]\n        \n        # Track context-specific performance\n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n    \n    def context_score(self, context):\n        if context not in self.context_performance:\n            return self.score\n        \n        ctx_data = self.context_performance[context]\n        if ctx_data['total'] == 0:\n            return self.score\n        \n        ctx_success_rate = ctx_data['success'] / ctx_data['total']\n        # Blend context-specific with general performance\n        weight = min(0.7, ctx_data['total'] / 5)\n        return weight * ctx_success_rate + (1 - weight) * (self.successes / max(self.successes + self.failures, 1))\n\nclass AdaptivePlanner:\n    def __init__(self):\n        self.strategies = {}\n        self.task_contexts = defaultdict(list)\n        \n    def register_strategy(self, name):\n        if name not in self.strategies:\n            self.strategies[name] = StrategyRecord(name)\n        return self.strategies[name]\n    \n    def select_strategy(self, task_size, context=None, complexity=1.0):\n        # Define strategy pools based on task characteristics\n        if task_size == 'small' and compl",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "17d4ca2306e0",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T07:24:11.671538"
      },
      {
        "id": "84eb6f8a1bae",
        "strategy_name": "Message Channel Router_gen1",
        "module": "coordination",
        "code_hash": "060e5561e17281db",
        "code_snippet": "from collections import defaultdict\nimport json\nimport time\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n        self.message_filters = {}\n        self.priority_queues = defaultdict(list)\n        \n    def subscribe(self, agent, topics, priority=0, message_filter=None):\n        for t in topics:\n            self.subscribers[t].append({'agent': agent, 'priority': priority})\n            if message_filter:\n                self.message_filters[f\"{agent}:{t}\"] = message_filter\n        # Sort by priority (higher first)\n        for t in topics:\n            self.subscribers[t].sort(key=lambda x: x['priority'], reverse=True)\n    \n    def send(self, topic, msg, sender, priority=0):\n        timestamp = time.time()\n        msg_obj = {\n            'topic': topic, \n            'msg': msg, \n            'sender': sender, \n            'priority': priority,\n            'timestamp': timestamp\n        }\n        self.history.append(msg_obj)\n        \n        receivers = []\n        for sub in self.subscribers.get(topic, []):\n            agent = sub['agent']\n            if agent != sender:\n                # Apply message filter if exists\n                filter_key = f\"{agent}:{topic}\"\n                if filter_key in self.message_filters:\n                    if self.message_filters[filter_key](msg_obj):\n                        receivers.append(agent)\n                else:\n                    receivers.append(agent)\n        \n        # Queue high priority messages\n        if priority > 0:\n            for receiver in receivers:\n                self.priority_queues[receiver].append(msg_obj)\n                \n        return receivers\n    \n    def broadcast(self, msg, sender, exclude_topics=None):\n        timestamp = time.time()\n        msg_obj = {\n            'topic': '*', \n            'msg': msg, \n            'sender': sender,\n            'timestamp': timestamp\n        }\n        self.history.append(msg_obj)\n        \n        all_agents = set()\n        for topic, agents in self.subscribers.items():\n            if exclude_topics and topic in exclude_topics:\n                continue\n            for sub in agents:\n                all_agents.add(sub['agent'])\n        all_agents.discard(sender)\n        return sorted(all_agents)\n    \n    def get_priority_messages(self, agent):\n        messages = self.priority_queues.get(agent, [])\n        self.priority_queues[agent] = []  # Clear after retrieval\n        return sorted(messages, key=lambda x: x['priority'], reverse=True)\n    \n    def get_history(self, topic=None, agent=None, limit=None):\n        filtered = self.history\n        if topic:\n            filtered = [m for m in filtered if m['topic'] == topic]\n        if agent:\n            filtered = [m for m in filtered if m['sender'] == agent]\n        if limit:\n            filtered = filtered[-limit:]\n        return filtered\n\n# Test the enhanced channel\nch = Channel('team-alpha')\n\n# Subscribe wit",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "8318140dffae",
        "source_paper": "2602.15019v1",
        "created_at": "2026-02-18T07:27:40.831679"
      },
      {
        "id": "4a7def25fc24",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "240b8f7aec034ac2",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []  # Track recent performance\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confidence with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Efficiency with logarithmic scaling\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 1000))\n        \n        # Recent performance trend\n        recent_weight = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success_rate = sum(self.recent_outcomes[-5:]) / len(self.recent_outcomes[-5:])\n            trend = recent_success_rate - success_rate\n            recent_weight = 1.0 + 0.2 * trend  # Boost if improving, penalize if declining\n        \n        base_score = 0.6 * adjusted_success_rate + 0.4 * efficiency\n        return round(base_score * recent_weight, 3)\n    \n    def record_outcome(self, success, tokens_used, context=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        self.total_tokens += tokens_used\n        self.recent_outcomes.append(1 if success else 0)\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes.pop(0)\n        \n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n\ndef select_strategy(records, task_size, context=None, complexity=1.0):\n    # Dynamic strategy mapping based on task characteristics\n    if task_size == 'small' and complexity < 0.5:\n        preferred = ['solo', 'pipeline']\n        fallback_weight = 0.8\n    elif task_size == 'small':\n        preferred = ['solo', 'pipeline', 'parallel']\n        fallback_weight = 0.9\n    elif complexity > 0.8:\n        preferred = ['debate', 'parallel']\n        fallback_weight = 0.7\n    else:\n        preferred = ['parallel', 'debate', 'pipeline']\n        fallback_weight = 0.85\n    \n    # Score strategies with preference weighting\n    scored_strategies = []\n    for record in records:\n        base_score = record.score\n        \n        # Apply preference bonus\n        if record.name in preferred:\n            preference_bonus = 0.1\n        else:\n            preference_bonus = -0.05\n        \n        # Context-specific performance\n        context_bonus = 0\n        if context and context in record.context_performance:\n            ctx_perf = record.context_perform",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "37958b65a336",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T07:28:02.979181"
      },
      {
        "id": "9eae5a47f753",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "25db7bab483b310b",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Efficiency with diminishing returns\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 5000))\n        \n        # Recent performance trend\n        recent_trend = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success_rate = sum(self.recent_outcomes[-5:]) / len(self.recent_outcomes[-5:])\n            trend_factor = recent_success_rate / max(success_rate, 0.1)\n            recent_trend = min(1.2, max(0.8, trend_factor))\n        \n        return round(0.6 * adjusted_success_rate + 0.25 * efficiency + 0.15 * recent_trend, 3)\n    \n    def update_outcome(self, success, tokens, context=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        self.total_tokens += tokens\n        self.recent_outcomes.append(1 if success else 0)\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes.pop(0)\n        \n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n\ndef select_strategy(records, task_size, context=None, complexity=1.0):\n    # Dynamic strategy mapping based on task characteristics\n    if task_size == 'small' and complexity < 0.5:\n        primary_candidates = [r for r in records if r.name in ('solo', 'pipeline')]\n        fallback_candidates = [r for r in records if r.name not in ('debate',)]\n    elif task_size == 'large' or complexity > 0.7:\n        primary_candidates = [r for r in records if r.name in ('parallel', 'debate')]\n        fallback_candidates = [r for r in records if r.name != 'solo']\n    else:\n        primary_candidates = [r for r in records if r.name in ('pipeline', 'parallel')]\n        fallback_candidates = records\n    \n    # Context-aware scoring\n    def context_adjusted_score(record):\n        base_score = record.score\n        if context and context in record.context_performance:\n            ctx_perf = record.context_performance[context]\n            if ctx_perf['total'] > 0:\n                ctx_success_rate = ctx_perf['success'] / ctx_perf['total']\n                # Blend context performance with overall score\n                weight = min(0.4, ctx_pe",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "17d4ca2306e0",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T07:32:56.495995"
      },
      {
        "id": "2b4229e0d01e",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "69280643bfd6e693",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_performance = []\n        self.context_scores = defaultdict(list)\n        self.last_used = 0\n        self.complexity_handled = []\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Token efficiency with diminishing returns\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 5000))\n        \n        # Recent performance trend\n        trend_bonus = 0\n        if len(self.recent_performance) >= 3:\n            recent_avg = sum(self.recent_performance[-3:]) / 3\n            overall_avg = sum(self.recent_performance) / len(self.recent_performance)\n            trend_bonus = min(0.1, max(-0.1, (recent_avg - overall_avg) * 0.5))\n        \n        # Freshness penalty for unused strategies\n        staleness_penalty = min(0.05, self.last_used * 0.01)\n        \n        return round(0.6 * adjusted_success_rate + 0.25 * efficiency + 0.15 * success_rate + trend_bonus - staleness_penalty, 3)\n    \n    def update_performance(self, success, tokens_used, context=None):\n        if success:\n            self.successes += 1\n            self.recent_performance.append(1.0)\n        else:\n            self.failures += 1\n            self.recent_performance.append(0.0)\n        \n        self.total_tokens += tokens_used\n        self.last_used = 0  # Reset staleness\n        \n        # Keep only recent performance history\n        if len(self.recent_performance) > 10:\n            self.recent_performance.pop(0)\n        \n        if context:\n            self.context_scores[context].append(1.0 if success else 0.0)\n\ndef select_strategy(records, task_size, context=None, complexity=1.0):\n    # Age all strategies\n    for record in records:\n        record.last_used += 1\n    \n    # Filter by task size and complexity\n    if task_size == 'small' and complexity < 0.7:\n        candidates = [r for r in records if r.name in ('solo', 'pipeline')]\n    elif task_size == 'large' or complexity > 0.8:\n        candidates = [r for r in records if r.name in ('parallel', 'debate', 'hierarchical')]\n    else:\n        candidates = records\n    \n    if not candidates:\n        candidates = records\n    \n    # Context-aware scoring\n    scored_candidates = []\n    for candidate in candidates:\n        base_score = candidate.score\n        context_bonus = 0\n        \n        if context and context in candidate.context_scores:\n            context_performance = candidate.context_scores[",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "37958b65a336",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T07:38:50.262643"
      },
      {
        "id": "8a96745bce82",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "9bca7d7fb9d2bb35",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []  # Track recent performance\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        \n        # Token efficiency with diminishing returns\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 1000))\n        \n        # Recent performance trend\n        recent_boost = 0\n        if len(self.recent_outcomes) >= 3:\n            recent_success = sum(self.recent_outcomes[-5:]) / min(5, len(self.recent_outcomes))\n            recent_boost = 0.1 * (recent_success - success_rate)\n        \n        # Weighted score with confidence scaling\n        base_score = 0.6 * success_rate + 0.3 * efficiency + 0.1\n        final_score = confidence * (base_score + recent_boost) + (1 - confidence) * 0.5\n        \n        return round(min(1.0, max(0.0, final_score)), 3)\n    \n    def record_outcome(self, success, tokens, context=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        self.total_tokens += tokens\n        self.recent_outcomes.append(1 if success else 0)\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes.pop(0)\n        \n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n\ndef select_strategy(records, task_size, context=None, exploration_rate=0.1):\n    # Filter strategies by task size\n    if task_size == 'small':\n        candidates = [r for r in records if r.name in ('solo', 'pipeline')]\n    elif task_size == 'large':\n        candidates = [r for r in records if r.name in ('parallel', 'debate')]\n    else:\n        candidates = records\n    \n    if not candidates:\n        candidates = records\n    \n    # Context-aware scoring\n    scored_candidates = []\n    for record in candidates:\n        base_score = record.score\n        \n        # Context adjustment\n        if context and context in record.context_performance:\n            ctx_perf = record.context_performance[context]\n            if ctx_perf['total'] > 0:\n                ctx_success_rate = ctx_perf['success'] / ctx_perf['total']\n                base_score = 0.7 * base_score + 0.3 * ctx_success_rate\n        \n        scored_candidates.append((record, base_score))\n    \n    # Exploration vs exploitation\n    import random\n    if random.random() < exploration_rate:\n        # Explore: favor less-t",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "37958b65a336",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T07:46:20.359563"
      },
      {
        "id": "6e9ea37d4f72",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "9f034c1f23d32b96",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)  # Fixed-size recent history\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=10)\n        self.complexity_scores = deque(maxlen=10)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))  # Square root scaling for smoother confidence\n        base_rate = 0.4 if total < 3 else 0.5  # Lower baseline for very new strategies\n        adjusted_success_rate = success_rate * confidence + base_rate * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 800))  # Tighter token scaling\n        \n        # Speed efficiency if available\n        speed_efficiency = 1.0\n        if self.execution_times:\n            avg_time = sum(self.execution_times) / len(self.execution_times)\n            speed_efficiency = 1.0 / (1 + avg_time / 5.0)  # Normalize around 5 second baseline\n        \n        # Complexity handling capability\n        complexity_bonus = 1.0\n        if self.complexity_scores:\n            avg_complexity = sum(self.complexity_scores) / len(self.complexity_scores)\n            complexity_bonus = 1.0 + 0.1 * min(avg_complexity / 10.0, 1.0)  # Bonus for handling complex tasks\n        \n        # Enhanced recent performance with momentum\n        momentum = 1.0\n        if len(self.recent_outcomes) >= 4:\n            recent_rate = sum(list(self.recent_outcomes)[-6:]) / min(6, len(self.recent_outcomes))\n            trend = recent_rate - success_rate\n            momentum = 1.0 + 0.3 * trend  # Stronger momentum effect\n            momentum = max(0.7, min(1.4, momentum))  # Bounded momentum\n        \n        # Weighted composite score with efficiency emphasis\n        efficiency_score = 0.6 * token_efficiency + 0.3 * speed_efficiency + 0.1 * complexity_bonus\n        base_score = 0.65 * adjusted_success_rate + 0.35 * efficiency_score\n        \n        return round(base_score * momentum, 3)\n    \n    def record_outcome(self, success, tokens_used, context=None, execution_time=None, complexity=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        \n        self.total_tokens += tokens_used\n        self.recent_outcomes.append(1 if success else 0)\n        \n        if execution_time is not None:\n            self.execution_times.append(execution_time)\n        \n        if complexity is not None:\n         ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "4a7def25fc24",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T07:57:48.144460"
      },
      {
        "id": "6e53cda49791",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "eca0ab680ea9a756",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'avg_tokens': 0})\n        self.execution_times = deque(maxlen=20)\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.streak = 0\n        self.best_streak = 0\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.log(1 + total) / math.log(21))  # Smoother confidence curve\n        base_score = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 3000))\n        \n        # Velocity bonus for faster execution\n        velocity_bonus = 1.0\n        if self.execution_times:\n            avg_time = sum(self.execution_times) / len(self.execution_times)\n            velocity_bonus = min(1.3, 1.0 + (10.0 / max(avg_time, 1.0)) * 0.1)\n        \n        # Recent performance with momentum\n        recent_momentum = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_window = list(self.recent_outcomes)[-7:]\n            recent_success = sum(recent_window) / len(recent_window)\n            momentum = recent_success / max(success_rate, 0.1)\n            recent_momentum = min(1.4, max(0.6, momentum))\n        \n        # Streak bonus for consistent performance\n        streak_bonus = min(1.2, 1.0 + (self.streak * 0.05))\n        \n        # Recency penalty to encourage exploration\n        recency_factor = max(0.9, 1.0 - (self.last_used * 0.02))\n        \n        final_score = (0.45 * base_score + \n                      0.2 * token_efficiency + \n                      0.15 * recent_momentum + \n                      0.1 * streak_bonus + \n                      0.1 * velocity_bonus) * recency_factor\n        \n        return round(min(1.0, final_score), 4)\n    \n    def update_outcome(self, success, tokens, context=None, complexity=1.0, execution_time=1.0):\n        if success:\n            self.successes += 1\n            self.streak += 1\n            self.best_streak = max(self.best_streak, self.streak)\n        else:\n            self.failures += 1\n            self.streak = 0\n            \n        self.total_tokens += tokens\n        self.recent_outcomes.append(1 if success else 0)\n        self.execution_times.append(execution_time)\n        self.last_used = 0  # Reset recency counter\n        \n        if context:\n            ctx_perf = self.context_performance[context]\n          ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "9eae5a47f753",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T08:03:38.224101"
      },
      {
        "id": "b698145872a2",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "c71bb563684c9b91",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []\n        self.context_performance = defaultdict(lambda: {'successes': 0, 'failures': 0})\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 20)  # More confident with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Efficiency with logarithmic scaling\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 1000))\n        \n        # Recent performance trend\n        recent_trend = self._calculate_trend()\n        \n        # Weighted combination\n        score = 0.5 * adjusted_success_rate + 0.3 * efficiency + 0.2 * recent_trend\n        return round(score, 3)\n    \n    def _calculate_trend(self):\n        if len(self.recent_outcomes) < 3:\n            return 0.5\n        \n        # Weight recent outcomes more heavily\n        weighted_sum = sum(outcome * (i + 1) for i, outcome in enumerate(self.recent_outcomes[-10:]))\n        weight_total = sum(range(1, min(11, len(self.recent_outcomes) + 1)))\n        return weighted_sum / weight_total if weight_total > 0 else 0.5\n    \n    def record_outcome(self, success, tokens_used, context=None):\n        if success:\n            self.successes += 1\n            self.recent_outcomes.append(1.0)\n        else:\n            self.failures += 1\n            self.recent_outcomes.append(0.0)\n        \n        self.total_tokens += tokens_used\n        \n        if context:\n            if success:\n                self.context_performance[context]['successes'] += 1\n            else:\n                self.context_performance[context]['failures'] += 1\n        \n        # Keep only recent outcomes\n        if len(self.recent_outcomes) > 20:\n            self.recent_outcomes = self.recent_outcomes[-20:]\n    \n    def contextual_score(self, context):\n        if context not in self.context_performance:\n            return self.score\n        \n        ctx_data = self.context_performance[context]\n        ctx_total = ctx_data['successes'] + ctx_data['failures']\n        \n        if ctx_total < 3:  # Not enough context data\n            return self.score\n        \n        ctx_success_rate = ctx_data['successes'] / ctx_total\n        # Blend contextual and overall performance\n        blend_factor = min(0.7, ctx_total / 10)\n        return round(blend_factor * ctx_success_rate + (1 - blend_factor) * self.score, 3)\n\ndef select_strategy(records, task_size, context=None, exploration_rate=0.1):\n    \"\"\"Enhanced strategy selection with contextual awareness and exploratio",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "37958b65a336",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:04:03.316379"
      },
      {
        "id": "776e36597fb9",
        "strategy_name": "Message Channel Router_gen1",
        "module": "coordination",
        "code_hash": "022392e1c5743810",
        "code_snippet": "from collections import defaultdict\nimport json\nimport time\n\nclass Channel:\n    def __init__(self, name):\n        self.name = name\n        self.subscribers = defaultdict(list)\n        self.history = []\n        self.message_filters = {}\n        self.priority_queues = defaultdict(list)\n        \n    def subscribe(self, agent, topics, priority=1, message_filter=None):\n        for t in topics:\n            if agent not in self.subscribers[t]:\n                self.subscribers[t].append(agent)\n        if message_filter:\n            self.message_filters[agent] = message_filter\n            \n    def unsubscribe(self, agent, topics=None):\n        if topics is None:\n            topics = list(self.subscribers.keys())\n        for t in topics:\n            if agent in self.subscribers[t]:\n                self.subscribers[t].remove(agent)\n                \n    def send(self, topic, msg, sender, priority=1, metadata=None):\n        timestamp = time.time()\n        message = {\n            'topic': topic, \n            'msg': msg, \n            'sender': sender,\n            'timestamp': timestamp,\n            'priority': priority,\n            'metadata': metadata or {}\n        }\n        self.history.append(message)\n        \n        receivers = []\n        for agent in self.subscribers.get(topic, []):\n            if agent != sender and self._passes_filter(agent, message):\n                receivers.append(agent)\n                self.priority_queues[agent].append((priority, timestamp, message))\n                self.priority_queues[agent].sort(reverse=True)\n                \n        return receivers\n        \n    def broadcast(self, msg, sender, priority=1, exclude_topics=None):\n        timestamp = time.time()\n        message = {\n            'topic': '*', \n            'msg': msg, \n            'sender': sender,\n            'timestamp': timestamp,\n            'priority': priority,\n            'metadata': {'broadcast': True}\n        }\n        self.history.append(message)\n        \n        all_agents = set()\n        for topic, agents in self.subscribers.items():\n            if exclude_topics and topic in exclude_topics:\n                continue\n            all_agents.update(agents)\n        all_agents.discard(sender)\n        \n        filtered_agents = []\n        for agent in all_agents:\n            if self._passes_filter(agent, message):\n                filtered_agents.append(agent)\n                self.priority_queues[agent].append((priority, timestamp, message))\n                self.priority_queues[agent].sort(reverse=True)\n                \n        return sorted(filtered_agents)\n        \n    def get_messages(self, agent, limit=10):\n        messages = self.priority_queues.get(agent, [])\n        result = [msg for _, _, msg in messages[:limit]]\n        self.priority_queues[agent] = messages[limit:]\n        return result\n        \n    def _passes_filter(self, agent, message):\n        if agent not in self.message_filters:\n            return True\n        filter_func = self.message_filters[",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "8318140dffae",
        "source_paper": "2602.15019v1",
        "created_at": "2026-02-18T08:09:28.342453"
      },
      {
        "id": "9e6414e3310f",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "91c5a0f832db4e0a",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_performance = deque(maxlen=20)\n        self.context_scores = defaultdict(lambda: deque(maxlen=10))\n        self.last_used = 0\n        self.complexity_handled = deque(maxlen=15)\n        self.execution_times = deque(maxlen=10)\n        self.adaptive_weight = 1.0\n        self.specialization_bonus = 0.0\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: \n            return 0.5 + random.uniform(-0.05, 0.05)  # Small exploration bonus\n        \n        # Enhanced success rate with Wilson score interval\n        success_rate = self.successes / total\n        z = 1.96  # 95% confidence\n        wilson_center = (success_rate + z*z/(2*total)) / (1 + z*z/total)\n        wilson_radius = z * math.sqrt(success_rate*(1-success_rate)/total + z*z/(4*total*total)) / (1 + z*z/total)\n        confidence_adjusted = wilson_center - wilson_radius * 0.5\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 3000))\n        \n        time_efficiency = 1.0\n        if self.execution_times:\n            avg_time = sum(self.execution_times) / len(self.execution_times)\n            time_efficiency = 1.0 / (1 + avg_time / 10.0)\n        \n        # Advanced trend analysis with momentum\n        trend_score = 0\n        if len(self.recent_performance) >= 5:\n            recent_window = list(self.recent_performance)[-5:]\n            weights = [0.1, 0.15, 0.2, 0.25, 0.3]  # More weight to recent\n            weighted_recent = sum(p * w for p, w in zip(recent_window, weights))\n            \n            overall_avg = sum(self.recent_performance) / len(self.recent_performance)\n            momentum = (weighted_recent - overall_avg) * 2.0\n            trend_score = max(-0.15, min(0.15, momentum))\n        \n        # Context specialization bonus\n        context_bonus = self.specialization_bonus * 0.1\n        \n        # Complexity handling capability\n        complexity_bonus = 0\n        if self.complexity_handled:\n            avg_complexity = sum(self.complexity_handled) / len(self.complexity_handled)\n            complexity_bonus = min(0.08, avg_complexity * 0.02)\n        \n        # Dynamic staleness with decay\n        staleness_penalty = min(0.1, self.last_used * 0.005 * (1 + self.last_used * 0.001))\n        \n        # Adaptive weighting based on performance consistency\n        consistency = 1.0\n        if len(self.recent_performance) > 3:\n            variance = sum((x - success_rate)**2 for x in self.recent_performance) / len(self.recent_performance)\n            consistency = 1.0 / (1 + variance * 2)\n        \n        final_score = (\n            0.45 * confidence_adjusted * consistency +\n ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "2b4229e0d01e",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:13:23.807122"
      },
      {
        "id": "2bfaf170786c",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "bdabb160d7c0849b",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'avg_tokens': 0})\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.streak = 0\n        self.best_streak = 0\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 3000))\n        \n        # Enhanced recent performance with momentum\n        recent_weight = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success = sum(self.recent_outcomes) / len(self.recent_outcomes)\n            momentum = recent_success / max(success_rate, 0.1)\n            recent_weight = min(1.3, max(0.7, momentum))\n            \n            # Streak bonus for consistent performance\n            if self.streak >= 3:\n                recent_weight *= min(1.15, 1 + (self.streak - 2) * 0.05)\n        \n        # Freshness factor to encourage exploration\n        freshness = max(0.9, 1.0 - (self.last_used * 0.02))\n        \n        # Composite score with balanced weighting\n        base_score = (0.5 * adjusted_success_rate + \n                     0.2 * token_efficiency + \n                     0.2 * recent_weight + \n                     0.1 * freshness)\n        \n        return round(min(1.0, base_score), 4)\n    \n    def contextual_score(self, context=None, complexity=1.0):\n        base = self.score\n        \n        # Context-specific adjustment\n        if context and context in self.context_performance:\n            ctx_perf = self.context_performance[context]\n            if ctx_perf['total'] >= 2:\n                ctx_success_rate = ctx_perf['success'] / ctx_perf['total']\n                base *= (0.7 + 0.6 * ctx_success_rate)\n        \n        # Complexity-based adjustment\n        complexity_key = int(complexity * 2) / 2  # Round to nearest 0.5\n        if complexity_key in self.complexity_performance:\n            comp_perf = self.complexity_performance[complexity_key]\n            if comp_perf['total'] >= 2:\n                comp_success_rate = comp_perf['success'] / comp_perf['total']\n                base *= (0.8 + 0.4 * comp_success_rate)\n        \n        return min(1.0, base)\n    \n    def update_outcome(self, success, tokens, context=None, complexity=1.0):\n        if succ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "9eae5a47f753",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T08:20:58.172558"
      },
      {
        "id": "b1a87f259863",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "7141061518e23854",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_performance = deque(maxlen=20)\n        self.context_scores = defaultdict(lambda: deque(maxlen=10))\n        self.last_used = 0\n        self.complexity_handled = deque(maxlen=15)\n        self.execution_times = deque(maxlen=10)\n        self.adaptation_factor = 1.0\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))\n        base_score = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency calculation\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 3000))\n        \n        avg_time = sum(self.execution_times) / max(len(self.execution_times), 1) if self.execution_times else 1.0\n        time_efficiency = 1.0 / (1 + math.log(1 + avg_time / 2.0))\n        \n        # Advanced trend analysis with momentum\n        trend_score = 0\n        if len(self.recent_performance) >= 5:\n            recent_window = list(self.recent_performance)[-5:]\n            weights = [0.1, 0.15, 0.2, 0.25, 0.3]  # More weight on recent\n            weighted_recent = sum(p * w for p, w in zip(recent_window, weights))\n            \n            overall_avg = sum(self.recent_performance) / len(self.recent_performance)\n            trend_momentum = (weighted_recent - overall_avg) * 0.4\n            trend_score = max(-0.15, min(0.15, trend_momentum))\n        \n        # Context adaptability bonus\n        context_bonus = 0\n        if len(self.context_scores) > 1:\n            context_variance = self._calculate_context_variance()\n            context_bonus = min(0.1, context_variance * 0.2)\n        \n        # Complexity handling capability\n        complexity_bonus = 0\n        if self.complexity_handled:\n            avg_complexity = sum(self.complexity_handled) / len(self.complexity_handled)\n            complexity_bonus = min(0.08, avg_complexity * 0.02)\n        \n        # Dynamic staleness with decay\n        staleness_penalty = min(0.08, self.last_used * 0.005 * (1 + self.last_used * 0.001))\n        \n        # Adaptive learning bonus\n        learning_bonus = min(0.05, (self.adaptation_factor - 1.0) * 0.1)\n        \n        final_score = (0.45 * base_score + \n                      0.2 * token_efficiency + \n                      0.15 * time_efficiency +\n                      trend_score + \n                      context_bonus + \n                      complexity_bonus + \n                      learning_bonus - \n                      staleness_penalty)\n        \n        return round(max(0.0, min(1.0, final_score)), 4)\n    \n ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "2b4229e0d01e",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:27:37.486103"
      },
      {
        "id": "955eaa53f656",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "3f3dab569e9ee180",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\nimport time\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)  # Fixed-size for efficiency\n        self.context_performance = defaultdict(lambda: {'successes': 0, 'failures': 0, 'tokens': 0})\n        self.timestamps = deque(maxlen=20)\n        self.complexity_scores = []\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = self._calculate_confidence(total)\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        efficiency = self._calculate_efficiency()\n        \n        # Trend analysis with decay\n        recent_trend = self._calculate_trend()\n        \n        # Context adaptability bonus\n        context_bonus = self._calculate_context_adaptability()\n        \n        # Weighted combination with dynamic weights\n        base_score = 0.4 * adjusted_success_rate + 0.25 * efficiency + 0.25 * recent_trend + 0.1 * context_bonus\n        \n        # Stability bonus for consistent performers\n        stability_bonus = self._calculate_stability_bonus()\n        \n        final_score = min(1.0, base_score + stability_bonus)\n        return round(final_score, 4)\n    \n    def _calculate_confidence(self, total):\n        # Sigmoid confidence curve - faster confidence building\n        return 1.0 / (1 + math.exp(-0.3 * (total - 10)))\n    \n    def _calculate_efficiency(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        avg_tokens = self.total_tokens / total\n        # Improved efficiency calculation with diminishing returns\n        base_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 500))\n        \n        # Complexity-adjusted efficiency\n        if self.complexity_scores:\n            avg_complexity = sum(self.complexity_scores) / len(self.complexity_scores)\n            complexity_factor = 1.0 + (avg_complexity - 0.5) * 0.2  # Bonus for handling complex tasks\n            return min(1.0, base_efficiency * complexity_factor)\n        \n        return base_efficiency\n    \n    def _calculate_trend(self):\n        if len(self.recent_outcomes) < 2:\n            return 0.5\n        \n        # Exponential weighted moving average\n        weights = [math.exp(-0.1 * i) for i in range(len(self.recent_outcomes))]\n        weighted_outcomes = [outcome * weight for outcome, weight in zip(reversed(self.recent_outcomes), weights)]\n        \n        trend_score = sum(weighted_outcomes) / sum(weights) if weights else 0.5\n        \n        # Velocity component - rate of improvement\n        if len(self.recent_outcomes) >= 5:\n            ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "b698145872a2",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:32:54.986643"
      },
      {
        "id": "88a68a0e935c",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "07bed4b214e2a6e8",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)  # More efficient circular buffer\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=10)\n        self.complexity_scores = deque(maxlen=10)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: \n            return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))  # Smoother confidence curve\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 800))  # More generous threshold\n        \n        # Time efficiency if available\n        time_efficiency = 1.0\n        if self.execution_times:\n            avg_time = sum(self.execution_times) / len(self.execution_times)\n            time_efficiency = 1.0 / (1 + avg_time / 5.0)  # Normalize around 5 seconds\n        \n        # Complexity handling capability\n        complexity_bonus = 0\n        if self.complexity_scores:\n            avg_complexity = sum(self.complexity_scores) / len(self.complexity_scores)\n            complexity_bonus = 0.05 * min(1.0, avg_complexity / 3.0)\n        \n        # Enhanced recent performance with momentum\n        momentum = 0\n        if len(self.recent_outcomes) >= 3:\n            recent_window = list(self.recent_outcomes)[-7:]  # Last 7 attempts\n            recent_success = sum(recent_window) / len(recent_window)\n            \n            # Calculate momentum (improving vs declining)\n            if len(recent_window) >= 6:\n                first_half = sum(recent_window[:3]) / 3\n                second_half = sum(recent_window[-3:]) / 3\n                momentum = 0.15 * (second_half - first_half)\n            \n            trend_boost = 0.12 * (recent_success - success_rate)\n            momentum += trend_boost\n        \n        # Context adaptability bonus\n        context_bonus = 0\n        if len(self.context_performance) > 1:\n            context_scores = []\n            for ctx_data in self.context_performance.values():\n                if ctx_data['total'] >= 2:\n                    ctx_score = ctx_data['success'] / ctx_data['total']\n                    context_scores.append(ctx_score)\n            if context_scores:\n                context_consistency = 1.0 - (max(context_scores) - min(context_scores))\n                context_bonus = 0.08 * context_consistency\n        \n        # Weighted composite score\n        base_score = (0.45 * success_rate + \n                     0.25 * token_efficiency + \n                     0.15 * time_efficiency + ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "8a96745bce82",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:33:22.024282"
      },
      {
        "id": "fc621de0c50a",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "a9a31163993f6822",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.consecutive_failures = 0\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Adaptive success rate with Bayesian confidence\n        success_rate = self.successes / total\n        alpha, beta = self.successes + 1, self.failures + 1\n        bayesian_mean = alpha / (alpha + beta)\n        confidence = min(1.0, total / 15)\n        adjusted_success_rate = bayesian_mean * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 3000))\n        \n        # Enhanced recent performance with momentum\n        recent_weight = 0.3\n        if len(self.recent_outcomes) >= 5:\n            recent_rate = sum(list(self.recent_outcomes)[-8:]) / min(8, len(self.recent_outcomes))\n            momentum = recent_rate - success_rate\n            recent_weight = 0.3 + 0.1 * max(-1, min(1, momentum * 2))\n        \n        # Penalty for consecutive failures\n        failure_penalty = max(0.8, 1.0 - self.consecutive_failures * 0.05)\n        \n        # Freshness bonus for recently used strategies\n        freshness = max(0.9, 1.0 - self.last_used * 0.01)\n        \n        base_score = (0.5 * adjusted_success_rate + 0.25 * token_efficiency + \n                     recent_weight * (recent_rate if len(self.recent_outcomes) >= 5 else success_rate))\n        \n        return round(base_score * failure_penalty * freshness, 3)\n    \n    def context_score(self, context, complexity=1.0):\n        if context in self.context_performance:\n            ctx_data = self.context_performance[context]\n            if ctx_data['total'] >= 2:\n                ctx_success_rate = ctx_data['success'] / ctx_data['total']\n                ctx_efficiency = 1.0 / (1 + ctx_data['tokens'] / max(ctx_data['total'] * 2000, 1))\n                return 0.7 * ctx_success_rate + 0.3 * ctx_efficiency\n        \n        # Complexity-based fallback\n        if complexity in self.complexity_performance:\n            comp_data = self.complexity_performance[int(complexity * 10)]\n            if comp_data['total'] > 0:\n                return comp_data['success'] / comp_data['total']\n        \n        return self.score * 0.9  # Slight penalty for unknown context\n    \n    def update_outcome(self, success, tokens, context=None, complexity=1.0):\n        if success:\n            self.successes += 1\n            self.consecutive_",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "9eae5a47f753",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T08:44:25.434473"
      },
      {
        "id": "64cea7c287f3",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "78e75c5499589bc7",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)  # More efficient bounded queue\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=10)\n        self.complexity_scores = deque(maxlen=10)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: \n            return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))  # Sqrt for smoother confidence growth\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / total\n        token_efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 800))  # More generous threshold\n        \n        # Recent performance with momentum detection\n        recent_boost = 0\n        if len(self.recent_outcomes) >= 4:\n            recent_window = list(self.recent_outcomes)[-6:]\n            recent_success = sum(recent_window) / len(recent_window)\n            momentum = self._calculate_momentum(recent_window)\n            recent_boost = 0.15 * (recent_success - success_rate) + 0.05 * momentum\n        \n        # Context adaptability bonus\n        context_bonus = self._calculate_context_adaptability()\n        \n        # Performance stability factor\n        stability = self._calculate_stability()\n        \n        # Composite scoring with dynamic weights\n        base_score = (0.5 * success_rate + \n                     0.25 * token_efficiency + \n                     0.15 * stability + \n                     0.1 * context_bonus)\n        \n        final_score = (confidence * (base_score + recent_boost) + \n                      (1 - confidence) * 0.5)\n        \n        return round(min(1.0, max(0.0, final_score)), 3)\n    \n    def _calculate_momentum(self, outcomes):\n        if len(outcomes) < 3:\n            return 0\n        # Calculate trend in recent outcomes\n        weights = [i + 1 for i in range(len(outcomes))]\n        weighted_avg = sum(o * w for o, w in zip(outcomes, weights)) / sum(weights)\n        simple_avg = sum(outcomes) / len(outcomes)\n        return weighted_avg - simple_avg\n    \n    def _calculate_context_adaptability(self):\n        if not self.context_performance:\n            return 0\n        \n        context_scores = []\n        for ctx_data in self.context_performance.values():\n            if ctx_data['total'] > 0:\n                ctx_success = ctx_data['success'] / ctx_data['total']\n                context_scores.append(ctx_success)\n        \n        if not context_scores:\n            return 0\n        \n        # Reward consistent performance across contexts\n        avg_performance = sum(context_scores) / len(con",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "8a96745bce82",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:44:51.561197"
      },
      {
        "id": "8e70c75511ce",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3",
        "module": "orchestration.planner",
        "code_hash": "9df0242da4c0032f",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=15)\n        self.complexity_scores = deque(maxlen=15)\n        self.learning_rate = 0.1\n        self.adaptation_history = deque(maxlen=50)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: \n            return 0.5\n        \n        # Enhanced success rate with exponential confidence\n        success_rate = self.successes / total\n        confidence = 1 - math.exp(-total / 10)  # Exponential confidence growth\n        \n        # Advanced token efficiency with dynamic scaling\n        avg_tokens = self.total_tokens / total\n        token_efficiency = math.exp(-avg_tokens / 1200)  # Exponential decay for efficiency\n        \n        # Sophisticated momentum with trend analysis\n        recent_boost = self._calculate_advanced_momentum()\n        \n        # Multi-dimensional context adaptability\n        context_bonus = self._calculate_enhanced_context_adaptability()\n        \n        # Performance stability with variance consideration\n        stability = self._calculate_enhanced_stability()\n        \n        # Learning velocity bonus\n        learning_velocity = self._calculate_learning_velocity()\n        \n        # Dynamic weight adjustment based on performance phase\n        weights = self._get_adaptive_weights(total, success_rate)\n        \n        base_score = (weights['success'] * success_rate + \n                     weights['efficiency'] * token_efficiency + \n                     weights['stability'] * stability + \n                     weights['context'] * context_bonus +\n                     weights['learning'] * learning_velocity)\n        \n        final_score = confidence * (base_score + recent_boost) + (1 - confidence) * 0.5\n        return max(0.0, min(1.0, final_score))\n    \n    def _calculate_advanced_momentum(self):\n        if len(self.recent_outcomes) < 6:\n            return 0\n        \n        recent = list(self.recent_outcomes)\n        # Weighted recent performance (more weight to recent)\n        weights = [math.exp(i * 0.1) for i in range(len(recent))]\n        weighted_recent = sum(w * o for w, o in zip(weights, recent)) / sum(weights)\n        \n        # Trend analysis with regression\n        trend = self._calculate_trend(recent)\n        \n        # Momentum combines weighted performance and trend\n        momentum = 0.2 * (weighted_recent - 0.5) + 0.1 * trend\n        return momentum\n    \n    def _calculate_trend(self, outcomes):\n        if len(outcomes) < 4:\n            return 0\n        n = len(outcomes)\n        x_mean = (n - 1) / 2\n        y_mean = sum(outcomes) / n\n        numer",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 3,
        "parent_id": "64cea7c287f3",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:51:03.773490"
      },
      {
        "id": "cb2e3ebe4664",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3",
        "module": "orchestration.planner",
        "code_hash": "664d51b6faf7e38b",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=15)\n        self.complexity_scores = deque(maxlen=15)\n        self.adaptation_history = deque(maxlen=10)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.6\n        \n        # Dynamic success rate with exponential confidence\n        success_rate = self.successes / total\n        confidence = 1 - math.exp(-total / 5)  # Exponential confidence buildup\n        base_rate = 0.45 + 0.1 * min(total / 10, 1)  # Adaptive baseline\n        adjusted_success_rate = success_rate * confidence + base_rate * (1 - confidence)\n        \n        # Advanced efficiency metrics\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = math.exp(-avg_tokens / 1200)  # Exponential penalty for token usage\n        \n        # Performance velocity (improvement over time)\n        velocity_bonus = 1.0\n        if len(self.recent_outcomes) >= 8:\n            first_half = sum(list(self.recent_outcomes)[:len(self.recent_outcomes)//2])\n            second_half = sum(list(self.recent_outcomes)[len(self.recent_outcomes)//2:])\n            improvement = (second_half - first_half) / (len(self.recent_outcomes)//2)\n            velocity_bonus = 1.0 + 0.2 * max(-0.5, min(0.5, improvement))\n        \n        # Context adaptability score\n        adaptability = 1.0\n        if len(self.context_performance) > 1:\n            context_scores = [ctx['success']/max(ctx['total'], 1) for ctx in self.context_performance.values() if ctx['total'] > 0]\n            if context_scores:\n                adaptability = 1.0 + 0.15 * (1 - (max(context_scores) - min(context_scores)))\n        \n        return min(0.95, adjusted_success_rate * 0.4 + token_efficiency * 0.25 + velocity_bonus * 0.2 + adaptability * 0.15)\n\nclass TaskComplexityAnalyzer:\n    def __init__(self):\n        self.complexity_factors = {\n            'dependencies': 0.3,\n            'data_volume': 0.25,\n            'time_constraints': 0.2,\n            'resource_requirements': 0.15,\n            'uncertainty_level': 0.1\n        }\n    \n    def analyze(self, task_metadata):\n        complexity = 0\n        for factor, weight in self.complexity_factors.items():\n            value = task_metadata.get(factor, 0.5)\n            complexity += value * weight\n        return min(1.0, complexity)\n\nclass AdaptivePlanner:\n    def __init__(self):\n        self.strategies = {}\n        self.global_context = {}\n        self.complexity_analyzer = TaskComplexityAnalyzer()\n        self.learning_rate = 0.1\n        self.exploration_rate = 0.15\n        \n    def register_strategy(",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 3,
        "parent_id": "6e9ea37d4f72",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:55:55.064262"
      },
      {
        "id": "1f10cca07d19",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3",
        "module": "orchestration.planner",
        "code_hash": "4940639c1870edce",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=15)\n        self.complexity_scores = deque(maxlen=15)\n        self.adaptation_history = deque(maxlen=10)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: \n            return 0.6  # Slightly optimistic initial score\n        \n        # Base success rate with confidence weighting\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # Faster confidence building\n        base_score = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Token efficiency with adaptive thresholds\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1.0 / (1 + (avg_tokens / 1000) ** 0.8)\n        \n        # Time efficiency with outlier resistance\n        time_efficiency = 1.0\n        if len(self.execution_times) >= 3:\n            sorted_times = sorted(self.execution_times)\n            median_time = sorted_times[len(sorted_times) // 2]\n            time_efficiency = 1.0 / (1 + median_time / 4.0)\n        \n        # Complexity mastery bonus\n        complexity_bonus = 0\n        if len(self.complexity_scores) >= 3:\n            max_complexity = max(self.complexity_scores)\n            avg_complexity = sum(self.complexity_scores) / len(self.complexity_scores)\n            complexity_bonus = 0.1 * min(1.0, (max_complexity * avg_complexity) / 9.0)\n        \n        # Recent momentum with trend analysis\n        momentum = 0\n        if len(self.recent_outcomes) >= 5:\n            recent = list(self.recent_outcomes)\n            recent_rate = sum(recent[-8:]) / min(8, len(recent))\n            overall_rate = sum(recent) / len(recent)\n            momentum = 0.15 * max(-0.5, min(0.5, recent_rate - overall_rate))\n        \n        # Adaptation learning bonus\n        adaptation_bonus = 0\n        if len(self.adaptation_history) >= 3:\n            improvements = sum(1 for i in range(1, len(self.adaptation_history)) \n                             if self.adaptation_history[i] > self.adaptation_history[i-1])\n            adaptation_bonus = 0.05 * (improvements / (len(self.adaptation_history) - 1))\n        \n        # Composite score with balanced weighting\n        final_score = (base_score * 0.5 + \n                      token_efficiency * 0.2 + \n                      time_efficiency * 0.15 + \n                      complexity_bonus + \n                      momentum + \n                      adaptation_bonus)\n        \n        return max(0.0, min(1.0, final_score))\n\nclass AdaptivePlanner:\n    def __init__(self):\n        self.strategies = {}\n        self.c",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 3,
        "parent_id": "88a68a0e935c",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T08:56:48.446355"
      },
      {
        "id": "bfdf55e943dd",
        "strategy_name": "Strategy Selector_gen1",
        "module": "orchestration.planner",
        "code_hash": "6e591225b85988d9",
        "code_snippet": "import math\nfrom collections import defaultdict\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = []\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Base success rate with confidence interval adjustment\n        success_rate = self.successes / total\n        confidence = min(1.0, total / 10)  # More confident with more data\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Efficiency with logarithmic scaling\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 1000))\n        \n        # Recent performance trend\n        recent_trend = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success_rate = sum(self.recent_outcomes[-5:]) / len(self.recent_outcomes[-5:])\n            trend_factor = recent_success_rate / max(success_rate, 0.01)\n            recent_trend = min(1.2, max(0.8, trend_factor))\n        \n        return round(0.6 * adjusted_success_rate + 0.25 * efficiency + 0.15 * recent_trend, 3)\n    \n    def update_outcome(self, success, tokens_used, context=None):\n        if success:\n            self.successes += 1\n        else:\n            self.failures += 1\n        self.total_tokens += tokens_used\n        self.recent_outcomes.append(1 if success else 0)\n        if len(self.recent_outcomes) > 10:\n            self.recent_outcomes.pop(0)\n        \n        if context:\n            self.context_performance[context]['total'] += 1\n            if success:\n                self.context_performance[context]['success'] += 1\n\ndef select_strategy(records, task_size, context=None, exploration_rate=0.1):\n    # Define strategy preferences based on task characteristics\n    strategy_preferences = {\n        'small': {'solo': 1.2, 'pipeline': 1.1, 'parallel': 0.8, 'debate': 0.7},\n        'medium': {'pipeline': 1.2, 'parallel': 1.1, 'solo': 0.9, 'debate': 1.0},\n        'large': {'parallel': 1.2, 'debate': 1.1, 'pipeline': 0.9, 'solo': 0.6}\n    }\n    \n    # Get base preferences for task size\n    prefs = strategy_preferences.get(task_size, {r.name: 1.0 for r in records})\n    \n    # Calculate context-aware scores\n    scored_records = []\n    for record in records:\n        base_score = record.score\n        \n        # Apply task size preference\n        preference_multiplier = prefs.get(record.name, 1.0)\n        \n        # Apply context-specific performance if available\n        context_multiplier = 1.0\n        if context and context in record.context_performance:\n            ctx_perf = record.context_performance[context]\n            if ctx_perf['total'] > 0:\n                ctx_success_rate = ctx_perf['success'] / ctx_perf['total']\n           ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 1,
        "parent_id": "17d4ca2306e0",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T09:06:51.577674"
      },
      {
        "id": "8eba2cdacff0",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3_gen4",
        "module": "orchestration.planner",
        "code_hash": "d7e37878c5a16ab6",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=25)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.execution_times = deque(maxlen=20)\n        self.complexity_scores = deque(maxlen=20)\n        self.adaptation_history = deque(maxlen=15)\n        self.streak_count = 0\n        self.last_outcome = None\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.65\n        \n        # Enhanced success rate with sigmoid confidence\n        success_rate = self.successes / total\n        confidence = 2 / (1 + math.exp(-total / 3)) - 1  # Sigmoid confidence\n        base_rate = 0.5 + 0.08 * min(total / 12, 1)\n        adjusted_success_rate = success_rate * confidence + base_rate * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        token_efficiency = 1 / (1 + (avg_tokens / 1000) ** 1.5)\n        \n        # Momentum and streak bonuses\n        momentum_bonus = 1.0\n        if len(self.recent_outcomes) >= 10:\n            recent_success_rate = sum(self.recent_outcomes) / len(self.recent_outcomes)\n            momentum_bonus = 1.0 + 0.25 * (recent_success_rate - 0.5)\n        \n        streak_bonus = 1.0 + min(0.3, self.streak_count * 0.05)\n        \n        # Context adaptability with variance penalty\n        adaptability = 1.0\n        if len(self.context_performance) > 1:\n            context_scores = [ctx['success']/max(ctx['total'], 1) for ctx in self.context_performance.values() if ctx['total'] > 2]\n            if len(context_scores) > 1:\n                avg_context = sum(context_scores) / len(context_scores)\n                variance = sum((s - avg_context) ** 2 for s in context_scores) / len(context_scores)\n                adaptability = 1.0 + 0.2 * avg_context - 0.15 * math.sqrt(variance)\n        \n        # Execution time consistency\n        time_consistency = 1.0\n        if len(self.execution_times) > 5:\n            avg_time = sum(self.execution_times) / len(self.execution_times)\n            time_variance = sum((t - avg_time) ** 2 for t in self.execution_times) / len(self.execution_times)\n            time_consistency = 1.0 - min(0.2, math.sqrt(time_variance) / max(avg_time, 1))\n        \n        return min(1.0, adjusted_success_rate * token_efficiency * momentum_bonus * streak_bonus * adaptability * time_consistency)\n    \n    def record_outcome(self, success, tokens_used, execution_time, context_key=None):\n        if success:\n            self.successes += 1\n            self.streak_count = self.streak_count + 1 if self.last_outcome else 1\n        else:\n            self.failures += 1\n            self.streak_count = 0\n        \n        self.last_out",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 4,
        "parent_id": "cb2e3ebe4664",
        "source_paper": "2602.14993v1",
        "created_at": "2026-02-18T09:07:15.120744"
      },
      {
        "id": "d3b535577b77",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3",
        "module": "orchestration.planner",
        "code_hash": "36f9c7ce7457a96f",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.consecutive_failures = 0\n        self.adaptation_rate = 0.1\n        self.context_weights = defaultdict(float)\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.6  # Slightly optimistic initial score\n        \n        # Enhanced Bayesian success rate with dynamic priors\n        success_rate = self.successes / total\n        alpha = self.successes + 2  # Stronger prior\n        beta = self.failures + 1\n        bayesian_mean = alpha / (alpha + beta)\n        confidence = min(1.0, total / 12)  # Faster confidence building\n        adjusted_success_rate = bayesian_mean * confidence + 0.6 * (1 - confidence)\n        \n        # Improved token efficiency with adaptive scaling\n        if total > 0:\n            avg_tokens = self.total_tokens / total\n            optimal_tokens = 2500  # Target token count\n            token_efficiency = math.exp(-abs(avg_tokens - optimal_tokens) / (2 * optimal_tokens))\n        else:\n            token_efficiency = 0.8\n        \n        # Advanced recent performance with trend analysis\n        recent_score = adjusted_success_rate\n        if len(self.recent_outcomes) >= 3:\n            recent_outcomes_list = list(self.recent_outcomes)\n            recent_rate = sum(recent_outcomes_list[-10:]) / min(10, len(recent_outcomes_list))\n            \n            # Trend detection\n            if len(recent_outcomes_list) >= 6:\n                early_recent = sum(recent_outcomes_list[-10:-5]) / 5 if len(recent_outcomes_list) >= 10 else sum(recent_outcomes_list[:-5]) / max(1, len(recent_outcomes_list) - 5)\n                late_recent = sum(recent_outcomes_list[-5:]) / 5\n                trend = (late_recent - early_recent) * 0.5  # Trend bonus/penalty\n                recent_rate = max(0, min(1, recent_rate + trend))\n            \n            recent_score = recent_rate\n        \n        # Adaptive failure penalty with recovery mechanism\n        if self.consecutive_failures > 0:\n            failure_penalty = max(0.7, 1.0 - (self.consecutive_failures * 0.08))\n            # Recovery bonus if recent performance is good despite failures\n            if recent_score > 0.7:\n                failure_penalty = min(1.0, failure_penalty + 0.1)\n        else:\n            failure_penalty = 1.0\n        \n        # Dynamic freshness with usage optimization\n        staleness_penalty = max(0.85, 1.0 - self.last_used * 0.008)\n        \n        # Context-aware scoring\n        context_bonus = 1.0\n        if self.context",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 3,
        "parent_id": "fc621de0c50a",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T09:11:35.432348"
      },
      {
        "id": "6a382f7527fb",
        "strategy_name": "Strategy Selector_gen1_gen2",
        "module": "orchestration.planner",
        "code_hash": "8f50e7181e78bf95",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=15)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'tokens': 0})\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.streak = 0\n        self.streak_positive = True\n    \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Enhanced success rate with adaptive confidence\n        success_rate = self.successes / total\n        confidence = min(1.0, math.sqrt(total / 8))\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Multi-factor efficiency scoring\n        avg_tokens = self.total_tokens / max(total, 1)\n        efficiency = 1.0 / (1 + math.log(1 + avg_tokens / 4000))\n        \n        # Enhanced recent performance with momentum\n        recent_trend = 1.0\n        if len(self.recent_outcomes) >= 3:\n            recent_success_rate = sum(self.recent_outcomes) / len(self.recent_outcomes)\n            momentum = self._calculate_momentum()\n            trend_factor = (recent_success_rate / max(success_rate, 0.1)) * momentum\n            recent_trend = min(1.3, max(0.7, trend_factor))\n        \n        # Streak bonus/penalty\n        streak_factor = 1.0\n        if abs(self.streak) >= 3:\n            streak_factor = 1.1 if self.streak_positive else 0.9\n        \n        # Recency factor (prefer recently successful strategies)\n        recency_factor = 1.0 + (0.1 * math.exp(-self.last_used / 5))\n        \n        base_score = 0.5 * adjusted_success_rate + 0.3 * efficiency + 0.2 * recent_trend\n        return round(base_score * streak_factor * recency_factor, 3)\n    \n    def _calculate_momentum(self):\n        if len(self.recent_outcomes) < 3:\n            return 1.0\n        \n        # Calculate weighted momentum (recent outcomes matter more)\n        weights = [i + 1 for i in range(len(self.recent_outcomes))]\n        weighted_sum = sum(outcome * weight for outcome, weight in zip(self.recent_outcomes, weights))\n        total_weight = sum(weights)\n        return weighted_sum / total_weight if total_weight > 0 else 1.0\n    \n    def update_outcome(self, success, tokens, context=None, complexity=1.0):\n        if success:\n            self.successes += 1\n            if self.streak_positive:\n                self.streak += 1\n            else:\n                self.streak = 1\n                self.streak_positive = True\n        else:\n            self.failures += 1\n            if not self.streak_positive:\n                self.streak += 1\n            else:\n                self.streak = 1\n                self.streak_positive = False\n        \n        self.total_tokens += tokens\n     ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 2,
        "parent_id": "9eae5a47f753",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T09:12:08.578421"
      },
      {
        "id": "263c79082b94",
        "strategy_name": "Strategy Selector_gen1_gen2_gen3",
        "module": "orchestration.planner",
        "code_hash": "c78155e39eba7988",
        "code_snippet": "import math\nimport random\nfrom collections import defaultdict, deque\n\nclass StrategyRecord:\n    def __init__(self, name):\n        self.name = name\n        self.successes = 0\n        self.failures = 0\n        self.total_tokens = 0\n        self.recent_outcomes = deque(maxlen=20)\n        self.context_performance = defaultdict(lambda: {'success': 0, 'total': 0, 'avg_tokens': 0})\n        self.complexity_performance = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.last_used = 0\n        self.streak = 0\n        self.best_streak = 0\n        self.adaptation_factor = 1.0\n        self.context_specialization = defaultdict(float)\n        \n    @property\n    def score(self):\n        total = self.successes + self.failures\n        if total == 0: return 0.5\n        \n        # Dynamic confidence with experience curve\n        success_rate = self.successes / total\n        confidence = min(1.0, (total / 10) ** 0.6)\n        adjusted_success_rate = success_rate * confidence + 0.5 * (1 - confidence)\n        \n        # Adaptive token efficiency with learning curve\n        avg_tokens = self.total_tokens / max(total, 1)\n        optimal_tokens = 2500 * self.adaptation_factor\n        token_efficiency = 1.0 / (1 + abs(avg_tokens - optimal_tokens) / optimal_tokens)\n        \n        # Enhanced momentum with trend analysis\n        recent_weight = 1.0\n        if len(self.recent_outcomes) >= 5:\n            recent_success = sum(self.recent_outcomes) / len(self.recent_outcomes)\n            \n            # Trend analysis for momentum\n            if len(self.recent_outcomes) >= 10:\n                first_half = sum(list(self.recent_outcomes)[:len(self.recent_outcomes)//2])\n                second_half = sum(list(self.recent_outcomes)[len(self.recent_outcomes)//2:])\n                trend = (second_half / (len(self.recent_outcomes)//2)) - (first_half / (len(self.recent_outcomes)//2))\n                recent_weight = 1.0 + (trend * 0.3)\n            \n            # Streak multiplier with diminishing returns\n            if self.streak >= 3:\n                streak_bonus = min(1.25, 1 + math.log(self.streak) * 0.1)\n                recent_weight *= streak_bonus\n        \n        # Context specialization bonus\n        specialization_bonus = 1.0\n        if self.context_specialization:\n            max_specialization = max(self.context_specialization.values())\n            specialization_bonus = 1.0 + (max_specialization * 0.15)\n        \n        # Adaptive freshness with usage patterns\n        usage_decay = max(0.85, 1.0 - (self.last_used * 0.03))\n        exploration_bonus = 1.0 if self.last_used > 5 else 0.95\n        freshness = usage_decay * exploration_bonus\n        \n        # Multi-dimensional scoring with adaptive weights\n        weights = self._get_adaptive_weights(total)\n        base_score = (weights['success'] * adjusted_success_rate + \n                     weights['efficiency'] * token_efficiency + \n                     weights['momentum'] * min(1.5, recent_weight) + \n              ",
        "fitness_scores": [],
        "current_fitness": 0.5,
        "generation": 3,
        "parent_id": "2bfaf170786c",
        "source_paper": "2602.15809v1",
        "created_at": "2026-02-18T09:17:38.721616"
      }
    ]
  }
}