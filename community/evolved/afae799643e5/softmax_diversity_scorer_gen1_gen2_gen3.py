"""Evolved strategy: Softmax Diversity Scorer_gen1_gen2_gen3

Auto-generated by agos evolution engine.
Source paper: 2602.15021v1
Target module: knowledge.semantic
Generated: 2026-02-19T19:41:11.399245
Defines: SemanticScorer
Code hash: 32a6b9a306cda948

Sandbox output: 
"""

from __future__ import annotations

import logging
from typing import Any

from agos.evolution.integrator import IntegrationStrategy, EvolutionProposal

_logger = logging.getLogger(__name__)

# ── Evolved pattern code ──────────────────────────────────────────
# This code passed sandbox validation and EXECUTES when apply() is called.

PATTERN_CODE = 'import math\nimport random\nfrom collections import defaultdict, deque\nimport json\nimport re\n\nclass SemanticScorer:\n    def __init__(self, temperature=0.3, diversity_weight=0.2, memory_size=100):\n        self.temperature = max(temperature, 0.01)\n        self.diversity_weight = diversity_weight\n        self.history = defaultdict(float)\n        self.recent_selections = deque(maxlen=memory_size)\n        self.context_embeddings = {}\n        self.semantic_clusters = defaultdict(set)\n        self.decay_factor = 0.95\n        self.semantic_cache = {}\n        self.pattern_weights = defaultdict(float)\n        \n    def softmax_score(self, values, labels=None, context=None):\n        if not values: return []\n        \n        # Apply temperature scaling with context awareness\n        temp = self._adaptive_temperature(context)\n        exp_vals = [math.exp(min(v / temp, 700)) for v in values]  # Prevent overflow\n        total = sum(exp_vals)\n        base_scores = [v / total for v in exp_vals]\n        \n        if labels and len(labels) == len(values):\n            # Multi-factor scoring with enhanced semantic analysis\n            diversity_bonus = self._calculate_diversity_bonus(labels)\n            semantic_bonus = self._calculate_semantic_bonus(labels, context)\n            recency_penalty = self._calculate_recency_penalty(labels)\n            pattern_bonus = self._calculate_pattern_bonus(labels, context)\n            \n            adjusted_scores = []\n            for i, score in enumerate(base_scores):\n                label = labels[i]\n                diversity_mult = 1 + self.diversity_weight * diversity_bonus.get(label, 0)\n                semantic_mult = 1 + 0.15 * semantic_bonus.get(label, 0)\n                recency_mult = 1 - 0.1 * recency_penalty.get(label, 0)\n                pattern_mult = 1 + 0.1 * pattern_bonus.get(label, 0)\n                \n                final_score = score * diversity_mult * semantic_mult * recency_mult * pattern_mult\n                adjusted_scores.append(max(final_score, 0.001))\n            \n            # Renormalize with smoothing\n            total_adjusted = sum(adjusted_scores)\n            return [s / total_adjusted for s in adjusted_scores]\n        \n        return base_scores\n    \n    def _adaptive_temperature(self, context=None):\n        base_temp = self.temperature\n        if context:\n            # Adjust temperature based on context complexity\n            complexity = self._estimate_complexity(context)\n            return base_temp * (1 + 0.3 * complexity)\n        return base_temp\n    \n    def _estimate_complexity(self, context):\n        if isinstance(context, str):\n            # Simple complexity heuristic based on text features\n            word_count = len(context.split())\n            unique_words = len(set(context.lower().split()))\n            complexity = min(unique_words / max(word_count, 1), 1.0)\n            return complexity\n        return 0.5\n    \n    def _calculate_diversity_bonus(self, labels):\n        bonus = {}\n        label_counts = defaultdict(int)\n        \n        # Count recent selections\n        for item in self.recent_selections:\n            if item in labels:\n                label_counts[item] += 1\n        \n        # Higher bonus for less frequently selected items\n        max_count = max(label_counts.values()) if label_counts else 1\n        for label in labels:\n            count = label_counts.get(label, 0)\n            bonus[label] = (max_count - count) / max(max_count, 1)\n        \n        return bonus\n    \n    def _calculate_semantic_bonus(self, labels, context):\n        if not context:\n            return {label: 0 for label in labels}\n        \n        cache_key = f"{hash(str(labels))}_{hash(str(context))}"\n        if cache_key in self.semantic_cache:\n            return self.semantic_cache[cache_key]\n        \n        bonus = {}\n        context_tokens = self._extract_semantic_tokens(context)\n        \n        for label in labels:\n            label_tokens = self._extract_semantic_tokens(str(label))\n            similarity = self._calculate_token_similarity(context_tokens, label_tokens)\n            bonus[label] = similarity\n        \n        self.semantic_cache[cache_key] = bonus\n        return bonus\n    \n    def _calculate_recency_penalty(self, labels):\n        penalty = {}\n        recent_items = list(self.recent_selections)\n        \n        for label in labels:\n            penalty_score = 0\n            for i, recent_item in enumerate(reversed(recent_items[-10:])):\n                if recent_item == label:\n                    # Higher penalty for more recent selections\n                    penalty_score += (10 - i) / 10.0\n            penalty[label] = min(penalty_score, 1.0)\n        \n        return penalty\n    \n    def _calculate_pattern_bonus(self, labels, context):\n        if not context:\n            return {label: 0 for label in labels}\n        \n        bonus = {}\n        context_patterns = self._extract_patterns(str(context))\n        \n        for label in labels:\n            label_patterns = self._extract_patterns(str(label))\n            pattern_match = len(context_patterns & label_patterns) / max(len(context_patterns | label_patterns), 1)\n            bonus[label] = pattern_match\n        \n        return bonus\n    \n    def _extract_semantic_tokens(self, text):\n        if not isinstance(text, str):\n            text = str(text)\n        # Extract meaningful tokens (words, numbers, identifiers)\n        tokens = re.findall(r\'\\b\\w+\\b\', text.lower())\n        return set(tokens)\n    \n    def _extract_patterns(self, text):\n        if not isinstance(text, str):\n            text = str(text)\n        patterns = set()\n        # Extract common patterns\n        patterns.update(re.findall(r\'\\b\\w+_\\w+\\b\', text))  # snake_case\n        patterns.update(re.findall(r\'\\b[A-Z][a-z]+(?:[A-Z][a-z]+)*\\b\', text))  # CamelCase\n        patterns.update(re.findall(r\'\\b\\d+\\b\', text))  # numbers\n        return patterns\n    \n    def _calculate_token_similarity(self, tokens1, tokens2):\n        if not tokens1 or not tokens2:\n            return 0.0\n        intersection = len(tokens1 & tokens2)\n        union = len(tokens1 | tokens2)\n        return intersection / union if union > 0 else 0.0\n    \n    def update_selection(self, selected_label, context=None):\n        """Update internal state based on selection"""\n        self.recent_selections.append(selected_label)\n        self.history[selected_label] = self.history[selected_label] * self.decay_factor + 1.0\n        \n        if context:\n            # Update pattern weights based on successful selections\n            patterns = self._extract_patterns(str(context))\n            for pattern in patterns:\n                self.pattern_weights[pattern] = self.pattern_weights[pattern] * 0.9 + 0.1\n    \n    def get_recommendation(self, candidates, context=None):\n        """Get top recommendation with confidence score"""\n        if not candidates:\n            return None, 0.0\n        \n        values = [random.random() for _ in candidates]  # Placeholder for actual scoring\n        scores = self.softmax_score(values, candidates, context)\n        \n        if scores:\n            best_idx = max(range(len(scores)), key=lambda i: scores[i])\n            return candidates[best_idx], scores[best_idx]\n        \n        return candidates[0], 0.5'

PATTERN_HASH = "32a6b9a306cda948"


def _execute_pattern() -> dict[str, Any]:
    """Execute the pattern code and return all defined names."""
    namespace = {"__builtins__": __builtins__}
    exec(compile(PATTERN_CODE, "<evolved:softmax_diversity_scorer_gen1_gen2_gen3>", "exec"), namespace)
    return {k: v for k, v in namespace.items()
            if not k.startswith("_") and k != "__builtins__"}


# ── Strategy wrapper ──────────────────────────────────────────────


class SoftmaxDiversityScorerGen1Gen2Gen3Strategy(IntegrationStrategy):
    """Evolved strategy that EXECUTES: Softmax Diversity Scorer_gen1_gen2_gen3"""

    name = "softmax_diversity_scorer_gen1_gen2_gen3"
    target_module = "knowledge.semantic"

    def __init__(self, components: dict[str, Any] | None = None, **kwargs: Any) -> None:
        self._components = components or {}
        self._applied = False
        self._exports: dict[str, Any] = {}

    def validate(self, proposal: EvolutionProposal) -> tuple[bool, str]:
        return True, ""

    async def snapshot(self) -> dict[str, Any]:
        return {"applied": self._applied}

    async def apply(self, proposal: EvolutionProposal) -> list[str]:
        changes: list[str] = []

        # Execute the pattern code for real
        try:
            self._exports = _execute_pattern()
        except Exception as e:
            _logger.warning("Pattern execution failed for softmax_diversity_scorer_gen1_gen2_gen3: %s", e)
            return [f"Pattern execution failed: {e}"]

        exported = [k for k in self._exports
                    if callable(self._exports[k]) or isinstance(self._exports[k], type)]
        if exported:
            changes.append(f"Executed pattern, defined: {', '.join(exported[:10])}")
        else:
            changes.append("Executed pattern (no callable exports)")

        # Hook into live system components
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        target = "knowledge.semantic"

        # Wire exported functions/classes into the target component
        if target.startswith("knowledge") and loom is not None:
            semantic = getattr(loom, "semantic", None)
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into semantic weave")
                    elif not hasattr(loom, f"_evolved_{fn_name}"):
                        setattr(loom, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into knowledge manager")
                elif isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Registered {fn_name} class in semantic weave")

        elif target.startswith("intent") and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into intent system")

        elif target == "policy" and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into policy system")

        elif target.startswith("orchestration") and event_bus is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(event_bus, f"_evolved_{fn_name}"):
                        setattr(event_bus, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into orchestration")

        if not changes:
            changes.append("Pattern executed but no hookable exports found")

        self._applied = True
        changes.append("Source: 2602.15021v1")
        return changes

    async def rollback(self, snapshot_data: dict[str, Any]) -> None:
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        for fn_name in self._exports:
            for comp in [loom, getattr(loom, "semantic", None) if loom else None,
                         event_bus, audit_trail]:
                if comp is not None:
                    attr = f"_evolved_{fn_name}"
                    if hasattr(comp, attr):
                        try:
                            delattr(comp, attr)
                        except Exception:
                            pass
        self._applied = snapshot_data.get("applied", False)

    async def health_check(self) -> bool:
        try:
            _execute_pattern()
            return True
        except Exception:
            return False
