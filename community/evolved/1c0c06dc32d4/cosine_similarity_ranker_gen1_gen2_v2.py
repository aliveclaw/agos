"""Evolved strategy: Cosine Similarity Ranker_gen1_gen2

Auto-generated by agos evolution engine.
Source paper: 2602.15021v1
Target module: knowledge.semantic
Generated: 2026-02-18T08:32:44.692932
Defines: SemanticIndex
Code hash: 998bef6812d22bfe

Sandbox output: 
"""

from __future__ import annotations

import logging
from typing import Any

from agos.evolution.integrator import IntegrationStrategy, EvolutionProposal

_logger = logging.getLogger(__name__)

# ── Evolved pattern code ──────────────────────────────────────────
# This code passed sandbox validation and EXECUTES when apply() is called.

PATTERN_CODE = 'import math\nimport re\nfrom collections import Counter, defaultdict\nfrom functools import lru_cache\n\nclass SemanticIndex:\n    def __init__(self):\n        self.docs = {}\n        self.doc_vectors = {}\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_norms = {}\n        \n    def add_document(self, doc_id, text):\n        words = self._tokenize(text)\n        self.docs[doc_id] = words\n        self.vocab.update(words)\n        self._invalidate_cache()\n    \n    def _invalidate_cache(self):\n        self.idf_cache.clear()\n        self.doc_vectors.clear()\n        self.doc_norms.clear()\n    \n    def _tokenize(self, text):\n        # Enhanced tokenization with better word boundaries and filtering\n        words = re.findall(r\'\\b[a-zA-Z]{2,}\\b\', text.lower())\n        return [w for w in words if len(w) >= 2]\n    \n    def _compute_idf(self):\n        if self.idf_cache:\n            return self.idf_cache\n        \n        doc_freq = defaultdict(int)\n        for words in self.docs.values():\n            for word in set(words):\n                doc_freq[word] += 1\n        \n        total_docs = len(self.docs)\n        # Smoothed IDF to handle edge cases better\n        self.idf_cache = {word: math.log((total_docs + 1) / (freq + 1)) + 1\n                         for word, freq in doc_freq.items()}\n        return self.idf_cache\n    \n    def _tfidf_vector(self, words, use_cache=True, doc_id=None):\n        if use_cache and doc_id and doc_id in self.doc_vectors:\n            return self.doc_vectors[doc_id]\n            \n        tf = Counter(words)\n        idf = self._compute_idf()\n        doc_len = len(words)\n        \n        vector = {}\n        for word in self.vocab:\n            if word in tf:  # Only compute for words that exist\n                tf_score = tf[word] / max(doc_len, 1)\n                idf_score = idf.get(word, 0)\n                tfidf_score = tf_score * idf_score\n                if tfidf_score > 0:  # Sparse representation\n                    vector[word] = tfidf_score\n        \n        if use_cache and doc_id:\n            self.doc_vectors[doc_id] = vector\n            \n        return vector\n    \n    def _vector_norm(self, vector, doc_id=None):\n        if doc_id and doc_id in self.doc_norms:\n            return self.doc_norms[doc_id]\n            \n        norm = math.sqrt(sum(v*v for v in vector.values()))\n        \n        if doc_id:\n            self.doc_norms[doc_id] = norm\n            \n        return norm\n    \n    def cosine_similarity(self, vec1, vec2):\n        # Optimized sparse vector dot product\n        if not vec1 or not vec2:\n            return 0.0\n            \n        dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) \n                         for w in vec1.keys() & vec2.keys())\n        \n        norm1 = self._vector_norm(vec1)\n        norm2 = self._vector_norm(vec2)\n        \n        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0.0\n    \n    def search(self, query, top_k=5, min_score=0.01):\n        if not self.docs:\n            return []\n            \n        query_words = self._tokenize(query)\n        if not query_words:\n            return []\n            \n        query_vec = self._tfidf_vector(query_words, use_cache=False)\n        \n        scores = []\n        for doc_id, doc_words in self.docs.items():\n            doc_vec = self._tfidf_vector(doc_words, doc_id=doc_id)\n            similarity = self.cosine_similarity(query_vec, doc_vec)\n            \n            if similarity >= min_score:\n                scores.append((doc_id, similarity))\n        \n        scores.sort(key=lambda x: x[1], reverse=True)\n        return scores[:top_k] if top_k else scores\n    \n    def get_document_keywords(self, doc_id, top_k=10):\n        """Extract top keywords from a document based on TF-IDF scores"""\n        if doc_id not in self.docs:\n            return []\n            \n        doc_words = self.docs[doc_id]\n        doc_vec = self._tfidf_vector(doc_words, doc_id=doc_id)\n        \n        keywords = sorted(doc_vec.items(), key=lambda x: x[1], reverse=True)\n        return keywords[:top_k]\n    \n    def find_similar_documents(self, doc_id, top_k=5, min_score=0.1):\n        """Find documents similar to a given document"""\n        if doc_id not in self.docs:\n            return []\n            \n        target_vec = self._tfidf_vector(self.docs[doc_id], doc_id=doc_id)\n        \n        scores = []\n        for other_id, other_words in self.docs.items():\n            if other_id == doc_id:\n                continue\n                \n            other_vec = self._tfidf_vector(other_words, doc_id=other_id)\n            similarity = self.cosine_similarity(target_vec, other_vec)\n            \n            if similarity >= min_score:\n                scores.append((other_id, similarity))\n        \n        scores.sort(key=lambda x: x[1], reverse=True)\n        return scores[:top_k]'

PATTERN_HASH = "998bef6812d22bfe"


def _execute_pattern() -> dict[str, Any]:
    """Execute the pattern code and return all defined names."""
    namespace = {"__builtins__": __builtins__}
    exec(compile(PATTERN_CODE, "<evolved:cosine_similarity_ranker_gen1_gen2>", "exec"), namespace)
    return {k: v for k, v in namespace.items()
            if not k.startswith("_") and k != "__builtins__"}


# ── Strategy wrapper ──────────────────────────────────────────────


class CosineSimilarityRankerGen1Gen2Strategy(IntegrationStrategy):
    """Evolved strategy that EXECUTES: Cosine Similarity Ranker_gen1_gen2"""

    name = "cosine_similarity_ranker_gen1_gen2"
    target_module = "knowledge.semantic"

    def __init__(self, components: dict[str, Any] | None = None, **kwargs: Any) -> None:
        self._components = components or {}
        self._applied = False
        self._exports: dict[str, Any] = {}

    def validate(self, proposal: EvolutionProposal) -> tuple[bool, str]:
        return True, ""

    async def snapshot(self) -> dict[str, Any]:
        return {"applied": self._applied}

    async def apply(self, proposal: EvolutionProposal) -> list[str]:
        changes: list[str] = []

        # Execute the pattern code for real
        try:
            self._exports = _execute_pattern()
        except Exception as e:
            _logger.warning("Pattern execution failed for cosine_similarity_ranker_gen1_gen2: %s", e)
            return [f"Pattern execution failed: {e}"]

        exported = [k for k in self._exports
                    if callable(self._exports[k]) or isinstance(self._exports[k], type)]
        if exported:
            changes.append(f"Executed pattern, defined: {', '.join(exported[:10])}")
        else:
            changes.append("Executed pattern (no callable exports)")

        # Hook into live system components
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        target = "knowledge.semantic"

        # Wire exported functions/classes into the target component
        if target.startswith("knowledge") and loom is not None:
            semantic = getattr(loom, "semantic", None)
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into semantic weave")
                    elif not hasattr(loom, f"_evolved_{fn_name}"):
                        setattr(loom, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into knowledge manager")
                elif isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Registered {fn_name} class in semantic weave")

        elif target.startswith("intent") and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into intent system")

        elif target == "policy" and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into policy system")

        elif target.startswith("orchestration") and event_bus is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(event_bus, f"_evolved_{fn_name}"):
                        setattr(event_bus, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into orchestration")

        if not changes:
            changes.append("Pattern executed but no hookable exports found")

        self._applied = True
        changes.append("Source: 2602.15021v1")
        return changes

    async def rollback(self, snapshot_data: dict[str, Any]) -> None:
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        for fn_name in self._exports:
            for comp in [loom, getattr(loom, "semantic", None) if loom else None,
                         event_bus, audit_trail]:
                if comp is not None:
                    attr = f"_evolved_{fn_name}"
                    if hasattr(comp, attr):
                        try:
                            delattr(comp, attr)
                        except Exception:
                            pass
        self._applied = snapshot_data.get("applied", False)

    async def health_check(self) -> bool:
        try:
            _execute_pattern()
            return True
        except Exception:
            return False
