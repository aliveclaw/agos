"""Evolved strategy: Cosine Similarity Ranker_gen1_gen2

Auto-generated by agos evolution engine.
Source paper: 2602.15021v1
Target module: knowledge.semantic
Generated: 2026-02-18T08:07:16.859602
Defines: SemanticRanker
Code hash: 9b4c79a0207cf72b

Sandbox output: 
"""

from __future__ import annotations

import logging
from typing import Any

from agos.evolution.integrator import IntegrationStrategy, EvolutionProposal

_logger = logging.getLogger(__name__)

# ── Evolved pattern code ──────────────────────────────────────────
# This code passed sandbox validation and EXECUTES when apply() is called.

PATTERN_CODE = 'import math\nimport re\nfrom collections import Counter, defaultdict\nfrom functools import lru_cache\n\nclass SemanticRanker:\n    def __init__(self, docs=None, alpha=0.5):\n        self.docs = docs or []\n        self.alpha = alpha  # BM25 parameter\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_vectors = []\n        self.avg_doc_len = 0\n        self._build_index()\n    \n    def _build_index(self):\n        if not self.docs:\n            return\n        \n        # Build vocabulary and compute document statistics\n        doc_lengths = []\n        for doc in self.docs:\n            tokens = self._tokenize(doc)\n            self.vocab.update(tokens)\n            doc_lengths.append(len(tokens))\n        \n        self.vocab = sorted(self.vocab)\n        self.avg_doc_len = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 0\n        self._compute_idf()\n        self._precompute_vectors()\n    \n    @lru_cache(maxsize=1000)\n    def _tokenize(self, text):\n        # Enhanced tokenization with stemming-like suffix removal\n        tokens = re.findall(r\'\\b[a-z]+\\b\', text.lower())\n        normalized = []\n        for token in tokens:\n            # Simple suffix removal for better matching\n            if len(token) > 4:\n                if token.endswith((\'ing\', \'ion\', \'tion\')):\n                    token = token[:-3] if token.endswith(\'ing\') else token[:-4]\n                elif token.endswith((\'ed\', \'er\', \'ly\')):\n                    token = token[:-2]\n            normalized.append(token)\n        return tuple(normalized)  # Return tuple for caching\n    \n    def _compute_idf(self):\n        doc_freq = defaultdict(int)\n        for doc in self.docs:\n            words = set(self._tokenize(doc))\n            for word in words:\n                doc_freq[word] += 1\n        \n        n_docs = len(self.docs)\n        for word in self.vocab:\n            # Enhanced IDF with smoothing\n            df = doc_freq[word]\n            self.idf_cache[word] = math.log((n_docs - df + 0.5) / (df + 0.5))\n    \n    def _precompute_vectors(self):\n        # Pre-compute document vectors for efficiency\n        self.doc_vectors = []\n        for doc in self.docs:\n            self.doc_vectors.append(self._compute_vector(doc))\n    \n    def _compute_vector(self, text):\n        words = list(self._tokenize(text))\n        tf = Counter(words)\n        doc_len = len(words)\n        \n        vector = []\n        for word in self.vocab:\n            if word in tf:\n                # BM25-style scoring\n                tf_score = tf[word]\n                idf_score = self.idf_cache.get(word, 0)\n                \n                # BM25 term frequency normalization\n                normalized_tf = (tf_score * (1.2 + 1)) / (\n                    tf_score + 1.2 * (1 - self.alpha + self.alpha * (doc_len / max(self.avg_doc_len, 1)))\n                )\n                \n                vector.append(normalized_tf * idf_score)\n            else:\n                vector.append(0.0)\n        \n        return vector\n    \n    def _cosine_similarity(self, vec_a, vec_b):\n        dot_product = sum(a * b for a, b in zip(vec_a, vec_b))\n        norm_a = math.sqrt(sum(a * a for a in vec_a))\n        norm_b = math.sqrt(sum(b * b for b in vec_b))\n        return dot_product / (norm_a * norm_b) if norm_a and norm_b else 0.0\n    \n    def _semantic_boost(self, query_words, doc_words):\n        # Boost score for exact phrase matches and word proximity\n        query_set = set(query_words)\n        doc_set = set(doc_words)\n        \n        # Exact match bonus\n        exact_matches = len(query_set & doc_set)\n        match_ratio = exact_matches / len(query_set) if query_set else 0\n        \n        # Phrase proximity bonus\n        proximity_bonus = 0\n        if len(query_words) > 1:\n            doc_words_list = list(doc_words)\n            for i in range(len(query_words) - 1):\n                word1, word2 = query_words[i], query_words[i + 1]\n                if word1 in doc_words_list and word2 in doc_words_list:\n                    pos1 = doc_words_list.index(word1)\n                    pos2 = doc_words_list.index(word2)\n                    if abs(pos1 - pos2) <= 3:  # Words within 3 positions\n                        proximity_bonus += 0.1\n        \n        return match_ratio * 0.2 + proximity_bonus\n    \n    def rank_documents(self, query, top_k=None):\n        if not self.docs:\n            return []\n        \n        query_vec = self._compute_vector(query)\n        query_words = list(self._tokenize(query))\n        scores = []\n        \n        for i, doc_vec in enumerate(self.doc_vectors):\n            # Base cosine similarity\n            similarity = self._cosine_similarity(query_vec, doc_vec)\n            \n            # Add semantic boost\n            doc_words = list(self._tokenize(self.docs[i]))\n            boost = self._semantic_boost(query_words, doc_words)\n            \n            final_score = similarity + boost\n            scores.append((i, round(final_score, 4)))\n        \n        # Sort by score descending\n        ranked = sorted(scores, key=lambda x: -x[1])\n        \n        return ranked[:top_k] if top_k else ranked\n    \n    def add_document(self, doc):\n        """Dynamically add a document and update the index"""\n        self.docs.append(doc)\n        self._build_index()  # Rebuild index - could be optimized for incremental updates\n    \n    def get_document_summary(self, doc_idx, max_words=50):\n        """Get a summary of the document at the given index"""\n        if 0 <= doc_idx < len(self.docs):\n            words = self.docs[doc_idx].split()\n            return \' \'.join(words[:max_words]) + (\'...\' if len(words) > max_words else \'\')\n        return ""\n\n# Test the enhanced semantic ranker\ndocs = [\n    \'agent memory retrieval system for cognitive architectures\',\n    \'machine learning algorithms for pattern recognition\',\n    \'semantic search and information retrieval techniques\',\n    \'cognitive agent decision making processes\',\n    \'memory management in artificial intelligence systems\'\n]\n\nranker = SemanticRanker(docs)\nresults = ranker.rank_documents(\'agent memory cognitive\', top_k=3)'

PATTERN_HASH = "9b4c79a0207cf72b"


def _execute_pattern() -> dict[str, Any]:
    """Execute the pattern code and return all defined names."""
    namespace = {"__builtins__": __builtins__}
    exec(compile(PATTERN_CODE, "<evolved:cosine_similarity_ranker_gen1_gen2>", "exec"), namespace)
    return {k: v for k, v in namespace.items()
            if not k.startswith("_") and k != "__builtins__"}


# ── Strategy wrapper ──────────────────────────────────────────────


class CosineSimilarityRankerGen1Gen2Strategy(IntegrationStrategy):
    """Evolved strategy that EXECUTES: Cosine Similarity Ranker_gen1_gen2"""

    name = "cosine_similarity_ranker_gen1_gen2"
    target_module = "knowledge.semantic"

    def __init__(self, components: dict[str, Any] | None = None, **kwargs: Any) -> None:
        self._components = components or {}
        self._applied = False
        self._exports: dict[str, Any] = {}

    def validate(self, proposal: EvolutionProposal) -> tuple[bool, str]:
        return True, ""

    async def snapshot(self) -> dict[str, Any]:
        return {"applied": self._applied}

    async def apply(self, proposal: EvolutionProposal) -> list[str]:
        changes: list[str] = []

        # Execute the pattern code for real
        try:
            self._exports = _execute_pattern()
        except Exception as e:
            _logger.warning("Pattern execution failed for cosine_similarity_ranker_gen1_gen2: %s", e)
            return [f"Pattern execution failed: {e}"]

        exported = [k for k in self._exports
                    if callable(self._exports[k]) or isinstance(self._exports[k], type)]
        if exported:
            changes.append(f"Executed pattern, defined: {', '.join(exported[:10])}")
        else:
            changes.append("Executed pattern (no callable exports)")

        # Hook into live system components
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        target = "knowledge.semantic"

        # Wire exported functions/classes into the target component
        if target.startswith("knowledge") and loom is not None:
            semantic = getattr(loom, "semantic", None)
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into semantic weave")
                    elif not hasattr(loom, f"_evolved_{fn_name}"):
                        setattr(loom, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into knowledge manager")
                elif isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Registered {fn_name} class in semantic weave")

        elif target.startswith("intent") and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into intent system")

        elif target == "policy" and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into policy system")

        elif target.startswith("orchestration") and event_bus is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(event_bus, f"_evolved_{fn_name}"):
                        setattr(event_bus, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into orchestration")

        if not changes:
            changes.append("Pattern executed but no hookable exports found")

        self._applied = True
        changes.append("Source: 2602.15021v1")
        return changes

    async def rollback(self, snapshot_data: dict[str, Any]) -> None:
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        for fn_name in self._exports:
            for comp in [loom, getattr(loom, "semantic", None) if loom else None,
                         event_bus, audit_trail]:
                if comp is not None:
                    attr = f"_evolved_{fn_name}"
                    if hasattr(comp, attr):
                        try:
                            delattr(comp, attr)
                        except Exception:
                            pass
        self._applied = snapshot_data.get("applied", False)

    async def health_check(self) -> bool:
        try:
            _execute_pattern()
            return True
        except Exception:
            return False
