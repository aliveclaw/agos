"""Evolved strategy: Cosine Similarity Ranker_gen1_gen2

Auto-generated by agos evolution engine.
Source paper: 2602.15021v1
Target module: knowledge.semantic
Generated: 2026-02-18T09:17:35.795546
Defines: SemanticIndex
Code hash: 7d4e1ade0c08d1dd

Sandbox output: 
"""

from __future__ import annotations

import logging
from typing import Any

from agos.evolution.integrator import IntegrationStrategy, EvolutionProposal

_logger = logging.getLogger(__name__)

# ── Evolved pattern code ──────────────────────────────────────────
# This code passed sandbox validation and EXECUTES when apply() is called.

PATTERN_CODE = 'import math\nimport re\nfrom collections import Counter, defaultdict\nfrom functools import lru_cache\nimport json\n\nclass SemanticIndex:\n    def __init__(self, min_word_len=2, max_vocab_size=10000):\n        self.docs = {}\n        self.vocab = set()\n        self.idf_cache = {}\n        self.doc_vectors = {}\n        self.min_word_len = min_word_len\n        self.max_vocab_size = max_vocab_size\n        self.stopwords = {\'the\', \'a\', \'an\', \'and\', \'or\', \'but\', \'in\', \'on\', \'at\', \'to\', \'for\', \'of\', \'with\', \'by\', \'is\', \'are\', \'was\', \'were\', \'be\', \'been\', \'have\', \'has\', \'had\', \'do\', \'does\', \'did\', \'will\', \'would\', \'could\', \'should\'}\n        \n    def add_document(self, text, doc_id=None, metadata=None):\n        doc_id = doc_id or f"doc_{len(self.docs)}"\n        words = self._tokenize(text)\n        self.docs[doc_id] = {\n            \'text\': text,\n            \'words\': words,\n            \'metadata\': metadata or {}\n        }\n        self.vocab.update(words)\n        self._prune_vocabulary()\n        self._invalidate_cache()\n        return doc_id\n        \n    def _tokenize(self, text):\n        words = re.findall(r\'\\b[a-zA-Z]+\\b\', text.lower())\n        return [w for w in words if len(w) >= self.min_word_len and w not in self.stopwords]\n        \n    def _prune_vocabulary(self):\n        if len(self.vocab) > self.max_vocab_size:\n            word_freq = Counter()\n            for doc_data in self.docs.values():\n                word_freq.update(doc_data[\'words\'])\n            self.vocab = set(dict(word_freq.most_common(self.max_vocab_size)).keys())\n            \n    @lru_cache(maxsize=128)\n    def _compute_idf(self):\n        if self.idf_cache and len(self.idf_cache) == len(self.vocab):\n            return self.idf_cache\n            \n        doc_freq = defaultdict(int)\n        for doc_data in self.docs.values():\n            words = set(doc_data[\'words\'])\n            for word in words:\n                if word in self.vocab:\n                    doc_freq[word] += 1\n                    \n        total_docs = len(self.docs)\n        self.idf_cache = {word: math.log(total_docs / (freq + 1)) + 1\n                         for word, freq in doc_freq.items()}\n        return self.idf_cache\n        \n    def _tfidf_vector(self, words):\n        tf = Counter(words)\n        idf = self._compute_idf()\n        \n        vector = []\n        vocab_list = sorted(self.vocab)\n        total_words = len(words)\n        \n        for word in vocab_list:\n            if word in tf:\n                tf_score = (1 + math.log(tf[word])) if tf[word] > 0 else 0\n                idf_score = idf.get(word, 0)\n                vector.append(tf_score * idf_score)\n            else:\n                vector.append(0.0)\n        return vector\n        \n    def _cosine_similarity(self, vec1, vec2):\n        if len(vec1) != len(vec2):\n            return 0.0\n            \n        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n        norm1 = math.sqrt(sum(x * x for x in vec1))\n        norm2 = math.sqrt(sum(x * x for x in vec2))\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n        return dot_product / (norm1 * norm2)\n        \n    def _invalidate_cache(self):\n        self.idf_cache = {}\n        self.doc_vectors = {}\n        self._compute_idf.cache_clear()\n        \n    def build_index(self):\n        self._compute_idf()\n        self.doc_vectors = {}\n        for doc_id, doc_data in self.docs.items():\n            self.doc_vectors[doc_id] = self._tfidf_vector(doc_data[\'words\'])\n        return len(self.doc_vectors)\n        \n    def search(self, query, top_k=5, min_similarity=0.1):\n        if not self.doc_vectors:\n            self.build_index()\n            \n        query_words = self._tokenize(query)\n        if not query_words:\n            return []\n            \n        query_vector = self._tfidf_vector(query_words)\n        \n        similarities = []\n        for doc_id, doc_vector in self.doc_vectors.items():\n            similarity = self._cosine_similarity(query_vector, doc_vector)\n            if similarity >= min_similarity:\n                similarities.append((doc_id, similarity))\n                \n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n        \n    def get_related_documents(self, doc_id, top_k=3, min_similarity=0.2):\n        if doc_id not in self.doc_vectors:\n            return []\n            \n        target_vector = self.doc_vectors[doc_id]\n        similarities = []\n        \n        for other_id, other_vector in self.doc_vectors.items():\n            if other_id != doc_id:\n                similarity = self._cosine_similarity(target_vector, other_vector)\n                if similarity >= min_similarity:\n                    similarities.append((other_id, similarity))\n                    \n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n        \n    def get_document_keywords(self, doc_id, top_k=10):\n        if doc_id not in self.docs or doc_id not in self.doc_vectors:\n            return []\n            \n        doc_vector = self.doc_vectors[doc_id]\n        vocab_list = sorted(self.vocab)\n        \n        word_scores = [(vocab_list[i], score) for i, score in enumerate(doc_vector) if score > 0]\n        word_scores.sort(key=lambda x: x[1], reverse=True)\n        return word_scores[:top_k]\n        \n    def export_index(self):\n        return {\n            \'docs\': {k: {\'text\': v[\'text\'], \'metadata\': v[\'metadata\']} for k, v in self.docs.items()},\n            \'vocab\': list(self.vocab),\n            \'config\': {\n                \'min_word_len\': self.min_word_len,\n                \'max_vocab_size\': self.max_vocab_size\n            }\n        }\n        \n    def import_index(self, data):\n        self.docs = {}\n        for doc_id, doc_info in data[\'docs\'].items():\n            self.add_document(doc_info[\'text\'], doc_id, doc_info[\'metadata\'])\n        self.build_index()'

PATTERN_HASH = "7d4e1ade0c08d1dd"


def _execute_pattern() -> dict[str, Any]:
    """Execute the pattern code and return all defined names."""
    namespace = {"__builtins__": __builtins__}
    exec(compile(PATTERN_CODE, "<evolved:cosine_similarity_ranker_gen1_gen2>", "exec"), namespace)
    return {k: v for k, v in namespace.items()
            if not k.startswith("_") and k != "__builtins__"}


# ── Strategy wrapper ──────────────────────────────────────────────


class CosineSimilarityRankerGen1Gen2Strategy(IntegrationStrategy):
    """Evolved strategy that EXECUTES: Cosine Similarity Ranker_gen1_gen2"""

    name = "cosine_similarity_ranker_gen1_gen2"
    target_module = "knowledge.semantic"

    def __init__(self, components: dict[str, Any] | None = None, **kwargs: Any) -> None:
        self._components = components or {}
        self._applied = False
        self._exports: dict[str, Any] = {}

    def validate(self, proposal: EvolutionProposal) -> tuple[bool, str]:
        return True, ""

    async def snapshot(self) -> dict[str, Any]:
        return {"applied": self._applied}

    async def apply(self, proposal: EvolutionProposal) -> list[str]:
        changes: list[str] = []

        # Execute the pattern code for real
        try:
            self._exports = _execute_pattern()
        except Exception as e:
            _logger.warning("Pattern execution failed for cosine_similarity_ranker_gen1_gen2: %s", e)
            return [f"Pattern execution failed: {e}"]

        exported = [k for k in self._exports
                    if callable(self._exports[k]) or isinstance(self._exports[k], type)]
        if exported:
            changes.append(f"Executed pattern, defined: {', '.join(exported[:10])}")
        else:
            changes.append("Executed pattern (no callable exports)")

        # Hook into live system components
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        target = "knowledge.semantic"

        # Wire exported functions/classes into the target component
        if target.startswith("knowledge") and loom is not None:
            semantic = getattr(loom, "semantic", None)
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into semantic weave")
                    elif not hasattr(loom, f"_evolved_{fn_name}"):
                        setattr(loom, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into knowledge manager")
                elif isinstance(obj, type):
                    if semantic is not None and not hasattr(semantic, f"_evolved_{fn_name}"):
                        setattr(semantic, f"_evolved_{fn_name}", obj)
                        changes.append(f"Registered {fn_name} class in semantic weave")

        elif target.startswith("intent") and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) and not isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name}() into intent system")

        elif target == "policy" and audit_trail is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(audit_trail, f"_evolved_{fn_name}"):
                        setattr(audit_trail, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into policy system")

        elif target.startswith("orchestration") and event_bus is not None:
            for fn_name, obj in self._exports.items():
                if callable(obj) or isinstance(obj, type):
                    if not hasattr(event_bus, f"_evolved_{fn_name}"):
                        setattr(event_bus, f"_evolved_{fn_name}", obj)
                        changes.append(f"Hooked {fn_name} into orchestration")

        if not changes:
            changes.append("Pattern executed but no hookable exports found")

        self._applied = True
        changes.append("Source: 2602.15021v1")
        return changes

    async def rollback(self, snapshot_data: dict[str, Any]) -> None:
        loom = self._components.get("loom")
        event_bus = self._components.get("event_bus")
        audit_trail = self._components.get("audit_trail")
        for fn_name in self._exports:
            for comp in [loom, getattr(loom, "semantic", None) if loom else None,
                         event_bus, audit_trail]:
                if comp is not None:
                    attr = f"_evolved_{fn_name}"
                    if hasattr(comp, attr):
                        try:
                            delattr(comp, attr)
                        except Exception:
                            pass
        self._applied = snapshot_data.get("applied", False)

    async def health_check(self) -> bool:
        try:
            _execute_pattern()
            return True
        except Exception:
            return False
